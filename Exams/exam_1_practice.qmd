---
title: "Practice Exam 1 Questions"
format: docx
---

Your actual exam will have questions similar to those below with space below each question to give answers. I've tried to emulate that here but I haven't taken the time to print the questions out and hand write the answers, which means there may be too much or too little space in this document. I'll make sure an appropriate amount of space is given on the actual exam (which can also help you guage how detailed of an answer to give!)

- Please note that the example questions below are not exhaustive! 
- You may notice there are no programming questions below (that is no R or python syntax at all)
- There are some pseudo code questions. Here you are writing out the logic of the process and how you would go about doing it within a programming language without worrying about the syntax or the process.
- There is very little calculation required for these questions. For most answers that involve output and reading/using it, you do not need to simplify calculations. 
- I have not attempted to make the practice exam questions the same length as your actual exam (there will be fewer questions on your actual exam!)
- If you have any other questions about the content or structure of the exam, please post to the discussion forum!  

&nbsp;  

1. When considering the statistical learning paradigm, we discussed supervised and unsupervised learning. What is the major difference between supervised and unsupervised learning?

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

2. When considering predictive modeling as our goal, what is the difference between a regression and classification task?

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


3. Suppose we collect data to understand how the background of graduate school applicants relates to their graduation status (graduated or failed to graduate). We collect the following information on 200 past students:

    - undergraduate major (Mathematics, Statistics, Other STEM field, Non-STEM field)
    - undergraduate GPA (a continuous 0-4 scale)
    - the time since receiving their undergraduate degree in months
    - whether or not they previously conducted academic research (yes or no). 
    
    a. Would this be a regression or classification task?
    
    &nbsp;  
    &nbsp;  
    b. What is the response variable in this scenario?
    
    &nbsp;  
    &nbsp;  
    c. Suppose we wanted to turn the undergraduate major predictor into numeric predictors for the purposes of modeling. How many indicator (or dummy) variables are needed to account for this variable? Define these indicator variables.
    
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    d. Considering your answer to c carefully, what are $n$ and $p$ in this scenario?
    
    &nbsp;  
    &nbsp;  

4. Suppose we want to use a multiple linear regression model but the relationships between the predictors and response are non-linear. What are strategies for using a multiple linear regression model to account for these non-linear relationships?  

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
5. Give an example of a non-linear regression model. 

    &nbsp;  

6. What is meant by the term 'parametric model' vs the term 'non-parametric model'? 

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
    a. What are the advantages of using a parametric model over a non-parametric model? What are the disadvantages?
    
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
    b. Give an example of a situation where a non-parametric model would be a preferable choice. The example doesn't need to be in a real-world context but should be detailed enough to ensure the non-parametric model is a good choice.
    
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
7. We discussed the terms overfit and underfit models. Describe these two terms, especially as they relate to bias and variance of the model and training and test errors.
    
\pagebreak

8. Suppose we have a large data set where we want to perform a regression task. We want to determine the best overall model between a kNN model and a ridge regression model. We want to use a train test split and compare the best kNN and ridge regression model on the test set. We wish to determine the appropriate tuning parameters on the training set only using the bootstrap. Fully outline the process for splitting the data, tuning, comparing, and fitting a final overall best model.

\pagebreak

9. Describe the curse of dimensionality. Where does this cause problems when doing predictive modeling?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
10. When doing predictive modeling, what is a model metric? What is the most commonly used model metric for a regression task?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
11. When using the bootstrap for tuning or training our model, what is an out of bag observation and why are they useful?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;      
    
12. When doing a regression task and considering prediction as our goal, we discussed the expected squared test error given a particular observation, 
$$
E[\{Y - \widehat f(X)\}^2 | X = x_0]
$$
We were able to show this could be decomposed into three pieces. Give those three pieces (non-mathematically is fine) and describe what each term represents.

\pagebreak

13. Consider the multiple linear regression setting. We discussed using $R^2$ as a way to judge the effectiveness of a model solely on the data on which the model was trained. We saw that use of $R^2$ can be misleading. However, we noted that we could use $R^2$ in the screening step of the best subset selection procedure in order to choose the best model of each size. Fully describe this screening step of the algorithm and explain why the use of $R^2$ here is defensible. 


\pagebreak

14. Suppose we are fitting a kNN model for a regression task. 

    a. Describe how leave-one-out cross-validation (LOOCV) can be used in this setting to choose $K$. 


    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    b. What are two drawbacks of LOOCV as compared to 5 or 10 fold cross-validation?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
15. Consider the LASSO procedure for fitting a multiple linear regression model. With this model we minimize the following criterion (recall $\lambda\geq 0$):
$$
\sum_{i}(Y_i - \beta_0 - X_{i1}\beta_1 - \ldots  - X_{ip}\beta_p)^2 + \lambda\sum_{j=1}^p|\beta_j|
$$

    a. What are the benefits of fitting a LASSO model as compared to an ordinary least squares model?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    b. What happens to our coefficient estimates for a 'large' value of the tuning parameter? What happens for a tuning parmeter value near 0?

    &nbsp;  
    &nbsp;  
    &nbsp;  

16. Why is it important to standarize our predictor values when doing a ridge regression, LASSO, or elastic net model?

\pagebreak
    
17. When fitting a regularized or penalized regression model such as the LASSO we discussed using the 'one-standard error' method for selecting the tuning parameter. Explain this idea and describe the effect of using this method on the selected model.


    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  


18. We perform best subset, forward stepwise, and backward stepwise selection on a single data set with $p$ predictors. For each approach we obtain $p+1$ models containing. One model containing 0 predictors, one containing 1 predictor, etc.

    a. Which of the three models with exactly $k$ predictors has the smallest training RSS or are we unable to tell?
    
    &nbsp;  
    &nbsp;  

    b. Which of the three models with exactly $k$ predictors has the smallest test RSS or are we unable to tell?
    
    &nbsp;  
    &nbsp;  

    - Indicate whether the following statements are true or false
        
        i. The predictors in the $k$-variable model identified by **forward** stepwise selection are a subset of the predictors in the $(k+1)$-variable model identified by **forward** stepwise selection.
        ii. The predictors in the $k$-variable model identified by **backward** stepwise selection are a subset of the predictors in the $(k+1)$-variable model identified by **backward** stepwise selection.
        iii. The predictors in the $k$-variable model identified by **backward** stepwise selection are a subset of the predictors in the $(k+1)$-variable model identified by the **best subset** method.
        iv. The predictors in the $k$-variable model identified by **best subset** selection are a subset of the predictors in the $(k+1)$-variable model identified by the **best subset** method.
          

\newpage

19. Suppose we fit a multiple linear regression model to data about how much people earn. Our response variable is the `wage` (in 1000's of dollars) and our predictors are `marital_status` (`married`, `never_married`, or `divorced`), `age`, and `year` that the data was collected. We include an interaction between `marital_status` and `age` in the model. Output for the model is given below.

    ```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ISLR2)
library(dplyr)
library(knitr)
wage_data <- ISLR2::Wage |>
  mutate(marital_status = ifelse(maritl == "1. Never Married", "never_married", ifelse(maritl == "2. Married", "married", "divorced")))
fit <- lm(wage ~ marital_status + age + year + marital_status*age, data = wage_data)
summary(fit)$coefficients |>
  round(3) |>
  kable()
    ```

    a. Write down the fitted equation for $\hat{y}$. Define any indicator variables as needed.
    
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    b. One column of the output represents the standard error. What is a standard error generally?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    c. Write down the form of a predicted value for somone that is `married`, has an `age` of 30, and had a `year` of data collection of 2008. No need to simplify.

    &nbsp;  
    &nbsp;  

    d. Write down the form of a predicted value for somone that is `divorced`, has an `age` of 30, and had a `year` of data collection of 2008. No need to simplify.

    &nbsp;  
    &nbsp;  

    e. Consider creating a confidence interval for the mean wage as compared to a prediction interval for a future wage. Which would be wider? Why?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    f. Conceptually, what does including an interaction between `marital_status` and `age` do to our model as compared to a model without that interaction (that still includes main effects for both, and an effect for `year`)?

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    g. Based on the above output, which of the **three** predictors would you deem important for the model? State the hypotheses and p-values you used to make this determination.

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  

    h. Suppose we wanted to check whether the interaction term between `marital_status` and `age` was important for the response, given the other variables in the model. Describe how we could investigate this (you can't tell with the output above and I'm not expecting you to give the exact formulas - describe the idea of the inferential method we looked at in class for answering this question).

    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    &nbsp;  
    
    i. What type of plot might we look at to investigate the distributional assumption on the errors?

    &nbsp;  
    &nbsp;  

    j. A few observed values and predicted values are given in the table below. What is the residual for the first observation. No need to simplify.
    
    ```{r echo = FALSE}
preds <- data.frame(wage = wage_data$wage[1:3], predicted = predict(fit, wage_data[1:3, ]))
row.names(preds) <- NULL
preds |> 
  kable()
    ```




