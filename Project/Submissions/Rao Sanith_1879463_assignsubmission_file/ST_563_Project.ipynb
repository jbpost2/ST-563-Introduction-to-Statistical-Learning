{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ST 563 Project\n",
        "\n",
        "By: Sanith Rao\n",
        "\n",
        "Overview:\n",
        "For this project, I analyze the New York City Airbnb Open Data, sourced from Kaggle. This dataset provides detailed information on Airbnb listings, including pricing, location, host details, and property characteristics. My goal is to build predictive models to estimate log-transformed price based on various listing features. The logarithmic transformation helps address price skewness and improves model interpretability. Accurate price prediction is valuable for hosts to set competitive rates and for potential guests to anticipate reasonable booking costs."
      ],
      "metadata": {
        "id": "sK68N83a9YIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading pygam for Splines\n",
        "!pip install pygam"
      ],
      "metadata": {
        "id": "34otzaKVHPpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from pygam import LinearGAM, s\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.sparse import issparse"
      ],
      "metadata": {
        "id": "vAyDxfDZ_MV0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Transformations:\n",
        "I went through several steps to clean and transform the data to make it ready\n",
        "to be used by the different models that I implemented. I got rid of some irrelevant columns that add no value to our models. I filled in values for the reviews per month with 0 to ensure there were no missing values. The price variable was transformed to a log price to account for potential skewness and to handle outliers, I got rid of all data points for prices above the 99th percentile. All the categorical variables were one-hot coded, a method I came across online, to avoid multicollinearity."
      ],
      "metadata": {
        "id": "UmfBnH7ml2Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV file and printing it's info to see what data cleaning and\n",
        "# transformations need to be done.\n",
        "df = pd.read_csv('/content/sample_data/AB_NYC_2019.csv')  # Add the path of the file\n",
        "df.info()"
      ],
      "metadata": {
        "id": "r3OZ1xR68O9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning & transformations\n",
        "#\n",
        "df.drop(columns=[\"id\", \"name\", \"host_id\", \"host_name\", \"last_review\"], inplace=True)\n",
        "\n",
        "df[\"reviews_per_month\"].fillna(0, inplace=True)\n",
        "\n",
        "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
        "\n",
        "price_cap = df[\"price\"].quantile(0.99)  # 99th percentile\n",
        "df = df[df[\"price\"] <= price_cap]\n",
        "\n",
        "df = pd.get_dummies(df, columns=[\"neighbourhood_group\", \"neighbourhood\", \"room_type\"], drop_first=True)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "0d-Xf7x08PF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())  # Should show 0 for all columns"
      ],
      "metadata": {
        "id": "_riKDMtn8PIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test set\n",
        "x = df.drop(columns=[\"price\", \"log_price\"])  # Features\n",
        "y = df[\"log_price\"]  # Target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2xULGmY08PNT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Model:\n",
        "K-Nearest Neighbors is a non-parametric, instance-based learning algorithm that predicts values based on the average of the k nearest training points. It has a key tuning parameter, k, which is the number of neighbors, which controls the bias-variance tradeoff: lower values lead to higher variance, while higher values smooth predictions. KNN does not perform variable selection and is not suitable for inference since it does not learn explicit parameters. Standardizing predictors is essential, as the algorithm relies on distance metrics that can be affected by differences in feature scales. Grid search with cross-validation was used to find the optimal k, ensuring the best model performance.\n",
        "\n",
        "After running, we see that it has a very high RMSE compared to the other models that I implemented indicating that it does not capture relationships well."
      ],
      "metadata": {
        "id": "IdCN4XDmm5ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "knn = KNeighborsRegressor()\n",
        "\n",
        "# Define the grid of possible values for k\n",
        "param_grid = {'n_neighbors': [5, 10, 15, 20, 25, 30]}\n",
        "\n",
        "# Use GridSearchCV to find the best k using cross-validation\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best k\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "print(f\"Best k: {best_k}\")\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "# Inverse the log transformation for the predictions (since price was log-transformed)\n",
        "y_pred_knn_exp = np.expm1(y_pred_knn)  # This is equivalent to exp(log(price)) - 1\n",
        "\n",
        "# Inverse the log transformation for the true values\n",
        "y_test_exp = np.expm1(y_test)\n",
        "\n",
        "# Calculate MSE and RMSE (on the original price scale)\n",
        "mse_knn = mean_squared_error(y_test_exp, y_pred_knn_exp)\n",
        "rmse_knn = np.sqrt(mse_knn)\n",
        "print(f\"kNN RMSE: {rmse_knn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rwa1WE08PPW",
        "outputId": "886d2e31-a7e3-4c9d-f695-41205ceee26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best k: 10\n",
            "kNN RMSE: 93.89033735345937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression with both Ridge and LASSO:\n",
        "Ridge and Lasso regression are both parametric linear models that introduce regularization to prevent overfitting. Ridge regression (L2 regularization) shrinks coefficients toward zero but does not set them exactly to zero, whereas Lasso (L1 regularization) can eliminate some coefficients, effectively performing variable selection. Both models have a key tuning parameter, alpha, which controls the strength of regularization: higher values increase regularization, reducing variance but increasing bias. These models can be used for inference, but Ridge retains all variables, while Lasso selects a subset. Standardizing predictors is recommended to ensure fair penalty application across all features. Cross-validation was used to optimize alpha, ensuring the best tradeoff between bias and variance.\n",
        "\n",
        "What I noticed here is that the Ridge regression had a very high RMSE while the LASSO regression had a significantly lower RMSE which tells us that we had to get rid of some features to enhance the accuracy of the model for linear regression."
      ],
      "metadata": {
        "id": "el97wRv6nxuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression with Ridge\n",
        "\n",
        "# Define the Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define the grid of possible values for alpha (regularization strength)\n",
        "param_grid_ridge = {'alpha': [0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "# Use GridSearchCV to find the best alpha using cross-validation\n",
        "grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Best alpha\n",
        "best_alpha_ridge = grid_search_ridge.best_params_['alpha']\n",
        "print(f\"Best alpha for Ridge Regression: {best_alpha_ridge}\")\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_ridge = grid_search_ridge.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_ridge = best_ridge.predict(X_test)\n",
        "\n",
        "# Inverse the log transformation for the predictions\n",
        "y_pred_ridge_exp = np.expm1(y_pred_ridge)\n",
        "\n",
        "# Inverse the log transformation for the true values\n",
        "y_test_exp = np.expm1(y_test)\n",
        "\n",
        "# Calculate MSE and RMSE\n",
        "mse_ridge = mean_squared_error(y_test_exp, y_pred_ridge_exp)\n",
        "rmse_ridge = np.sqrt(mse_ridge)\n",
        "print(f\"Ridge Regression RMSE: {rmse_ridge}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cIb9Tc68PRh",
        "outputId": "0f52422a-a7cd-4aa7-f39d-850204d8d6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alpha for Ridge Regression: 1\n",
            "Ridge Regression RMSE: 81.3224124928855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression with LASSO\n",
        "\n",
        "# Define the model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Define the grid of possible values for alpha (regularization strength)\n",
        "param_grid = {'alpha': [0.1, 0.5, 1, 5, 10, 50, 100]}\n",
        "\n",
        "# Use GridSearchCV to find the best alpha using cross-validation\n",
        "grid_search_lasso = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Best alpha\n",
        "best_alpha_lasso = grid_search_lasso.best_params_['alpha']\n",
        "print(f\"Best alpha for Lasso Regression: {best_alpha_lasso}\")\n",
        "\n",
        "# Get the best model and evaluate\n",
        "best_lasso = grid_search_lasso.best_estimator_\n",
        "y_pred_lasso = best_lasso.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "rmse_lasso = np.sqrt(mse_lasso)\n",
        "print(f\"Lasso Regression RMSE: {rmse_lasso}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADFYy1LQ8PU3",
        "outputId": "55b9faf2-2b91-470b-e788-2b2902ec5867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alpha for Lasso Regression: 0.1\n",
            "Lasso Regression RMSE: 0.5641653464975939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Tree Model:\n",
        "The decision tree regression model is a non-parametric model that recursively splits the data into homogeneous groups to make predictions. It has key tuning parameters, such as max_depth, which controls how deep the tree can grow—deeper trees capture more complexity but risk overfitting. Decision trees do not assume a specific functional form, making them useful for capturing non-linear relationships, but they are generally not used for inference due to their lack of interpretability. Unlike Lasso regression, decision trees perform implicit variable selection by prioritizing the most informative features for splitting. Standardizing predictors is not required since decision trees are invariant to feature scaling. Cross-validation was used to optimize max_depth, balancing model complexity and predictive performance.\n",
        "\n",
        "We see that this also has a very low RMSE which indicators that more complex tree models could be used to obtain more accuracte results."
      ],
      "metadata": {
        "id": "2OHiYXnCoL5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single Tree Model\n",
        "\n",
        "# Define the model\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'max_depth': [3, 5, 10, 15, 20, None]}\n",
        "\n",
        "# Use GridSearchCV to find the best depth using cross-validation\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best depth\n",
        "best_depth = grid_search.best_params_['max_depth']\n",
        "print(f\"Best depth: {best_depth}\")\n",
        "\n",
        "# Get the best model and evaluate\n",
        "best_dt = grid_search.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "rmse_dt = np.sqrt(mse_dt)\n",
        "print(f\"Decision Tree RMSE: {rmse_dt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naaBpKmQHZX1",
        "outputId": "b3845019-3d5c-424c-851f-ba4b5a513cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best depth: 10\n",
            "Decision Tree RMSE: 0.42714883115952146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model:\n",
        "The random forest regression model is an ensemble-based, non-parametric model that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. It has key tuning parameters, including n_estimators, the number of trees in the forest, and max_depth, the depth of each tree, both of which affect model complexity and generalization. While random forests provide strong predictive performance, they are not typically used for inference due to their black-box nature. The model performs implicit variable selection by determining feature importance based on how often a variable is used for splitting across trees. Standardizing predictors is not necessary, as tree-based models are scale-invariant. Cross-validation was used to optimize hyperparameters, ensuring a balance between bias and variance.\n",
        "\n",
        "Although we do not use random forest much for inference, we see that the results are the best of what we have had so far, indicating that tree based models give us the best results for this particular problem."
      ],
      "metadata": {
        "id": "wpSfu3OEolZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Random Forest as the ensemble tree model\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search_rf = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "# RMSE Calculation\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "\n",
        "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
        "print(f\"Random Forest RMSE: {rmse_rf}\")"
      ],
      "metadata": {
        "id": "X9cc3U9SHZZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I had actually finished running this block of code and got the following results:\n",
        "Best parameters: {'max_depth': 15, 'n_estimators': 200}\n",
        "Random Forest RMSE: 0.3986131908193861\n",
        "\n",
        "I tried to run it again before submitting but it was taking too long which is why I'm pasting my results in a text box."
      ],
      "metadata": {
        "id": "bPOSy3SZtr4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine:\n",
        "Support Vector Machines are non-parametric models that find an optimal hyperplane to minimize prediction error while maximizing margin. The model has a key tuning parameter, C, which controls the trade-off between model complexity and error tolerance. A higher C reduces bias but may lead to overfitting. SVMs are not commonly used for inference due to their complex decision boundaries, and they do not perform variable selection directly. Standardizing predictors is essential, as SVMs are sensitive to feature scales. In this implementation, a linear kernel (via LinearSVR) was chosen for efficiency, and a smaller dataset was used to improve training speed. Hyperparameter tuning was performed using cross-validation to optimize performance.\n",
        "\n",
        "From the result, we see that this also performed well but did not perform as well as our tree models."
      ],
      "metadata": {
        "id": "5zO7G6NAo3dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine\n",
        "\n",
        "# Subsample for speed (use ~10,000 rows)\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=10000, random_state=42)\n",
        "\n",
        "# Feature scaling (critical for SVMs)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sample)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Use LinearSVR (much faster)\n",
        "svm = LinearSVR(random_state=42, max_iter=10000)\n",
        "\n",
        "# Smaller hyperparameter grid for speed\n",
        "param_grid = {'C': [0.1, 1, 10]}\n",
        "\n",
        "# Grid search with cv=3 (faster than cv=5)\n",
        "grid_search_svm = GridSearchCV(svm, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
        "grid_search_svm.fit(X_train_scaled, y_train_sample)\n",
        "\n",
        "# Best model\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred_svm = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# RMSE Calculation\n",
        "rmse_svm = np.sqrt(mean_squared_error(y_test, y_pred_svm))\n",
        "\n",
        "print(f\"Best parameters: {grid_search_svm.best_params_}\")\n",
        "print(f\"SVM RMSE: {rmse_svm}\")"
      ],
      "metadata": {
        "id": "sQOwr2d9Nssx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c189d1dd-c29b-4c33-8b6e-ecb449b274fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "Best parameters: {'C': 0.1}\n",
            "SVM RMSE: 0.45366025696105317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After completing all the models, we observe that our most accurate model was the random forest model just because of its predictive power. However, even the SVM and Linear Regression with LASSO regression gave us extremely good results, indicating that the appropriate features were used."
      ],
      "metadata": {
        "id": "jhLayck_pMMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Random Forest model with the best hyperparameters\n",
        "rf_final = RandomForestRegressor(\n",
        "    n_estimators=200,  # Best n_estimators\n",
        "    max_depth=15,       # Best max_depth\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "# Combine the training and testing data (X_train + X_test and y_train + y_test)\n",
        "X_full = pd.concat([X_train, X_test])\n",
        "y_full = pd.concat([y_train, y_test])\n",
        "\n",
        "# Fit the model on the entire dataset\n",
        "rf_final.fit(X_full, y_full)\n",
        "\n",
        "# Optionally, predict on the entire dataset\n",
        "y_pred_full = rf_final.predict(X_full)\n",
        "\n",
        "# Calculate RMSE on the full dataset\n",
        "mse_full = mean_squared_error(y_full, y_pred_full)\n",
        "rmse_full = np.sqrt(mse_full)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Random Forest Model refitted on the entire dataset\")\n",
        "print(f\"RMSE on the full dataset: {rmse_full}\")"
      ],
      "metadata": {
        "id": "2TSwAuxqp30i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}