---
title: "Homework 2 Key"
always_allow_html: yes
format: docx
editor_options: 
  chunk_output_type: console
---

# Conceptual Problems

## Book Problems

Complete the following problems from the Introduction to Statistical Learning with R book (I'm not sure if the problems are in the same order in the python book so use the R book to identify which problems to do).

Section 3.7

- Book Problem 3

  Our fitted model is given by $$
\hat{y} = 50 + 20(GPA) + 0.07(IQ) + 35(College Indicator)+0.01(GPA)(IQ)-10(GPA)(College Indicator)
  $$

    a. For those with college our fitted line is
$$
\hat{y} = 85 + 10(GPA)+0.07(IQ)+0.01(GPA)(IQ)
$$
For those with high school our fitted line is
$$
\hat{y} = 50 + 20(GPA)+0.07(IQ)+0.01(GPA)(IQ)
$$
iii is the answer as the high school predicted values can be larger, given that the GPA is high enough.
    b. College graduate, IQ = 110, and GPA = 4.0 yields
        ```{r}
50 + 20*4 + 0.07*110 + 35+0.01*4*110-10*4
        ```
    c. False, we'd need to look at a hypothesis test or confidence interval to determine this. Without knowing the standard error or other information, we can't determine whether the effect is significant or not.

- Book Problem 4

    a. We'd expect the training RSS to be smaller for the cubic model as the RSS always decreases with the addition of more predictors.
    b. As the true relationship is linear, we'd expect the test MSE for the linear model to be smaller as the cubic model would be overfitting to the data and should not generalize as well.
    c. Same answer as a.
    d. We don't have enough information to determine this. It would depend on how well the linear function approximates the true function vs how well the cubic function does.

Section 6.6

- Book Problem 1

    a. The best subset selection model as that method would look include looking at the forward and backward models.
    b. It isn't clear. We can't say that one method will produce the best model in terms of prediction error.
    c. True/False
    
        i. True
        ii. True
        iii. False
        iv. False
        v. False

- Book Problem 2

    a. For the LASSO, iii is the best choice. The LASSO introduces bias in a hope for a reduction in variance. As it shrinks the estimates towards 0 generally, this would mean it is less flexible.
    b. For RR, iii is the best choice with the same reasoning as a.
    c. For non-linear methods, ii is the best choice. They allow for even less bias towards the true f but can introduce more variance.

- Book Problem 3

    a. iv, as the training RSS will steadily decrease. We know the training RSS is the same function that least squares minimizes.
    b. ii, the test RSS will generally be optimized for some s in the range of 0 to the sum of the absolute value of the OLS solutions.
    c. iii, the variance of the predictions will generally get larger as the model increases its flexibility.
    d. iv, the squared bias will decrease as we get closer to the OLS solution
    e. v, the irreducibile error doesn't change
    
- Book Problem 4

    a. iii, as the training RSS will steadily increase as we penalize more
    b. ii, the test RSS will generally be optimized for some $\lambda$ larger than 0 but will get worse as we penalize too much
    c. iv, the variance of the predictions will generally get less and less the more we penalize the size of the coefficients
    d. iii, the squared bias will increase the further we get from the OLS solution
    e. v, the irreducibile error doesn't change

# Implementation Problems 

## Problem 1

### Fitting an Elastic Net Model

For this problem we'll fit an Elastic net model to our bike sharing data set. The code below reads the data in and does the modifications from the notes:

```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(lubridate)
#read in data from previous set of notes
bike_share <- read_csv("https://www4.stat.ncsu.edu/online/datasets/SeoulBikeData.csv",
                       local = locale(encoding = "latin1"))

bike_share <- bike_share |>
  rename("date" = "Date",
         "rented_bike_count" = `Rented Bike Count`,
         "hour" = "Hour",
         "temperature" = `Temperature(°C)`,
         "humidity" = `Humidity(%)`,
         "wind_speed" = `Wind speed (m/s)`,
         "visibility" = `Visibility (10m)`,
         "dew_point_temperature" = `Dew point temperature(°C)`,
         "solar_radiation" = `Solar Radiation (MJ/m2)`,
         "rainfall" = `Rainfall(mm)`,
         "snowfall" = `Snowfall (cm)`,
         "seasons" = "Seasons",
         "holiday" = "Holiday",
         "functioning_day" = "Functioning Day" 
         ) |>
  mutate(date = dmy(date), #convert the date variable from character
         seasons = factor(seasons),
         holiday = factor(holiday),
         functioning_day = factor(functioning_day),
         log_rented_bike_count = log(rented_bike_count)) |>
  filter(functioning_day == "Yes")
```

a. First, split the data into a train/test set. 

```{r}
library(caret)
set.seed(8)
split <- createDataPartition(bike_share$log_rented_bike_count, 
                             p = 0.8,
                             list = FALSE)
bike_train <- bike_share[split, ]
bike_test <- bike_share[-split,]
```


b. Create your own grid of tuning parameter values (in `R` the `expand.grid()` function can be useful for creating a data frame at every combination of two vectors), use 5 fold CV, preprocess your numeric predictors, and select your optimal tuning parameters on the training set.

```{r}
tc <- trainControl(method = "CV",
                  number = 5)
#did a quick fit with a few values of alpha and lambda
#used that to narrow it down to this grid
tuning_grid <- expand.grid(alpha = seq(from = 0, to =1, length = 15),
                           lambda = seq(from = 0, to = 0.5, length = 15))

#train the model!
en_fit <- train(log_rented_bike_count ~ hour +  
                temperature + humidity + wind_speed +
                  dew_point_temperature + solar_radiation +
                  rainfall + snowfall + seasons, 
                data = bike_train,
                method = "glmnet",
                preProc = c('center', 'scale'),
                tuneGrid = tuning_grid,
                trControl = tc
                )
#best parameters
en_fit$bestTune
en_fit$bestTune |>
  knitr::kable()
```


c. Determine the RMSE on the test data set using your best model.

```{r}
preds <- en_fit |> 
  predict(newdata = bike_test)
metrics <- postResample(preds, obs = bike_test$log_rented_bike_count)
metrics
data.frame(value = metrics) |>
  knitr::kable()
```


## Problem 2

The `datasets` package in `R` has a data set called `trees` (`datasets::trees` in `R`). The data is also available at <https://www4.stat.ncsu.edu/online/datasets/trees.csv> if you are using another software.

a. First, create scatter plots between the predictors and the response (two scatter plots).

```{r}
#baseR pairs function summarizes small datasets well
pairs(trees)
```

Now we want to fit a couple of linear regression models to predict `Height` of the trees. 

b. Fit a model with main effects for both `Girth` and `Volume`. Are either of these predictors statistically significant? Report the relevant hypothesis tests, p-values, and interpretations. Are the assumptions of inference met (at least roughly) here? Provide evidence (plots!).

```{r}
tree_fit <- lm(Height ~ Girth + Volume, data = datasets::trees)
#summary(tree_fit)
summary(tree_fit)$coef |> 
  knitr::kable()
f_info <- summary(tree_fit)$fstatistic
f_info |>
  knitr::kable()
#pvalue manually
1-pf(f_info[1], df1 = f_info[2], df2 = f_info[3])
```

The F-stat is significant (9.82, p-value = 0.0005868, tests if both slopes are zero vs at least one differs) so the model is useful. The `Volume` variable has a significant p-value (0.0145, tests if that slope is 0 vs not 0) but the `Girth` variable does not (0.1188, tests if that slope is 0 vs not 0). 

To check assumptions we can run:
```{r}
plot(tree_fit)
```

Looking at these we see that there is a slight pattern in the fitted vs residuals chart but nothing too extreme. The qqplot looks great so the normality assumptions seems reasonable. No points seem to have extreme leverage or appears to be an outlier.

c. Produce a predicted height for a tree with a `Girth` of 10 and a `Volume` of 12.

```{r}
predict(tree_fit, newdata = data.frame(Volume = 12, Girth = 10))
```

d. Fit a model with main effects for both `Girth` and `Volume` along with their interaction. Are either of these predictors statistically significant? Report the relevant hypothesis tests, p-values, and interpretations. Are the assumptions of inference met (at least roughly) here? Can you explain any discrepancies in the importance of the predictors in this model as compared to the first model we fit?

```{r}
tree_fit_int <- lm(Height ~ Girth*Volume, data = datasets::trees)
#summary(tree_fit_int)
summary(tree_fit_int)$coef
f_info_int <- summary(tree_fit_int)$fstatistic
f_info_int |>
  knitr::kable()
#pvalue manually
1-pf(f_info_int[1], df1 = f_info_int[2], df2 = f_info_int[3])
```

The global F-test is again significant (F = 11.21, p-value very small, tests if the slopes are all zero vs at least one differs). Now we see that every predictor is significant (we reject the individual tests that each slope is 0 in favor of the alternative that they are non-zero).  

This in some ways contradicts the previous model... However, recall that these are tests of whether that predictor is important given the other predictors are in the model. In this case, `Girth` is important given that the interaction between `Girth` and `Volume` is in the model. This is important! We now know that each predictor is important and the effect of the variable on the tree height depends on the other predictor variable's value!

Checking assumptions:
```{r}
plot(tree_fit_int)
```

We see very little pattern in the residuals vs fitted plot. Normality still looks great in the qqplot. There is one point that seems to have high leverage and is affecting the model fit a bit. That point might be interesting to look at further if we knew more about the data.

e. Produce a predicted height for a tree with a `Girth` of 10 and a `Volume` of 12.

```{r}
predict(tree_fit_int, newdata = data.frame(Girth = 10, Volume = 12))
```

f. Fit two different models. First a model with just `Volume` as a predictor. Second a model with `Volume`, `Volume` squared, and `Volume` cubed. Recall the `I()` function if you are using `R`. Perform the F-test discussed in the notes to determine if the squared and cubic terms are important for the model.

```{r}
linear <- lm(Height ~ Volume, data = datasets::trees)
cubic <- lm(Height ~ Volume + I(Volume^2) + I(Volume^3), 
            data = datasets::trees)
anova(linear, cubic) |>
  knitr::kable()
```

Here we see a marginally significant p-value (0.09841). This indicates that the cubic model may be useful above and beyond the simple linear model.

## Problem 3

Using the `Carseats` data from the `ISLR2` package, split the data into a training and a test set (80/20). Then use best subset, backward, and forward selection to determine the optimal model size for each type of model (similar to how we did this in the notes, no need to use CV here). Once you've determined the best size for each model, fit that to the entire data set.

I'll just give code for the best subset selection method. The forward and backward are very similar.

```{r}
set.seed(101)
library(ISLR2)
library(leaps)
## Create test and training sets
data_split <- createDataPartition(Carseats$Sales, 
                                  p = 0.8, 
                                  list = FALSE
                                  )

test_set <- Carseats[-data_split, ]
train_set <- Carseats[data_split, ]

## Best subset selection on the training data
best_train <- regsubsets(Sales ~ .,
                         data = train_set,
                         nvmax = 11)

train_sum <- summary(best_train)
```

- For each model size, estimate the test performance. Pull this function from our notes to help:

```{r}
#We'll write a function to predict and estimate the error on the test set. 
#Inputs are 
#- model size (mod_size),
#- summary output of the selection process (reg_summary)
#- model matrix of the test data (test_model)
#- test set response (test_resp)
test_err <- function(mod_size, 
                     reg_summary, 
                     test_model,
                     test_resp){
  # get regression coefs
  betahat <- coef(reg_summary$obj, mod_size)
  # get best subset of the specified size
  sub <- reg_summary$which[mod_size, ]
  # Create test model matrix, prediction, test error
  model <- test_model[, sub]
  yhat <- model %*% betahat
  err <- mean((test_resp - yhat)^2)
  return(err)
}
```

- Apply the function to each model size

```{r}
#define the test model
test_model <- model.matrix(~ CompPrice + Income + Advertising + 
                             Population + Price + ShelveLoc +
                             Age + Education + Urban + US,
                           data = test_set)

#define the test response
test_resp <- test_set$Sales

#apply the function to each of the model sizes
hold_err <- sapply(1:11, #apply the function to these 
                  FUN = test_err, 
                  reg_summary = train_sum,
                   test_model = test_model, 
                   test_resp = test_resp)
```

- Now let's plot the errors
```{r, fig.cap = "Holdout error as a function of model size", fig.alt = 'The image depicts a line graph displaying a decreasing trend. The x-axis is the model size, ranging from 1 to 11. The y-axis is the holdout error, with values decreasing from 0.9 to below 0.6. The graph shows a steep decline initially, which gradually levels off as the model size increases.', fig.margin = FALSE, fig.width=5, fig.height=3}
plot(hold_err, type = 'b', pch=19, lwd=2)
```

- Choose the optimal model size and use that model size on a model fit to the full data set

```{r}
size_opt <- which.min(hold_err)
size_opt
#fit on the full data set
bestmod <- regsubsets(Sales ~ .,
                         data = Carseats,
                         nvmax = 11)
#Use the optimal size
coef(bestmod, size_opt) |>
  round(3) |>
  knitr::kable()
```