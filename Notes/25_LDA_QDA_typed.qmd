---
title: "Linear & Quadratic Discriminant Analysis"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(nnet)
library(glmnet)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(tidyverse)
library(mda)
```


```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\pagebreak


We talked about two major ways to create classification models:

- Models that directly try to model the conditional class probabilities, $P(Y|X)$
- Generative models that model $X|Y$'s distribution and use Bayes' theorem to obtain estimated conditional class probabilities

LDA & QDA take the generative model approach!

# Bayes' Theorem

To understand how generative models work, let's start with understanding Bayes' theorem. Bayes' theorem is built on the idea of conditional probability. 

**Conditional Probability** - Let $A$ and $B$ be two events where $P(B\neq 0)$. The conditional probability of A given B has occurred is
$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
Assuming the $P(A)>0$, we can use conditional probability on $P(B|A)$ to rewrite this as
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

This is essentially Bayes' theorem!

Example: Suppose that we have three types of wines (cultivars). Call these $A_1, A_2,$ and $A_3$. Assume we have a measurement of quality on the wine (good or bad). We know, given the type of wine, the probability the wine is good:
$$P(\mbox{Good}|A_1) = 0.6, P(\mbox{Good}|A_2) = 0.4, P(\mbox{Good}|A_3) = 0.9$$
Further, we know the relative occurrences of the three types of wine:
$$P(A_1) = 0.7, P(A_2) = 0.1, P(A_3) = 0.2$$

We can use Bayes' theorem to find the probability the wine is of type $A_1$ given its good or bad status.

$$P(A_1|\mbox{Good}) = \frac{P(\mbox{Good}|A_1)P(A_1)}{P(\mbox{Good})}$$

We just need to find the overall probability of good wine. This can be found using the law of total probability!

$$P(\mbox{Good}) = P(\mbox{Good}|A_1)P(A_1)+P(\mbox{Good}|A_2)P(A_2)+P(\mbox{Good}|A_3)P(A_3)$$

Thus, we have 
$$P(A_1|\mbox{Good}) = \frac{P(\mbox{Good}|A_1)P(A_1)}{P(\mbox{Good}|A_1)P(A_1)+P(\mbox{Good}|A_2)P(A_2)+P(\mbox{Good}|A_3)P(A_3)}$$

The Bayes' theorem can be extended to probability distributions (PDFs and PMFs if you are familiar with that language) rather than simply events like these. 

In general, we have the following setup (I'm not going to use the book notation here as I think it is less clear than I'd like it to be): 

+ For $i$-th item, we observe predictors $\X_i = (X_{i1}, \ldots, X_{ip})^T$, and a class label $Y_i$ (taking values in $1, 2, \ldots, K$).

+ The conditional density function of $X_i | Y_i = K$ is $f_{X_i|Y_i=K}(x_i|y_i=K)$, $k = 1, 2, \ldots, K$.

+ $\text{P}(Y_i = k) = p_Y(K)$, such that $p_Y(1) + \ldots + p_Y(K) = 1$. These are the *prior probabilities* of the classes. 

    - In other words, probability that a randomly chosen observation comes from the prior $k$-th class is $p_Y(k) = P(Y=k)$. 

By *Bayes theorem*, we obtain

$$
P(Y_i = k | X_i = x) = \frac{p_Y(k)f_{X_i|Y_i}(x|k)}{p_Y(1)f_{X_i|Y_i}(x|1) + \ldots + p_Y(K)f_{X_i|Y_i}(x|K)}
$$
The sample is predicted to be in the class that has the *highest posterior probability*. Notice that the denominator in the expression is same for any value of $k$. 

- The posterior probability is thus highest where the numerator $p_Y(k)f_{X|Y}(x|k)$ is highest.

Of course, the density functions $f_{X|Y}(x|y)$ and the prior probabilities $p_Y(k)$ are unknown. 

- We can estimate $p_Y(k)$ by using the relative proportion of its occurrence (assuming a random sample). 
$$
\widehat p_k = n_k/n.
$$
where $n_k$ represents the number of observations in class $k$.

For the unknown densities, we need to estimate them from the data! This can be modeled in a number of different ways.

# Linear & Quadratic Discriminant Analysis

LDA and QDA assume the conditional densities of $X|Y$ are Normally distributed. The difference between LDA and QDA comes from the assumption on the variance of those Normal distributions.

These ideas are most easily shown through an example.

## Example: Developing QDA with One Predictor

To illustrate the basic ideas, consider the `wines` data set [available at the UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/wine). 

Recall: The dataset contains quantities of 13 constituents found in each of the three types (cultivars) of wines.

- $Y_i = 1, 2, \mbox{ or }3$, but these numbers represent categories. 

```{r, warning = FALSE, message = FALSE}
# Read the data
wines <- read_table("https://www4.stat.ncsu.edu/online/datasets/Wines.txt")
# classes of wine
tab <- table(wines$Class) |> data.frame()
names(tab) <- c("Class", "Frequency")
tab |>
  kable()
```

We can easily approximate $p_Y(k)$ for $k = 1, 2, 3$.

```{r}
p_Y <- table(wines$Class)/nrow(wines)
p_Y_df <- data.frame(p_Y)
names(p_Y_df) <- c("Class", "Probability")
p_Y_df |> 
  kable()
```

Now we just need to obtain estimates for the conditional distributions of $X|Y$ and we can use Bayes' theorem to get a classifier!

Let's just consider the `Alcohol` variable in creation of our model.

```{r, tidy=FALSE}
# new data set
wine_small <- wines |>
  dplyr::select(Class, Alcohol) |>
  mutate(Class = as.factor(Class))
```

```{r, echo=FALSE}
a <- aggregate(Alcohol ~ Class, length, data = wine_small)
n1 <- a$Alcohol[1]
n2 <- a$Alcohol[2]
n3 <- a$Alcohol[3]
```

We might consider the Normal distribution as a candidate for $X|Y$'s distribution.

- We really have three distributions, one for each value $Y$ can take on.

```{r qqa, fig.height=4, fig.width=8, fig.margin=T, echo=FALSE, fig.cap="QQ plot for Alcohol for the two groups.", fig.alt = 'QQplots are shown comparing the observations of alcohol within each class to a Normal distribution. All three qqplots show a roughly straight line, indicating that Normal distributions may be reasonable for the alcohol values within a particular class.'}
# Normal Q-Q plots
par(mfrow=c(1,3))
alc_1 <- wine_small |> 
  filter(Class == 1) |>
  pull(Alcohol)
alc_2 <- wine_small |> 
  filter(Class == 2) |>
  pull(Alcohol)
alc_3 <- wine_small |> 
  filter(Class == 3) |>
  pull(Alcohol)
qqnorm(alc_1, main = "Class = 1", pch=19)
qqline(alc_1)
qqnorm(alc_2, main = "Class = 2", pch=19)
qqline(alc_2)
qqnorm(alc_3, main = "Class = 3", pch=19)
qqline(alc_3)
```

- Plots show a fairly linear pattern
- Not unreasonable to assume that the data from all three classes follow normal distributions with possibly different means and variances. 


```{r winealc, echo=FALSE, fig.cap="Estimated density functions of Alcohol for the two classes in wine data.", eval=FALSE, fig.alt= 'Density plots for the distribution of alcohol for each class of wine are shown. The alcohol distribution for class 2 is located towards the smaller values of alcohol, 11-13, and the data appear to be unimodal, roughly bell-shaped, but with a bit of a skew towards the right. The alcohol distribution for class 3 is located in the middle of the alcohol values, 12-14, and the data appears to be roughly normally distributed. The alcohol distribution for class 1 is located furthest to the right, values between 13 and 15. This distribution is unimodal and roughly bell-shaped as well.'}
ggplot(wine_small) + 
  geom_density(aes(x = Alcohol, group = Class, fill = Class), lwd=2, alpha = 0.3) + 
  theme_bw(base_size = 18) + 
  scale_color_viridis_d(option = "magma") + 
  labs(fill='Class') + 
  theme(legend.position = "top")
```

We have: 

+ Three classes: $K=3$
+ One predictor: $X_i =$ Alcohol content of a wine sample
+ The conditional density function of $X_i | Y_i = k$ is normal:
$$
X_i | Y_i = 1 \sim N(\mu_1, \sigma_1^2)
$$
$$
X_i | Y_i = 2 \sim N(\mu_2, \sigma_2^2)
$$
$$
X_i | Y_i = 3 \sim N(\mu_3, \sigma_3^2)
$$

where we have possibly different means, $\mu_k$, and variance, $\sigma_k^2.$ 

The normal density function has the form

$$
f_{X|Y}(x|y) = \frac{1}{\sqrt{2\pi\sigma_k^2}}exp\left\{-\frac{1}{2\sigma_k^2}(x - \mu_k)^2   \right\}.
$$
To estimate the densities, we need to estimate the unknown mean and variance parameters:

$$
\widehat\mu_k = \mbox{sample mean of $X_i$'s from the $k$-th group}
$$
$$
\widehat\sigma_k^2  = \mbox{sample variance of $X_i$'s from the $k$-th group}
$$

```{r}
means_variances <- wine_small |>
  group_by(Class) |>
  summarize(means = mean(Alcohol), vars = var(Alcohol))
means_variances |>
  kable()
```

```{r, echo=FALSE}
# sample means
mn <- aggregate(Alcohol ~ Class, mean, data = wine_small)
xbar <- mn$Alcohol
xbar.1 <- xbar[1]
xbar.2 <- xbar[2]
xbar.3 <- xbar[3]

# sample sd
vr <- aggregate(Alcohol ~ Class, sd, data = wine_small)
sdev <- vr$Alcohol
var.1 <- sdev[1]^2
var.2 <- sdev[2]^2
var.3 <- sdev[3]^2

var.p <- ( (n1-1)*var.1 + (n2-1)*var.2 + (n3-1)*var.3)/(n1+n2+n3-3)
sd.p <- sqrt(var.p)
```

With the assumption of normality, the estimated density functions are
$$
\widehat f_k(x) = \frac{1}{\sqrt{2\pi\widehat\sigma_k^2}}exp\left\{-\frac{1}{2\widehat\sigma_k^2}(x - \widehat\mu_k)^2   \right\},
$$

as shown in the figure below.

```{r dda, fig.width=8, fig.height=5, echo=FALSE, fig.cap="Estimated distribution of Alcohol for the two groups.", fig.margin=T, fig.alt="Fitted Normal distributions for the alcohol variable are shown for each class of wine. The distributions show roughly the same spread of about 1 unit. Class 2's alcohol distribution haa a mean of 12, class 3's alcohol distribution has a mean of about 13, and class 1's alcohol distribution has a mean of about 13.75"}
# set up the grid over which to plot
Alcohol <- seq(10, 16, length.out = 101)

# Normal pdfs of two groups
f1 <- dnorm(Alcohol, mean = xbar[1], sd = sdev[1])
f2 <- dnorm(Alcohol, mean = xbar[2], sd = sdev[2])
f3 <- dnorm(Alcohol, mean = xbar[3], sd = sdev[3])
df <- data.frame(Alcohol, f1, f2, f3)
gg <- gather(df, "Class", "Density", -Alcohol)
ggplot(gg) + 
  geom_line(aes(Alcohol, Density, col = Class), lwd = 2) + 
  theme_bw(base_size = 18) + 
#  labs(col=guide_legend(title="Class")) + 
  scale_color_discrete(labels = 1:3)
# Plot the pdfs
#matplot(grid, cbind(f1, f2), lwd=3, type = "l", lty=1:2, col = c("#990000", "steelblue"), ylab = "Density", xlab = "Alcohol")
#abline(h=0, lwd=2, col="lightgray")
#points(alc[newclass==1], 0*alc[newclass==1], pch=19, cex=0.5)
#points(alc[newclass==2], 0*alc[newclass==2], pch=17, cex=0.5)
#legend(15, 0.7, legend = c("Group 1", "Group 2"), col = c("#990000", "steelblue"), lwd=2, lty=1)
```

We can now combine this with our prior probabilities calculated earlier to obtain the values we need to make our classifications:

$$\widehat{p}_Y(1)\widehat f_{X|Y}(x|1)$$
$$\widehat{p}_Y(2)\widehat f_{X|Y}(x|2)$$
$$\widehat{p}_Y(3)\widehat f_{X|Y}(x|3)$$

- For a given wine sample with Alcohol value, $x$, we will classify the sample to group 1 if it has the largest value above

    - Suppose we have new data $x = 12$. 

```{r}
# new data
x <- 12
# density evaluated at x
f <- dnorm(x, 
           mean = means_variances$means, 
           sd = sqrt(means_variances$vars))
# p_k * f_k
pf <- p_Y * f
pf_df <- data.frame(round(pf, 4))
names(pf_df) <- c("Class", "Numerator")
pf_df |>
  kable()
```

We see that Class 2 is our predicted class

- The posterior probabilities are

```{r}
post_prob <- pf / sum(pf)
post_prob_df <- data.frame(round(post_prob, 3))
names(post_prob_df) <- c("Class", "Prob")
post_prob_df |>
  kable()
```

Reconsidering the image showing the three Normal densities, we see that 12 falls most closely to the middle of Class 2's distribution and furthest from Class 1.

We can try to develop rules around this to understand the idea more clearly.

- We can see that we favor Class 1 over Class 2 if our $x$ (Alcohol) is greater than some value, call it $c$.

- To see this, we see that we favor Class 1 if $\widehat p_Y(1)\widehat f_{X|Y}(x|1) > \widehat p_Y(2)\widehat f_{X|Y}(x|2)$. This is equivalent to favoring class 1 if
$$
log(\widehat p_Y(1)) + log(\widehat f_{X|Y}(x|1)) > log(\widehat p_Y(2)) + log(\widehat f_{X|Y}(x|2))
$$
Substituting the functional form of $\widehat f_{X|Y}(x|y)$, we can rewrite the condition above as
$$
log(\widehat p_Y(1)) - log(\widehat\sigma_1) - \frac{(x - \widehat\mu_1)^2}{2\widehat\sigma_1^2} > log(\widehat p_Y(2)) - log(\widehat\sigma_2) - \frac{(x - \widehat\mu_2)^2}{2\sigma_2^2}.
$$
Define the functions 
$$
\widehat \delta_k(x) = log(\widehat p_Y(k)) - log(\widehat\sigma_k) - \frac{(x - \widehat\mu_k)^2}{2\sigma_k^2}, k = 1, 2, 3 
$$
Thus our classification rule is equivalent to assigning $x$ to class 1 if 
$$
\widehat \delta_1(x) > \widehat \delta_2(x)
$$
and 
$$
\widehat \delta_1(x) > \widehat \delta_3(x)
$$
The functions $\widehat \delta_k(x)$ are called *discriminant functions*. 

Now we can determine the decision boundaries with these functions. For instance, we can determine the value $c$ so that
$$
\widehat \delta_1(c) = \widehat \delta_2(c).
$$
(where $\widehat \delta_3(c)$ is smaller)

- Solving this requires solving quadratic equations!

- Assume these values are larger than $\delta_3(c)$, then solving gives us

$$
-\left(\frac{1}{2\widehat\sigma_1^2} - \frac{1}{2\widehat\sigma_2^2}\right)c^2 + \left(\frac{\widehat\mu_1}{\widehat\sigma_1^2} - \frac{\widehat\mu_2}{\widehat\sigma_2^2}\right)c - \left(\frac{\widehat\mu_1^2}{2\widehat\sigma_1^2} - \frac{\widehat\mu_2^2}{2\widehat\sigma_2^2} + log(\widehat p_Y(1)/\widehat p_Y(2)) - log(\widehat\sigma_1/\widehat\sigma_2)\right) = 0
$$

- This classification method is called **Quadratic Discriminant Analysis (QDA)**. 

    + The name is due to the fact that the discriminant functions, $\widehat\delta_k(x)$ are quadratic polynomials of $x$. 

- If we assumed the variances were the same across the Normal densities, the discriminant functions would be linear in $x$. This procedure is called **Linear Discriminant Analysis**

## Example: QDA with Multiple Predictors

Now let us consider the case where we have two predictors ($p=2$): $X_{i1}$ and $X_{i2}$

- To extend QDA, we need to generalize the normal distribution to two-dimensions. We call such a distribution a *bivariate  normal distribution*.

### Bivariate Normal Distribution

- Denote $X_i = (X_{i1}, X_{i2})^T$. $X_i$ is a *random vector*. 

    + We can define the mean as $\boldsymbol{\mu}$: 
$$
\boldsymbol{\mu} = E(X_i) = (E(X_{i1}), E(X_{i2}))^T = (\mu_1, \mu_2)^T
$$

    + To define the variance of the random vector we need to look at their individual variances, $\sigma_1^2 = var(X_{i1})$ and $\sigma_2^2 = var(X_{i2})$, as well as their covariance, $\sigma_{12} = cov(X_{i1}, X_{i2})$. 
    + In general, we use the *variance-covariance matrix* of $X_i$ to summarize the variability of $X_i$:
$$
\boldsymbol{\Sigma} = cov(X_i) = \begin{pmatrix} \sigma_1^2 & \sigma_{12} \\ 
\sigma_{12} & \sigma_2^2 \end{pmatrix},
$$

**Bivariate Normal Distribution**:

- The random vector $\boldsymbol{X} = (X_1, X_2)^T$ follows a bivariate normal (Gaussian) distribution with mean vector $\boldsymbol{\mu} = (\mu_1, \mu_2)^T$ and variance-covariance (positive definite) matrix $\boldsymbol{\Sigma}$ if its joint PDF is given by

$$f_{\boldsymbol{X}}(\boldsymbol{x}) = (2\pi)^{-1} |\boldsymbol{\Sigma}|^{-1/2} \exp\{ - (\boldsymbol{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) /2 \}$$
This is denoted by $\boldsymbol X \sim N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma})$

The shape of the PDF (and that of the scatterplot of a random sample generated from the distribution) is determined by $\boldsymbol{\Sigma}$. Generally, a bivariate normal distribution looks like a mound. If the covariance is near 0 then the hill is symmetric in both directions. If there is a large covariance, the hill gets stretched to be longer.


### Multivariate Normal Distribution

Of course there is a general multivariate version of this distribution.

- A random vector $\boldsymbol{X_i} = (X_{i1}, \ldots, X_{ip})^T$ is said for follow a multivariate normal distribution $N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, where $\boldsymbol{\mu}$ is a $p \times 1$ vector and $\boldsymbol{\Sigma}$ is positive definite matrix, if the PDF of $\boldsymbol{X}$ is 
$$f(\boldsymbol{x}) = (2\pi)^{-p/2} |\boldsymbol{\Sigma}|^{-1/2} \exp\{ - (\boldsymbol{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) /2 \}.$$
We can show that $E(\boldsymbol{X}) = \boldsymbol{\mu}$ and that $cov(\boldsymbol{X}) = \boldsymbol{\Sigma}$. 

### QDA with $p$ Predictors

To develop a classifier with $p$ predictors, look at the conditional distribution of the random vector $\boldsymbol{X}_i$ (containing the $p$ predictors)
$$
\boldsymbol{X}_i | Y = k \sim N(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k), \;\; k=1, \ldots, K,
$$
where $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ are the mean vector and variance-covariance matrix corresponding to class $k$. 

- As with our univariate predictor case, the true values of $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$ are unknown, we estimate these parameters as
$$
\hat{\boldsymbol{\mu}}_k = \mbox{ sample mean of group k}
$$
$$
\hat{\boldsymbol{\Sigma}} = \mbox{ sample variance-covariance matrix of group k}
$$

The rest of the process is exactly as before! 

- We need values for our 'prior' distributions, $\widehat p_Y(k)$

- We find the value of $\widehat p_Y(k) \widehat f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y=k)$

- We classify a new observation with data $\boldsymbol{x}$ to class $k$ if the estimated posterior probability of class $k$ is highest, or equivalently, if $\widehat p_Y(k) \widehat f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y=k)$ is highest. 

The discriminant functions are quadratic in each of the predictors in $\boldsymbol{x}$ and are given by 
$$
\widehat\delta_k(\boldsymbol{x}) = -\frac{1}{2}(\boldsymbol x - \widehat{\boldsymbol{\mu}}_k)^T\widehat{\boldsymbol{\Sigma}}_k^{-1}(\boldsymbol{x} - \widehat{\boldsymbol{\mu}}_k) -\frac{1}{2}|\widehat{\boldsymbol{\Sigma}}_k| + log(\widehat p_Y(k)).
$$

- As before, an equivalent classification rule can be constructed using $\widehat\delta_k(\x)$: 

    + Assign $\boldsymbol{x}$ to class $k$ where   $\widehat\delta_k(\boldsymbol{x})$ is largest among $\widehat\delta_1(\boldsymbol{x}), \ldots, \widehat\delta_K(\boldsymbol{x})$
    
This is the form of QDA for multiple predictors!

Of course we implement this using software!

### QDA Example Implementation

Let us consider the `wines` data again with all three classes, and two predictors, `Alcohol` and `Proline`.

- First let's do some of this ourselves!

    + The estimated mean vectors and variance-covariance matrices for the conditional bivariate normal distributions are shown below.

```{r}
X <- cbind(wines$Alcohol, wines$Proline)
mu <- vector("list")
Sigma <- vector("list")
for(ii in 1:3){
  mu[[ii]] <- colMeans(X[wines$Class == ii, ])
  Sigma[[ii]] <- cov(X[wines$Class == ii, ])
}
for (ii in 1:3){
  mu[[ii]] |>
    kable(col.names = paste0("Class ", ii, " Means"))
  Sigma[[ii]] |>
    kable(col.names = c("Alcohol", "Proline"), label = paste0("Class ", ii, " Variance/Covariance Matrix"))
}
```

+ The figure below shows the data for the three classes, overlayed with bivariate normal density contours for each class. 

```{r fullwine, echo=FALSE, fig.cap="Three class wines data overlayed with normal density contours.", message=FALSE, warning=FALSE, fig.width=8, fig.height=6, fig.margin = TRUE, fig.alt = "A scatterplot between Alcohol and Proline is shown. The alcohol values range from 11 to 15 and the proline values from 400 to 1600. The points are colored by the wine class. Three distinct groups are shown with class 2's values mostly being located around alcohol values from 11.5 to 13 and proline values from 400 to 1000. Class 3's values are mostly located around alcohol values of 12.5 to 14.5 and proline values from 500 to 800. Class 1's values re mostly located around alcohol values from 13 to 15 and proline values from 800 to 1600. Bivariate mormal distribution contours are overlayed on top of these showing roughly where the bivariate normal distributions are located. The mean of class 2's bivariate normal distribution is at alochol of 12.25 and proline of 550. The mean of class 3's bivariate normal distribution is at alcohol of 13.2 and proline of 700. The mean of class 1's bivariate normal distribution is at alcohol of 13.75 and proline of 1150."}
library(car)
lv <- c(0.25, 0.5, 0.75)
cl <- c("#990000", "darkgreen", "orange")
pc <- c(19, 15, 21)
plot(X, col = rep(cl, table(wines$Class)), 
     pch= rep(pc, table(wines$Class)),#19,
     xlab = "Alcohol", ylab = "Proline")
for(ii in 1:3){
dataEllipse(X[wines$Class == ii, ], 
            add = TRUE, levels = lv,
            col = cl[ii], lty=2,
            fill = TRUE, fill.alpha = 0.1, 
            center.pch = pc[ii],
            center.cex = 2, cex = 1)
}
legend(11, 1600, c("1", "2", "3"), title = "Class",
       pch = pc, col = cl, cex = 2)
```

+ As before, **for a given data point** $\boldsymbol{x}$, we can compute $\widehat p_Y(k) \widehat f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y=k)$, and the associated posterior probabilities.
    
```{r, warning=FALSE, message=FALSE}
# For multivariate normal density
library(mnormt)
# new data
newx <- data.frame(Alcohol = 13, Proline = 600)
# p-hat
p <- table(wines$Class)/nrow(wines)
# f-hat
f <- c()
for(ii in 1:3){
  f[ii] <- dmnorm(newx, mean = mu[[ii]], varcov = Sigma[[ii]])
}
# Posterior prob
post_prob <- p*f / sum(p*f)
post_prob_df <- data.frame(round(post_prob, 4))
names(post_prob_df) <- c("Class", "Probability")
post_prob_df |>
  kable()
```

- In general, we can use the `qda()` function in `MASS` package to build QDA models. 

```{r}
wine_qda <- qda(Class ~ Alcohol + Proline, data = wines)
wine_qda
# prediction of new x
pred <- predict(wine_qda, newdata = newx)
pred_df <- data.frame(round(pred$posterior, 4))
names(pred_df) <- c("Class 1 Probability", "Class 2 Probability", "Class 3 Probability")
pred_df |>
  kable()
```

The decision boundaries for this QDA are shown in the figure below. 

- Here the boundaries are quadratic functions of `Alcohol` and `Proline` since the discriminant functions are quadratic!  

```{r qdadis, echo=FALSE, fig.cap="Decision boundary of QDA when applied to wines data.", fig.alt = "A scatterplot of alcohol and proline is shown along with a decision boundary for the QDA classifier. The decision boundary for class 2 is formed by two curves: one roughly straight line from alcohol 11 and proline of 1600 to alcohol 12.7 and proline of 850. The second curve sprouts out of alcohol of 12.7 and proline of 850 heads down and to the left to roughly alcohol of 12.7 and proline of 700 before becoming mostly a straight line heading down and to the right, ending at alcohol of 13.7 and proline of 350. The decision boundary for class 3 is formed by the curve that sprouts out of alcohol of 12.7 and proline of 850 heads down and to the left to roughly alcohol of 12.7 and proline of 700 before becoming mostly a straight line heading down and to the right, ending at alcohol of 13.7 and proline of 350 and a mostly straight line from alcohol of 12.7 and proline of 850 to alcohol of 15 and proline of 800. The rest of the plot, the upper right area, corresponds to the class 1 region." }
drawparti(as.factor(wines$Class), 
          wines$Alcohol, 
          wines$Proline, 
          method = "qda", 
          prec = 100, 
          col.mean = cl, 
          gs = rep(pc, table(wines$Class)), 
          imageplot = FALSE, 
          lwd=2, 
          col.contour="black", 
          xlab = "Alcohol", 
          ylab = "Proline", 
          print.err = 0, 
          cex.mean = 2)
```

We can also estimate the test error rate of QDA when applied to wine data using data splitting methods such as CV or holdout. We can use `caret` to do so.

```{r}
set.seed(1001)
caret_qda <- train(factor(Class) ~ Alcohol + Proline,
                   data = wines,
                   method = "qda",
                   trControl = trainControl(method = "CV",
                                            number = 10))
caret_qda$results |>
  as.data.frame() |> 
  kable()
#columns represent the true value and rows the predicted value for each class
confusionMatrix(caret_qda)$table |>
  kable()
```


## Different Clasification Methods

Following similar logic to our QDA exploration, we could specify the densities in a different way, or put other assumptions on the parameters, to obtain other classification methods!

- For example, we might assume that the multivariate normal distributions have the *same variance-covariance matrix*, $\boldsymbol{\Sigma}$, in each class. 

    + Then it can be shown that the discriminant functions are *linear* in $\boldsymbol{x}$. 
$$
\widehat\delta_k(\boldsymbol{x}) = \boldsymbol{x}^T\widehat{\boldsymbol{\Sigma}}^{-1}\widehat{\boldsymbol{\mu}}_k - \frac{1}{2}\widehat{\boldsymbol{\mu}}_k^T\widehat{\boldsymbol{\Sigma}}^{-1}\widehat{\boldsymbol{\mu}}_k  + log(\widehat p_Y(k))
$$

    + The common variance-covariance matrix, $\boldsymbol{\Sigma}$, can be estimated by a "pooled" estimator: 
$$
\hat{\boldsymbol{\Sigma}} = \frac{(n_1 - 1)\boldsymbol{S}_1 + \ldots + (n_K - 1)\boldsymbol{S}_K}{n_1 + \ldots + n_K - K},
$$
where $\boldsymbol{S}_1, \ldots, \boldsymbol{S}_K$ are sample covariance matrices of $\boldsymbol{X}$'s from class $1, \ldots, K$, respectively.

The corresponding classification method is known as *Linear Discriminant Analysis (LDA)*, and the decision boundaries are linear!

In general, various classification methods specify or estimate the densities in different ways, giving rise to different classification rules. Some of these methods are shown below:

+ **Linear discriminant analysis** (LDA) (`lda` function in `MASS` library): use Gaussian densities with different means but *same* covariance matrix for each class

+ **Quadratic discriminant analysis** (QDA) (`qda` function in `MASS` library): use Gaussian densities with different means and *different* covariance matrices for each class

+ **Naive Bayes Classifier** (`NaiveBayes` function in `klaR` library): uses estimated density assuming that the inputs are conditionally independent in each class

+ **Regularized Discriminant Analysis** (`rda` function in `klaR` library): using regularized group covariance matrices that are robust against multicollinearity in the data

+ **Flexible discriminant analysis** (`fda` function in `mda` library): regerssion based classifier, captures nonlinear features of the covariates

+ **Mixture discriminant analysis** (`mda` function in `mda` library): density of each class is modeled using a *mixture* (weighted sum) of normal densities, can model multimodal densities

+ **Kernel Density Classification** (`kda` function in `ks` library): densities are estimated nonparametrically using kernel density estimation.
 
The figure below shows the decision boundaries of a few classification methods applied to `wines` data. 

- Keep in mind that there are *many* more discrimination analysis methods available in literature and in various R and python packages!

```{r cbd, echo=FALSE, fig.width=12, fig.height=3, warning=FALSE, message=FALSE, fig.fullwidth=T, fig.cap="Classification boundaries for four classifiers.", cache=TRUE, fig.margin = FALSE, fig.fullwidth = TRUE, fig.alt = 'The decision boundaries for four different classifiers are shown as applied to the wine data using alcohol and proline as predictors. The LDA model shows decision boundaries that are straight lines. The QDA model shows decision boundaries that are curved. The flexible discriminant analysis model show decision boundaries that are piecewise linear. The naive bayes model shows decision boundaries that are very wiggly lines. All of the decision boundaries roughly separate the predictor space into the same three regions.'}
set.seed(1001)
library(caret)
library(earth)
alc <- wines$Alcohol
pro <- wines$Proline
group <- as.factor(wines$Class)
colors <- rep(cl, table(wines$Class)) #c("#990000", "darkorange", "black")[group]
data <- cbind(pro, alc)

nbp <- 200;
PredA <- seq(min(alc), max(alc), length = nbp)
PredB <- seq(min(pro), max(pro), length = nbp)
Grid <- expand.grid(alc = PredA, pro = PredB)

df <- data.frame(alc = alc, pro = pro, group = group)

TrControl <- trainControl(savePredictions = TRUE)
par(mfrow = c(1,4))
Seed <- 345
Formula = "group ~ ."
Method = "lda"

Model <- train(as.formula(Formula), data = df, 
               method = Method, trControl = TrControl)
Pred <- predict(Model, newdata = Grid)

contour(PredA, PredB, matrix(as.numeric(Pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=2,
        xlab = "Alcohol", ylab = "Proline", main = "Linear DA")
points(df$alc, df$pro, pch=19, col=colors)

Method = "qda"
Model <- train(as.formula(Formula), data = df, 
               method = Method, trControl = TrControl)
Pred <- predict(Model, newdata = Grid)

contour(PredA, PredB, matrix(as.numeric(Pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=2,
        xlab = "Alcohol", ylab = "Proline", main = "Quadratic DA")
points(df$alc, df$pro, pch=19, col=colors)

Method = "fda"
V <- 10
T <- 4
TrControl <- trainControl(method = "repeatedcv",
                          number = V,
                          repeats = T
                          )
Model <- train(as.formula(Formula), data = df, 
               method = Method, trControl = TrControl)
Pred <- predict(Model, newdata = Grid)

contour(PredA, PredB, matrix(as.numeric(Pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=2,
        xlab = "Alcohol", ylab = "Proline", main = "Flexible DA")
points(df$alc, df$pro, pch=19, col=colors)

Method = "nb"
Model <- train(as.formula(Formula), data = df,
               method = Method, trControl = TrControl)
Pred <- predict(Model, newdata = Grid)

contour(PredA, PredB, matrix(as.numeric(Pred), ncol = nbp),
        drawlabels = F, col="red", nlevels = 2, lwd=2,
        xlab = "Alcohol", ylab = "Proline", main = "Naive Bayes")
points(df$alc, df$pro, pch=19, col=colors)
```


## LDA vs. QDA

Even though QDA can be considered more general method that LDA (due to the restrictive assumption made in LDA that each class has the same variance-covariance matrix), we might still prefer LDA in some situations over QDA.

- QDA requires estimation of a larger number of parameters than LDA. 

    + In our `wine` data example, suppose we use all $p=13$ predictors, with $K=3$ classes. 
    + QDA estimates $K$ variance-covariance matrix with size $p\times p$. 
    + Thus QDA estimates a total of $Kp(p+1)/2 = `r 3*(13*14)/2`$ parameters! 
    + In comparison, LDA requires estimation of only one common variance-covariance matrix. 
    + Since LDA is a less flexible model, it may have more bias but less variance. Thus sometimes LDA might have better prediction performance than QDA.  

Roughly speaking, LDA tends to be a better choice than QDA if training sample size is small and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the $K$ classes is clearly untenable.

A possible disadvantage of QDA/LDA is the assumption of multivariate normality of predictors in each class. When this assumption is unreasonable LDA/QDA can perform badly. 
However, assumption of normality is more crucial for QDA than LDA since another formulation of LDA does not require normality of the predictors. 

## Naive Bayes classifier

Recall that LDA and QDA estimate the densities $f_{\boldsymbol{X}|Y}(\cdot)$ for the $K$ classes using the multivariate normality assumption. 

The naive Bayes classifier makes a single simplifying assumption: 

- The joint density of $X_{i1}, \ldots, X_{ip}$ conditional on $Y$ is the product of $p$ individual conditional density functions,
$$
f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y) = f_{X_1|Y}(x_1|y) \times \ldots \times f_{X_p|Y}(x_p|y). 
$$

- In other words, within each class the $p$ predictors are assumed to be *independent*. 
- This is a quite strong assumption -- this will also imply that there is *no relation* (linear or otherwise) between the predictors *within each class*. 
- In most situations, this assumption is not appropriate. 
- However, a classifier can still be constructed based on this assumption, and it often gives good results (especially for smaller $n$)!

With this assumption, we only need to form estimates of the *marginal conditional density functions*, $\widehat f_{X_j|Y}(x_j|y), j = 1, \ldots, p$, to obtain an estimate of $f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y)$:
$$
\widehat f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y) = \widehat f_{X_1|Y}(x_1|y) \times \ldots \times \widehat f_{X_p|Y}(x_p|y).
$$

The rest of the procedure is the same as before: 

- We compute the posterior probabilities, or equivalently $\widehat p_Y(k) \widehat f_{\boldsymbol{X}|Y}(\boldsymbol{x}|y)$, and classify observations accordingly

We can form $\widehat f_{X_j|Y}(x_j|y)$ by following any of the options below:

+ If $X_{ij}$ is quantitative, we can assume $X_{ij} | Y_i = k \sim N(\mu_{kj}, \sigma^2_{kj})$. 

    + Then we only need to estimate $\mu_{kj}$ and $\sigma_{kj}^2$.
    + This is equivalent to running QDA with a \textit{diagonal variance-covariance matrix}

+ Another option for quantitative predictors is to estimate the densities with nonparametric methods.

    + Examples of such estimators are *relative frequency  histograms* and *kernel density estimators* -- a smoothed version of histogram. 

```{r, echo=FALSE, fig.cap="Relative frequancy histogram and kernel density estimator (solid line) of a sample.", fig.alt = "A histogram of the alcohol variable is shown with a smoothed density overlayed. The alcohol distribution looks roughly bell-shaped."}
Alcohol <- wines |> 
  filter(Class == 1) |>
  pull(Alcohol)
ggplot() + 
  geom_histogram(aes(x = Alcohol, 
                     y = after_stat(density)), 
                 bins = 15,
                 fill = "white", col = "red") +
  geom_density(aes(Alcohol), lwd=2)
```

+ If $X_{ij}$ is qualitative, then, conditional on the class $k$, we can simply take the proportion of sample observations for each value of the predictor. 

    + In other words, $\widehat f_{X_j|Y}(x_j|y=k)$ is the estimated *probability mass function* of the $j$-th predictor:
$$
\widehat f_{X_j|Y}(x_j|y=k) = \frac{1}{n_{\rm train, k}} \sum_i I(X_{ij} = x_j), 
$$
where $n_{\rm train, k}$ is the size of the training set for the $k$-th class.

In R, we can use the `NaiveBayes()` function in the `klaR` library for build a naive Bayes classifier. 

```{r, message=FALSE, warning=FALSE}
library(klaR)
nb_wine <- NaiveBayes(as.factor(Class) ~ Alcohol + Proline, 
                      data = wines,
                      usekernel = FALSE)
nb_kern <- NaiveBayes(as.factor(Class) ~ Alcohol + Proline, 
                      data = wines,
                      usekernel = TRUE)
pred_normal <- predict(nb_wine, newdata = data.frame(Alcohol = 13,
                                      Proline = 600))
pred_normal$posterior |> 
  kable()
pred_kernel <- predict(nb_kern, newdata = data.frame(Alcohol = 13,
                                      Proline = 600))
pred_kernel$posterior |>
  kable()
```

- In the first fit, we assumed normal distribution for each predictor (`usekernel = FALSE`)
- In the second fit, we use kernel density estimation (`usekernel = TRUE`). 

We can also use `caret` to evaluate test error with `method =  nb`!

Below we fit an LDA, QDA, and two forms of Naive bayes to a simple model for the wine data.

```{r, eval=TRUE, echo= TRUE}
tr <- trainControl(method = "repeatedcv", number = 10, repeats = 25)
lda <- train(as.factor(Class) ~ Alcohol + Proline, 
            data = wines,
            method = "lda",
            trControl = tr)
qda <- train(as.factor(Class) ~ Alcohol + Proline,
            data = wines,
            method = "qda",
            trControl = tr)
nb_g <- train(as.factor(Class) ~ Alcohol + Proline, 
            data = wines,
            method = "nb", 
            tuneGrid = expand.grid(usekernel = FALSE,
                                   fL = 0,
                                   adjust = 1),
            trControl = tr)
nb_k <- train(as.factor(Class) ~ Alcohol + Proline, 
            data = wines,
            method = "nb", 
            tuneGrid = expand.grid(usekernel = TRUE,
                                   fL = 0,
                                   adjust = 1),
            trControl = tr)
```

Here we can plot the accuracy and kappa values across the repeated test sets.

```{r, out.width = '400px', fig.cap="Accuracy and Kappa values for the four models on the wine data set.", fig.alt = "Accuracy estimates and kappa estimates are shown as boxplots for each of the four classifiers fit: QDA, LDA, Naive Bayes using a normal distribution, and Naive Bayes using a kernel density estimate. The distributions of each quantity are roughly similar with accuracies centered at 0.85 and kappa values centered at around 0.75"}
bwplot(resamples(list(LDA = lda, 
                      QDA = qda, 
                      NaiveB_gauss = nb_g,
                      NaiveB_kern = nb_k)))
```



# Comparison of a few classifiers

Chapter 4.5 of the textbook \textit{Introduction to Statistical Learning} provides a nice comparison of the different classifiers we learned. I recommend you read the chapter to get more insight. We will provide only a brief  overview of the discussion presented in the book.

Analytically, we can compare the form of log-odds of LDA, QDA, naive Bayes and logistic regression. We can show that:

+ LDA and logistic regression models the log-odds as a linear combination of the predictors. 

+ QDA models the log-odds as a quadratic function of the predictors.

+ Naive Bayes classifier model the log-odds as a sum of non-linear functions of the predictors. 

Any classifier with a linear decision boundary is a special case of naive Bayes. 

However, neither QDA nor naive Bayes is a special case of the other. 

Between LDA and logistic regression, we expect LDA to perform better if the assumptions for LDA are satisfied. 

The textbook \textit{Introduction to Statistical Learning} also provides numerical comparison between various classifiers discussed so far. 

In general, no single classifier performs best in every scenario. Their test performance depends on the underlying structure (distribution, variance-covariance patterns) of the the data. 

- If it often a good idea to build several classifiers, and evaluate them using their test error rate!
