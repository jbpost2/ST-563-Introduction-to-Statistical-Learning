---
title: "Logistic Regression"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(tidyverse)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(AppliedPredictiveModeling)
library(kableExtra)
library(nnet)
library(glmnet)
```


```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\pagebreak

We've now discussed the main ideas associated with the predictive modeling tasks:

- Regression task: 

    + numeric response variable
    + MSE or RMSE is the commonly used metric

- Classification task:

    + categorical response variable
    + accuracy, confusion matrix-based metrics, and log-loss are common metrics

Now we'll talk about other models that are commonly used for the classification task!

# Logistic Regression

We talked about two major ways to create classification models:

- Models that directly try to model the conditional class probabilities, $P(Y|X)$
- Generative models that model $X|Y$'s distribution and use Bayes' theorem to obtain estimated conditional class probabilities

Logistic regression takes the first approach.

- LR models the model the probabilities associated with the distribution of $Y|X$ as functions of the data vector $X$ without actually specifying any distribution of $X$.

Often, logistic regression models are used for their abilities to help us understand the role of the predictors in explaining the outcome (our discrimination task)

## Why not an MLR Model?

We might first ask, why can't we use our usual MLR model for the classification setting?


Consider just a binary response

- What is the mean of the response?

<!-- Here write out some 0 and 1's as the population values. Look at probability as the mean-->    
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Suppose you have a predictor variable as well, call it $X$
    
- Given two values of $X$ we could model separate proportions

$$E(Y|X=x_1) = P(Y=1|X = x_1)$$
$$E(Y|X=x_2) = P(Y=1|X = x_2)$$

- For a continuous $X$, we could consider a SLR model

$$E(Y|X) = P(Y=1|X) = \beta_0+\beta_1X$$

- What's wrong with this? Consider data about [heart disease](https://www4.stat.ncsu.edu/online/datasets/heart.csv)

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
heart_data <- read_csv("https://www4.stat.ncsu.edu/online/datasets/heart.csv") |>
  filter(RestingBP > 0) #remove one value
heart_data |> 
  dplyr::select(HeartDisease, everything()) 
#Cholesterol has many values set to 0 so we ignore that
```

- Heart Disease is our response variable. We can look at the mean of the binary variable to estimate $E(Y) = P(Y=1)$

```{r}
heart_data |>
  group_by(HeartDisease) |> 
  summarize(prop = n()/nrow(heart_data))
```

- Of course, we may think that these probabilities may differ based on the values of a predictor variable

```{r}
heart_data |>
  group_by(HeartDisease) |>
  summarize(mean_Age = mean(Age),
            mean_RestingBP  = mean(RestingBP))
```

We can try to fit our SLR model with `Age` as our predictor.

$$P(Y=1|Age) = E(Y|Age) = \beta_0+\beta_1Age$$

```{r, out.width = "400px", fig.align = 'center', message = FALSE, warning = FALSE}
ggplot(heart_data, aes(x = Age, y = HeartDisease, color = RestingBP)) +
         geom_point() +
  geom_smooth(method = "lm")
```

That doesn't quite look right... why? The points all lie on top of one another. Let's fix that by jittering them.

```{r, out.width = "400px", fig.align = 'center', message = FALSE, warning = FALSE}
ggplot(heart_data, aes(x = Age, y = HeartDisease, color = RestingBP)) +
         geom_jitter() +
  geom_smooth(method = "lm")
```

That's better but still not clear. We really are modeling probabilities (proportions) at each value `Age` can take on. We can visualize this with small `Age` group means.

- Obtain proportion with heart disease for different `Age` groups

```{r, out.width = '320px', fig.align = 'center'}
Age_x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 20)
heart_data_grouped <- heart_data |>
  mutate(Age_groups = cut(Age, breaks = Age_x)) |>
  group_by(Age_groups) |>
  summarize(HeartDisease_mean = mean(HeartDisease), counts = n())
heart_data_grouped
```

- Now plot that data as well!

```{r, out.width = '400px', fig.align = 'center'}
ggplot(data = heart_data, aes(x = Age, y = HeartDisease)) +
  geom_jitter(aes(color = RestingBP)) +
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_smooth(method = "lm", color = "Green")
```

Here SLR isn't actually terrible! However, theoretically we have some issues.

- Response = success/failure (0/1)

    + We are modeling the average number of successes for a given $X$. This is a probability!

        + predictions should never go below 0  
        + predictions should never go above 1  

The SLR model doesn't require this! We just got lucky. Consider a much wider set of `Age` values.

```{r, out.width = '400px', fig.align = 'center'}
ggplot(data = heart_data, aes(x = Age, y = HeartDisease)) +
  geom_jitter(aes(color = RestingBP)) +
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_smooth(method = "lm", color = "Green") +
  xlim(0, 100)
```

The predicted probability (the value of the line) will go below 0 and above 1 sometimes!

## Logistic Regression Model

- The basic Logistic Regression model uses a *logistic function* to model the success probability:

$$P(Y =1|X) = P(success|X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$$
 
This function never goes below 0 or above 1!

```{r, echo = FALSE, out.width = '400px', fig.align = 'center'}
x <- seq(0, 2, 0.01)
b0 <- -5
b1 <- 11
exp_fun <- function(x, b0, b1){exp(b0+b1*x)}
plot(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), ylim = c(0,1), xlim = c(0,2), xlab = "x", ylab= "P(Success|x)", col = "red", type = "l")
b0 <- -10
b1 <- 11
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "blue")
b0 <- -5
b1 <- 6
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "green")
b0 <- 10
b1 <- -11
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "purple")
legend(x = 1.25, y = 0.75, legend = c("b0 = -5, b1 = 11", "b0 = -10, b1 = 11", "b0 = -5, b1 = 6", "b0 = 10, b1 = -11"),
       col = c("red", "blue", "green", "purple"), lty = "solid", cex = 1)
```

This is actually a non-linear model! It is much harder to fit the model...

- The logistic regression model doesn't have a closed form solution (maximum likelihood often used to fit parameters)

### Interpreting the Model

How can we relate this to our usual linear model?

- Back-solving shows the *logit* or *log-odds* of success is linear in the parameters  

$$log\left(\frac{P(success|X)}{1-P(success|X)}\right) = \beta_0+\beta_1 X$$
- Coefficient interpretation changes greatly from linear regression model!  

    - $\frac{P(success|X)}{1-P(success|X)}$ is call the odds of success
    - $\beta_1$ represents a change in the log-odds of success  

Let's plot the logistic regression model that uses `Age` to predict `HeartDisease`:

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
log_reg_fit <- glm(factor(HeartDisease) ~ Age, data = heart_data, family = "binomial")
x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 200)
plot_df <- tibble(Age = x, Logistic_Pred = predict(log_reg_fit, newdata = data.frame(Age = x), type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred)) + 
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_line()
```

This is a sigmoid function that looks linear close up! Let's zoom out:

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
x <- seq(from = -0, to = 120, length = 2000)
plot_df <- tibble(Age = x, Logistic_Pred = predict(log_reg_fit, newdata = data.frame(Age = x), type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred)) + 
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_line()
```

Nice! 

### Including More Than One Predictor

In the MLR model we could easily add other numeric predictors, interactions, quadratics, and categorical predictors via the use of indicator variables. We can do this here too!

- Adding a dummy variable corresponding to a binary predictor just changes the 'intercept'

    - Call `Age` $X_1$
    - Let's add a predictor for `Sex` ($X_2 = 1$ if 'Male' and $X_2 = 0$ if "Female")

$$log\left(\frac{P(success|X)}{1-P(success|X)}\right) = \beta_0+\beta_1 X_1 + \beta_2X_2$$

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
Age_x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 20)
heart_data2 <- heart_data |>
  mutate(Age_groups = cut(Age, breaks = Age_x))
heart_data_grouped2 <- heart_data2 |>
  group_by(Age_groups, Sex) |>
  summarize(HeartDisease_mean = mean(HeartDisease), counts = n()) 
heart_data_grouped2 <- heart_data_grouped2[-39, ]
heart_data_grouped2$Age_x <- rep(seq(from = Age_x[1]+(Age_x[2]-Age_x[1])/2, to = max(Age_x)-(Age_x[2]-Age_x[1])/2, by = Age_x[2]-Age_x[1]), each = 2)
```


```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 200)
plot_df <- tibble(Age = rep(x, times = 2), Sex = factor(rep(c("F", "M"), length(x))),
                  Logistic_Pred = predict(log_reg_fit, 
                                          newdata = data.frame(Age = rep(x, times = 2), 
                                                               Sex = factor(rep(c("F", "M"), length(x)))
                                          ),
                                          type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred, color = Sex)) + 
  geom_point(data = heart_data_grouped2, aes(x = Age_x, y = HeartDisease_mean, size = counts, color = Sex)) +
  geom_line()
```

We can see that we don't have parallel lines in this case. Let's investigate the model fit a bit more. 

- Similar to the `lm()` function to fit MLR models, `glm()` with `family = "binomial"` allows us to fit logistic regression models.

```{r}
log_reg_fit <- glm(factor(HeartDisease) ~ Age + Sex, 
                   data = heart_data, family = "binomial")
summary(log_reg_fit)
```

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- If we fit an interaction term between `Age` and our `Sex` dummy variable, we essentially fit two separate logistic regression models

```{r}
log_reg_fit_int <- glm(factor(HeartDisease) ~ Age + Sex + Age*Sex, 
                   data = heart_data, family = "binomial")
summary(log_reg_fit_int)
```

- Let's check out the visual of the fits

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
plot_df_int <- tibble(Age = rep(x, times = 2), Sex = factor(rep(c("F", "M"), length(x))),
                  Logistic_Pred = predict(log_reg_fit_int, 
                                          newdata = data.frame(Age = rep(x, times = 2), 
                                                               Sex = factor(rep(c("F", "M"), length(x)))
                                          ),
                                          type = "response"))
ggplot(plot_df_int, aes(x = Age, y = Logistic_Pred, color = Sex)) + 
  geom_point(data = heart_data_grouped2, aes(x = Age_x, y = HeartDisease_mean, size = counts, color = Sex)) +
  geom_line()
```

- We can also include more than one numeric predictor and add in polynomial terms increases flexibility as well!

    - The plot below shows a model with a linear and quadratic term included
    
```{r, echo = FALSE, out.width = '200px', fig.align = 'center'}
x <- seq(-1, 5, 0.01)
b0 <- -1
b1 <- 3
b2 <- -1
exp_fun <- function(x, b0, b1, b2){exp(b0+b1*x+b2*x^2)}
plot(x, exp_fun(x, b0, b1, b2)/(1+exp_fun(x, b0, b1, b2)), ylim = c(0,1), xlim = c(-1, 5), xlab = "x", ylab= "P(Success|x)", col = "red", type = "l", main = "True Model: logit = -1 + 3*x -x^2")
```

### General Logistic Regression Model

Suppose we have a set of covariates $X = (X_{i1}, \ldots, X_{ip})^T$ and a binary response.  

- We can model the conditional (posterior) probabilities as follows: 
$$
P(Y_i = 1 | X) = \frac{exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)}{1 + exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)};
$$

- Since we have only two classes for $Y$, we know $P(Y=1|X) + P(Y=0|X) = 1$. This means we can write the following:

$$
P(Y_i = 0 | X) = 1 - P(Y_i = 1 | X) = \frac{1}{1 + exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)}.
$$
Another way to write the same model is using the *log-odds*:

$$
log\left[\frac{P(Y_i = 1 | X)}{1-P(Y_i = 1 | X)}\right] = log\left[\frac{P(Y_i = 1 | X)}{P(Y_i = 0 | X)}\right] = \beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p.
$$

- The parameters $\beta_0, \beta_1, \dots, \beta_p$ quantify the impact of the covariates to the prediction of $Y$

- The group used in the denominator (class 0 in our formulation above) is called the *reference group*. The choice of reference group is arbitrary as the estimates of the conditional (posterior) probabilities are same. 

### Fitting the Logistic Regression Model

For the MLR model we said we could use least squares or maximum likelihood to fit the model and the fit (estimated coefficients) would be the same.

- Here, the use of least squares doesn't make sense
- Maximum likelihood still works! However, we don't get a closed form solution as we did in the MLR case

    + We can show that the equation to find the 'best' $\beta$'s is equivalent to minimizing the negative log loss over the training data!

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

## Odds and log-odds

Let's investigate the concepts of *odds* and *log-odds* in more detail. 

Reconsider the `wines` data with $K=2$ classes (1 and 2) and only one covariate, $X = `Alcohol`$. 

- The odds are defined as$$
\frac{P(Y = 1 | X)}{P(Y = 2 | X)} = \frac{P(Y = 1 | X)}{1 - P(Y = 1 | X)}.$$

    - If $P(Y = 1 | X) = 0.1$ then the odds of class 1 are $0.1/0.9 = 1/9$. 
    - In contrast, if $P(Y = 1 | X) = 0.9$ then the odds are $0.9/0.1 = 9$

- Thus greater odds relate to higher posterior probability of class 1.

- In terms of log-odds,
$$
log\left[\frac{P(Y = 1 | X)}{P(Y = 2 | X)}\right] = \beta_0 +  X\beta_1.
$$
    + Model implies that for a one unit increase in $X$ (Alcohol), the log-odds will change by $\beta_1$ units
    + This change in log-odds does *not* depend on the value of $X$, just that it increases by 1
    + Equivalently, due to one unit increase in $X$, the *odds* gets multiplied by $e^{\beta_1}$.
    
$$
\frac{P(Y = 1 | X = x+1)}{P(Y = 2 | X = x+1)} = e^{\beta_1}\frac{P(Y = 1 | X = x)}{P(Y = 2 | X = x)}
$$
## Classification Using the Logistic Regression Model

We estimate our conditional probabilities using the fitted model:

$$
\widehat P(Y = 1 | X = x) = \frac{exp(\widehat\beta_0 + x_1\widehat\beta_1 + \ldots x_p\widehat\beta_p)}{1 + exp(\widehat\beta_0 + x_1\widehat\beta_1 + \ldots x_p\widehat\beta_p)};
$$
$$
\widehat P(Y = 2 | X = x) = 1 - \widehat P(Y = 1 | x),
$$

As with our $KNN$ model, we can predict the class for a particular observation via the following rule:

> The item is classified in group $Y = 1$ if $\widehat P(Y=1|x) \geq \widehat P(Y=2|x)$, otherwise in group $Y = 2$.

- Again, in the binary case, this is equivalent to 

$$\mbox{Assign to class 1 if }P(Y=1|x)>0.5$$

Logistic regression can be performed using the `glm()` function in base R. For demonstration purposes, let us consider only two classes (1 and 2), and two covariates `Alcohol` and `Proline`. We create the small wines data -- the only change is that now we explicitly change `Class` to a factor.
```{r}
# wine data for classes 1 and 2
wine_new <- wines[wines$Class == 1 | wines$Class == 2, ]
wine_new$Class <- as.factor(wine_new$Class)
wine_new$Class <- relevel(wine_new$Class, ref = 2)
```
\noindent Note that we called the function `relevel()` with the argument `"ref = 2"`. This is done to set class 2 as the reference group. Now we can perform logistic regression using the `glm()` function. 
```{r}
# Logistic regression
wine_glm =  glm(Class ~ Proline + Alcohol, 
               family = binomial(), 
               data = wine_new)
```
\noindent The first part `Class ~ Proline + Alcohol` is specifying `Class` as response and `Proline` and `Alcohol` as covariates. The statement `family = binomial()` is used to perform logistic regression.


The estimated coefficients are as follows.
```{r}
wine_coef <- wine_glm$coefficients
wine_coef
```

```{r, echo=FALSE}
cc <- wine_coef
```

We can interpret $\widehat\beta_0$ as the *log-odds* when both Proline and Alcohol levels are zero. Thus we have the corresponding odds of $`r exp(cc[1])`$. Keep in mind that in our dataset, zero values for Alcohol and Proline are not present, so such an interpretation is purely mechanical. 

The estimated value of $\beta_1$ can be interpreted as the amount log-odds will *change* due to one unit *increase*  in `Proline` while *keeping the `Alcohol` level fixed*. Thus keeping  `Alcohol` level fixed, one unit increase in `Proline` level is associated with $`r cc[2]`$ unit change in log-odds. Equivalently, odds will change by a multiplicative factor of $`r exp(cc[2])`$ (in other words, increase by $`r round(100*(exp(cc[2]) - 1), 3)`$ percent). Similar interpretation can be given for $\widehat\beta_2$. 


Suppose we have a new sample with `Proline = 600` and `Alcohol = 13`.  So here $x_1 = 600$ and $x_2 = 13$. We can compute the estimated posterior probabilities as
$$
\widehat P(Y = 1 | x = (600,13)) = \frac{e^{(`r cc[1]` + `r cc[2]`*600 + `r cc[3]`*13)}}{1 + e^{(`r cc[1]` + `r cc[2]`*600 + `r cc[3]`*13)}} = \frac{exp(`r cc[1] +  cc[2]*600 + cc[3]*13`)}{1 + exp(`r cc[1] +  cc[2]*600 + cc[3]*13`)} = `r exp( cc[1] +  cc[2]*600 + cc[3]*13)/(1 + exp(cc[1] +  cc[2]*600 + cc[3]*13))`,
$$
$$
\widehat P(Y = 2 | x = (600,13)) = 1 -  \widehat P(Y = 1 | x_1 = 600, x_2 = 13) = `r 1 - exp( cc[1] +  cc[2]*600 + cc[3]*13)/(1 + exp(cc[1] +  cc[2]*600 + cc[3]*13))`.
$$
Thus the new sample will be classified to class 2.

In R, we can simply use the `predict()` function to compute the probabilities shown above. By default, `predict()` gives the probability of *non-reference class*, class 1 in our example.

```{r}
newx <- data.frame(Proline = 600,
                   Alcohol = 13)
predict(wine_glm, 
        newdata = newx,
        type = "response")
```

We can also view the estimated posterior probabilities of the training set using `predict()` or using the `$fitted.values` component of the fit. The probabilities for the *non-reference group* is computed by default. We have also included the prediction of the whole dataset. Figure \ref{fig:postprob} shows the posterior probabilities along with the true class labels.   
```{r}
# Training set estimation of P(Y = 1)
post.prob.1 = wine_glm$fitted
# Training set estimation of P(Y = 2)
post.prob.2 = 1 - post.prob.1
# Predicted groups
Y.hat = as.factor(ifelse(post.prob.1>post.prob.2, 1, 2))
Y.hat <- relevel(Y.hat, ref = 2)
df <- data.frame("Class_1" = post.prob.1,
            "Class_2" = post.prob.2,
            "Predicted" = Y.hat,
            "Observed" = wine_new$Class)
head(df, 4)
```

\noindent As an example, for the first wine sample (1st row), $\hat P(Y = 1 | x) = 99.987\%$ and $\hat P(Y = 1 | x) = 0.013\%$. Thus this particular sample will be classified to group 1. Figure \ref{fig:lgdb} shows the decision boundary of the classifier, that is, all values of (`Alcohol`, `Proline`) that has the same posterior probability of being in class 1 or 2, or equivalently, odds of 1 and log-odds of 0. Formally, we have the boundary is given by all the solutions of the linear equation (the estimated formula of log-odds):
$$
`r cc[1]` + `r cc[2]`*Proline + `r cc[3]`*Alcohol = 0.
$$

```{r postprob, echo=FALSE, fig.cap="Esrimated posterior probabilities of class 1 along with the true class labels."}
#plot(df, pch = c(17, 19), col = wines$Class)
ggplot(df) + 
  geom_point(aes(x = Class_1, 
                 y = Observed,
                 col = Predicted,
                 pch = Predicted),
             size = 3) + 
  theme_bw(base_size = 18) + 
  xlab("Estimated P(Y = 1 | X)") + 
  theme(legend.position = "top")
```



```{r lgdb, echo=FALSE, fig.cap="Decision boundary of a 2-class logistic regression based classifier."}
cc <- wine_coef
newx <- expand.grid(Alcohol = seq(11, 15, len = 101),
                    Proline = seq(270, 1700, len = 101))
pp <- predict(wine_glm, newdata = newx, type = "response")
ggplot() + 
  geom_raster(aes(newx$Alcohol, 
                  newx$Proline, 
                  fill = pp))  + 
  theme_bw(base_size = 18) + 
  geom_contour(aes(newx$Alcohol, 
                   newx$Proline,
                   z = pp), lwd=1.2, bins=2, 
               col = "white") +  
  geom_point(aes(Alcohol, Proline,
                 col = Class, pch = Class), size = 2,
             data = wine_new) +
  labs(fill = "P(Y = 1 | X)", 
       x = "Alcohol",
       y = "Proline") +
  scale_fill_viridis_c(option = "B", guide = ) + 
  theme(legend.position = "top") + 
  geom_abline(slope = -cc[3]/cc[2], 
              intercept = -cc[1]/cc[2],
              col = "white")

```

We can use the function `errormatrix()` in the `klaR` package to obtain the training confusion matrix or, alternatively, we can also use the function `confusionMatrix()` in the `caret` package for a detailed output.]
```{r}
# Confusion matrix
err <- klaR::errormatrix(true = wine_new$Class, 
                         predicted = Y.hat, 
                         relative = TRUE)
round(err, 3)
```
\noindent Ideally, we should use cross-validation, training-testing sets to estimate the accuracy, as we have learned before.

## Hypothesis testing and confidence intervals

We can get a summary of the fit as follows.

```{r}
# testing each beta coefficient
summary(wine_glm)
```
```{r, echo=FALSE, eval=FALSE}
cat("Call:
glm(formula = class ~ proline + alcohol, family = binomial(), 
    data = datatwo)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.19220  -0.03196  -0.00558   0.02786   1.57716  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)   
(Intercept) -69.027638  22.757675  -3.033  0.00242 **
proline       0.013695   0.004362   3.140  0.00169 **
alcohol       4.453109   1.592916   2.796  0.00518 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 179.11  on 129  degrees of freedom
Residual deviance:  19.72  on 127  degrees of freedom
AIC: 25.72

Number of Fisher Scoring iterations: 9")
```
\noindent The summary of the fit produces $z$-tests for coefficient of each covariate^[This is actually an approximate (large sample) test.]; it seems both `alcohol` and `proline` are associated with the group labels. Formally, the test statistic is essentially same as in linear regression: suppose we want to test whether `Alcohol` (the second predictor with coefficient $\beta_2$) has any association with $Y$. Thus we test for $H_0:\beta_2 = 0$ vs. $H_1:\beta_2 \neq 0$. The test statistic is 
$$
z = \frac{\widehat\beta_2 - 0}{\widehat{SE}(\widehat\beta_2)}.
$$

```{r tpv, echo=FALSE, fig.margin = TRUE, fig.cap="Two-tailed p-value for a z-test.", fig.height=6, fig.width=6}

curve(dnorm(x), from = -3.5, to = 3.5, lwd=2, ylab = "N(0,1) PDF")
# shaded region
left <- 1.8
right <- 3.5
x  <- c(left, seq(left, right, 0.01), right)
y  <- c(0, dnorm(seq(left, right, 0.01)), 0)
polygon(x,y, col="steelblue")

left <- -3.5
right <- -1.8
x  <- c(left, seq(left, right, 0.01), right)
y  <- c(0, dnorm(seq(left, right, 0.01)), 0)
polygon(x,y, col="steelblue")
``` 

\noindent Using large sample theory it can be shown that $z$ approximately follows a $N(0,1)$ distribution. Thus we reject $H_0$ for large positive or large negative values of $z$. Equivalently, the p-value can be computed as
$$
p-value = 2*P(Z > |z|),
$$
where $Z$ denotes $N(0,1)$ random variable. Figure \ref{fig:tpv} shows the p-value of a two-sided $z$-test. 


```{r, echo=FALSE}
logss <- summary(wine_glm)
se <- logss$coefficients[,2]
int <- cc[2] + c(-1,1)*1.96*se[2]
```
We can produce large sample $100(1-\alpha)\%$ confidence intervals for $\beta_j$ as
$$
[\widehat\beta_j \pm z_{1 - \alpha/2} SE(\widehat\beta_j)],
$$
where $z_{1 - \alpha/2}$ is the $(1 - \alpha/2)$ quantile of the $N(0,1)$ distribution. For example, a $95\%$ confidence intervals for $\beta_1$ is
$$
[`r cc[2]` \pm 1.96*`r se[2]`] = [`r int[1]`, `r int[2]`].
$$
We can interpret this intervals as follows: with every unit increase in level of $X_1$ (`Proline` in our example), we can expect an *increase in log-odds* by an amount of `r int[1]` to `r int[2]`. Equivalently, every unit increase in level of $X_1$, *odds* will be changed by a factor of `r exp(int[1])` to `r exp(int[2])` (in other words, increase in odds will be between `r round(100*(exp(int[1])-1),3)` percent and `r  round(100*(exp(int[2])-1),3)` percent.)



## Logistic regression with multiple classes

We can extend logistic regression presented for two classes to the case of multiple classes; the regression method is called *Multinomial Logistic Regression*. Suppose we have $K$ classes, and we take the $K$-th class as the reference. The log-odds of classes vs the reference class $K$ are modeled as follows:
$$
log\left[\frac{P(Y = 1 | X_i)}{P(Y = K | X_i)}\right] = \beta_{10} + X_{i1}\beta_{11} + \ldots + X_{ip}\beta_{1p}.
$$
$$
log\left[\frac{P(Y = 2 | X_i)}{P(Y = K | X_i)}\right] = \beta_{20} + X_{i1}\beta_{21} + \ldots + X_{ip}\beta_{2p}.
$$
$$
\vdots
$$
$$
log\left[\frac{P(Y = K-1 | X_i)}{P(Y = K | X_i)}\right] = \beta_{K-1,0} + X_{i1}\beta_{K-1,1} + \ldots + X_{ip}\beta_{K-1,p}.
$$
Some algebra shows that the corresponding posterior probabilities are as follows:
$$
P(Y = k | X_i) = \frac{exp(\beta_{k0} + X_{i1}\beta_{k1} + \ldots + X_{ip}\beta_{kp})}{1 + \sum_{\ell = 1}^{K-1}exp(\beta_{\ell 0} + X_{i1}\beta_{\ell 1} + \ldots + X_{ip}\beta_{\ell p})}, \;\;\; k = 1, 2, \ldots, K-1,
$$
$$
P(Y = K | X_i) = \frac{1}{1 + \sum_{\ell=1}^{K-1}exp(\beta_{\ell 0} + X_{i1}\beta_{\ell 1} + \ldots + X_{ip}\beta_{\ell p})}.
$$



\noindent We can similarly build a classification rule as follows.

> An item with covariate $x$ is predicted to be in class $k$ if the estimated probability $\hat P(Y = k | x)$ is larger than the other posterior probabilities.


We can use the `multinom()` function in the `nnet` library to perform multinomial logistic regression. Let us consider the wine data with all the three classes.
```{r}
# Convert Class in wines data to a fcator
# and relevel to make 3 as reference
wines$Class <- as.factor(wines$Class)
wines$Class <- relevel(wines$Class, ref=3)
# multinomial logistic regression
multilogit <- multinom(Class ~ Proline + Alcohol, 
                       data = wines, 
                       maxit = 200, trace=FALSE)
# summary
summary(multilogit)
```
\noindent The option "trace = FALSE" in `multinom()` function suppresses the printing of convergence steps. The option "maxit = 200" sets the upper bound of the number of iterations to be performed to find the solutions.

The estimated posterior probabilities are as follows.
```{r}
# estimated posterior probabilities
probs <- multilogit$fitted.values
head(probs)
```


```{r trig, fig.width=6, fig.height=6, fig.margin=TRUE, fig.cap="Estimated posterior probailities for the wine data using multinomial logistic regression.", echo=FALSE}
# Plot the group probability
plot(probs[, 2], probs[, 3], col = wines[ , 1], 
ylim = c(0,1), xlim = c(0,1),
pch = 16, cex = 1.25,
xlab = "Estimated probability of Y=2",
ylab = "Estimated probability of Y=3",
main = "Probability of group membership")
legend(0.7, 1, legend = c("1", "2", "3 (ref)"), col = 1:3, pch=19, cex=1, title = "True Classes")
lines(c(0,0), c(1,0), lwd=2, col="grey", lty=2)
lines(c(0,1), c(0,0), lwd=2, col="grey", lty=2)
lines(c(0,1), c(1,0), lwd=2, col="grey", lty=2)
```


\noindent Each row shows the estimated probability of group membership for the corresponding wine sample. For example, the first wine sample (first row), has a 99.82% probability of being in group 1.  We can also visualize the posterior probabilities as shown in Figure \ref{fig:trig}. 

We can also hypothesis testing for each $\beta_{kj}$ using the standard errors reported in the summary output.

```{r}
# Extract the coefficients and se
mlogit_sum <- summary(multilogit)
coef <- mlogit_sum$coefficients
se <- mlogit_sum$standard.errors
# z-statistics
z <- coef/se
z
# p-value
pv <- 2*pnorm(abs(z), lower.tail = FALSE)
pv
```

\noindent We see for both classes 1 and 2, the two predictor variables are significant at any reasonable test level. 

```{r, echo=FALSE, eval=FALSE}
newx <- expand.grid(Alcohol = seq(11, 15, len = 101),
                    Proline = seq(270, 1700, len = 101))
pp <- predict(multilogit, newdata = newx, type = "probs")
ggplot() + 
  geom_raster(aes(newx$Alcohol, 
                  newx$Proline, 
                  fill = pp[,2]  )) +
  scale_fill_viridis_c(option = "D") + 
  theme_bw(base_size = 18) + 
  geom_contour(aes(newx$Alcohol, 
                   newx$Proline,
                   z = pp[,1]), lwd=1.2, bins=2, col = "red") + 
  geom_contour(aes(newx$Alcohol, 
                   newx$Proline,
                   z = pp[,2]), lwd=1.2, bins=2, col = "red") + 
  geom_contour(aes(newx$Alcohol, 
                   newx$Proline,
                   z = pp[,3]), lwd=1.2, bins=2, col = "red") + 
  geom_point(aes(Alcohol, Proline, col = Class), data = wines)

```


## Issues to consider

There are some situations where logistic regression might not perform well. One such situation is *complete (or quasi-complete) separation* of the data.

This situation happens when the outcome variable separates a predictor completely. This leads to perfect prediction of the outcome by the predictor. Consider the following data set with binary response $Y$ and two predictors $X_1$ and $X_2$. Figure \ref{fig:sep} shows relationship between $Y$ and the two predictors. In such a case, logistic regression may produce unreasonable over-inflated estimates of regression coefficients. 

```{r sep, echo=FALSE, fig.cap="Simulated data set. The response separates $X_1$ completely, but not $X_2$."}
set.seed(1001)
x1 <- rnorm(200)
x2 <- rnorm(200)
y <- as.factor((x1 < 0) + 1)
ggplot() + 
  geom_point(aes(x1, x2, col = y, pch = y), size = 2) + 
  theme_bw(base_size = 18) +
  geom_vline(xintercept = 0, lty=2)
```

```{r}
glm(y ~ x1 + x2, family = binomial())
```

In general, if there is a linear combination $Z = aX_1 + bX_2$ that is completely separated by $Y$, logistic regression will fail to produce reasonable results. Figure \ref{fig:septwo} shows one such example where the data is completely separated by the line $X_1 + X_2 = 0$. 


```{r septwo, echo=FALSE, fig.cap="Simulated data set. The response separates the data completely -- the boundary (dashed line) is $X_1 + X_2 = 0$. Negative values of $X_1 + X_2$ corresponds to $Y=2$, and positive values corresponds to $Y = 1$."}
set.seed(1001)
x1 <- rnorm(200)
x2 <- rnorm(200)
y <- as.factor((x1 + x2 < 0) + 1)
ggplot() + 
  geom_point(aes(x1, x2, col = y, pch = y), size = 2) + 
  theme_bw(base_size = 18) +
  geom_abline(intercept = 0, slope=-1, lty=2)
```

```{r}
glm(y ~ x1 + x2, family = binomial())
```


Although the examples above shows complete separation using continuous predictors, it is more like to happen when using categorical predictors coded by dummy variables. Small sample size might contribute to this problem as well.  In such situations, applying other classification methods (e.g., LDA) is preferred. 

Since logistic regression deals with binary outcome, often it requires more sample size that linear regression. Multinomial logit regression requires even more sample size than binary logistic  regression due to the fact that it estimates parameters for multiple classes. 

In presence of categorical predictor, it might happen that there are some combination of predictor and response values that are not present in the data. In such a case, logistic fit may become unstable, or might even fail to converge. 



As a practical example of perfect separability, consider the wines data with two classes, "1" and "2", but with all 13 predictors. Note that glm did not converge, and produces extremely inflated standard errors. 

```{r}
wines <- read.table("data/Wines.txt", header = TRUE)
wines_all <- glm(as.factor(Class) ~ ., 
                 data = wines[1:130,], 
                 family = binomial())
summary(wines_all)
```

```{r, echo=FALSE, fig.cap="Example of quasi-separable wines data.", eval=FALSE}
ggplot(wines[1:130,]) + 
  geom_point(aes(Color, Proline, 
                 col = as.factor(Class),
                 pch = as.factor(Class)), size = 2) + 
  theme_bw(base_size = 18) + 
  labs(col = "Class", pch = "Class")
```

```{r, echo=FALSE, eval=FALSE}
df <- data.frame(x = c(rep(0, 50), 
                       rep(1, 50), 
                       rep(2, 50)),
                 y = c(runif(50)<0.5, 
                       rep(0, 50), 
                       runif(50)<0.5))
```

```{r, echo=FALSE, eval=FALSE}
table(df)
glm(y ~ x, family = binomial())
```


# High-Dimensional Problems

When the number of predictors $p$ is larger than (or close to) the sample size $n$, the methods described in this section suffer from numerical instability or simply can not be applied to the data.^[Recall same issues in Linear Regression. ]   We can apply similar strategies discussed for linear regression here as well: regularization/shrinkage and dimension reduction methods. Some classifier such as naive Bayes are more appropriate in hig-dimensional data than others. 

For example, we can extend LDA for high-dimensional data by assuming a *diagonal* covariance matrix (i.e., assuming features are independent in each class). This method is called *Diagonal Linear Discriminant Analysis*. Similar approach can be taken for QDA as well resulting in *Diagonal Quadratic Discriminant Analysis*. Regularized versions of LDA such as *nearest shrunken centroids (NSC)*, *Regularized discriminant analysis (RDA)*, and many other methods are also available in literature. R packages such as `sparsediscrim`, `HDclassif`, `HiDimDA` among others have various classifiers for high-dimensional data. 


Like linear regression, we can develop ridge, lasso and elastic net methods for logistic regression as well. All these methods are available in  `glmnet()` package. As before, these methods shrink the regression coefficients towards zero, can be used in high-dimensional setting. We show the lasso based logistic regression fit of the two-class wines data (class 1 and 2) with the penalty parameter $\lambda$ chosen by CV below (`glmnet()` automatically scales the predictors before estimating the regression coefficients, and then outputs the coefficients in the original scale.)

```{r, warning=FALSE, message=FALSE}
set.seed(1102)
# CV to choose lambda
logit_cv <- cv.glmnet(x = as.matrix(wine_new[,-1]), 
             y = wine_new$Class,
             family = binomial(),
             alpha = 1)
```

```{r, echo=FALSE, fig.cap="CV results for logistic regression of wines data with lasso penalty."}
plot(logit_cv)
```

```{r}
# Final fit with lambda chosen by 1-SE rule
wine_lasso <- glmnet(x = as.matrix(wine_new[,-1]), 
             y = wine_new$Class,
             family = binomial(),
             alpha = 1,
             lambda = logit_cv$lambda.1se)
```


```{r}
# Estimated coefs
coef(wine_lasso)
```


Dimension reduction method like PCA can still be applied to the predictors before building classifiers. PCA often is useful in providing better visualization of the data. For example, the full wines data has 13 predictors, which are hard to visualize in a plot. Let us perform PCA and plot the first few PCs along with class labels.

```{r}
# wine data PCA
wine_pca <- prcomp(wines[, -1], scale. = TRUE)
wine_score <- wine_pca$x
```



```{r winepclda, echo=FALSE, fig.cap="LDA based classification of wines data using first two PCs.", fig.height=6}
df <- data.frame(wine_score, Class = as.factor(wines$Class))
# ggplot(df) + 
#   geom_point(aes(PC1, PC2, 
#                  col = Class,
#                  pch = Class),
#              size = 2) + 
#   theme_bw(base_size = 18)
# 
# ggplot(df) + 
#   geom_point(aes(PC1, PC3, 
#                  col = Class,
#                  pch = Class),
#              size = 2) + 
#   theme_bw(base_size = 18)
# 
# ggplot(df) + 
#   geom_point(aes(PC2, PC3, 
#                  col = Class,
#                  pch = Class),
#              size = 2) + 
#   theme_bw(base_size = 18)

cl <- c("#990000", "darkgreen", "orange")
pc <- c(19, 21, 15)
drawparti(df$Class, 
          df$PC1, 
          df$PC2, 
          method = "lda", 
          prec = 100, 
          col.mean = cl, 
          gs = rep(pc, table(wines$Class)), 
          imageplot = FALSE, 
          lwd=2, 
          col.contour="black", 
          xlab = "PC1", 
          ylab = "PC2", 
          print.err = 1, 
          cex.mean = 2)
legend(-4,-2, legend = 1:3, pch = pc, title = "Class")

```

\noindent Figure \ref{fig:winepclda} shows results from LDA using only the first two PCs as predictors. We can see that PC1 and PC2 almost completely separate the three classes.  Thus we have reduced dimension from 13 to two. Keep in mid that PCA is an unsupervised technique, and such a nice classification performance may not always happen. Also, even though PC1 and PC2  lead to excellent classification, together they only explain $55\%$ variation of the data. The opposite can be true as well -- the first two PCs  of some data might explain a large amount of variation but fail to produce good classification results. 

```{r}
summary(wine_pca)
```

