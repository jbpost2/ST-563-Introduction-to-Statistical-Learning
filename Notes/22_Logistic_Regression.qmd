---
title: "Logistic Regression"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(tidyverse)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(AppliedPredictiveModeling)
library(kableExtra)
library(nnet)
library(glmnet)
```


```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\pagebreak

We've now discussed the main ideas associated with the predictive modeling tasks:

- Regression task: 

&nbsp;  
&nbsp;  
&nbsp;  

- Classification task:

&nbsp;  
&nbsp;  
&nbsp;  

Now we'll talk about other models that are commonly used for the classification task!

# Logistic Regression

We talked about two major ways to create classification models:

- Models that directly try to model the conditional class probabilities, $P(Y|X)$
- Generative models that model $X|Y$'s distribution and use Bayes' theorem to obtain estimated conditional class probabilities

Logistic regression takes the first approach. Often, logistic regression models are used for their abilities to help us understand the role of the predictors in explaining the outcome (our discrimination task)

## Why not an MLR Model?

We might first ask, why can't we use our usual MLR model for the classification setting?


Consider just a binary response

- What is the mean of the response?

<!-- Here write out some 0 and 1's as the population values. Look at probability as the mean-->    
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Suppose you have a predictor variable as well, call it $X$
    
- Given two values of $X$ we could model separate proportions

$$E(Y|X=x_1) = P(Y=1|X = x_1)$$
$$E(Y|X=x_2) = P(Y=1|X = x_2)$$
\newpage

- For a continuous $X$, we could consider a SLR model

&nbsp;  
&nbsp;  
&nbsp;  


- What's wrong with this? Consider data about [heart disease](https://www4.stat.ncsu.edu/online/datasets/heart.csv)

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
heart_data <- read_csv("https://www4.stat.ncsu.edu/online/datasets/heart.csv") |>
  filter(RestingBP > 0) |> #remove one value
  mutate(HeartDiseaseFac = factor(HeartDisease))
heart_data |> 
  dplyr::select(HeartDiseaseFac, everything()) 
#Cholesterol has many values set to 0 so we ignore that
```

- Heart Disease is our response variable. We can look at the mean of the binary variable to estimate $E(Y) = P(Y=1)$

```{r}
heart_data |>
  group_by(HeartDiseaseFac) |> 
  summarize(prop = n()/nrow(heart_data))
```

- Of course, we may think that these probabilities may differ based on the values of a predictor variable

```{r}
heart_data |>
  group_by(HeartDiseaseFac) |>
  summarize(mean_Age = mean(Age),
            mean_RestingBP  = mean(RestingBP))
```

We can try to fit our SLR model with `Age` as our predictor.

$$P(Y=1|Age) = E(Y|Age) = \beta_0+\beta_1Age$$

```{r, out.width = "400px", fig.align = 'center', message = FALSE, warning = FALSE}
ggplot(heart_data, aes(x = Age, y = HeartDisease, color = RestingBP)) +
         geom_point() +
  geom_smooth(method = "lm")
```

That doesn't quite look right... why? The points all lie on top of one another. Let's fix that by jittering them.

```{r, out.width = "400px", fig.align = 'center', message = FALSE, warning = FALSE}
ggplot(heart_data, aes(x = Age, y = HeartDisease, color = RestingBP)) +
         geom_jitter() +
  geom_smooth(method = "lm")
```

That's better but still not clear. We really are modeling probabilities (proportions) at each value `Age` can take on. We can visualize this with small `Age` group means.

- Obtain proportion with heart disease for different `Age` groups

```{r, out.width = '320px', fig.align = 'center'}
Age_x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 20)
heart_data_grouped <- heart_data |>
  mutate(Age_groups = cut(Age, breaks = Age_x)) |>
  group_by(Age_groups) |>
  summarize(HeartDisease_mean = mean(HeartDisease), counts = n())
print(heart_data_grouped, n = 4)
```

- Now plot that data as well!

```{r, out.width = '400px', fig.align = 'center'}
ggplot(data = heart_data, aes(x = Age, y = HeartDisease)) +
  geom_jitter(aes(color = RestingBP)) +
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_smooth(method = "lm", color = "Green")
```

Here SLR isn't actually terrible! However, theoretically we have some issues.

- Response = success/failure (0/1)

    + We are modeling the average number of successes for a given $X$. This is a probability!

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


The SLR model doesn't require this! We just got lucky. Consider a much wider set of `Age` values.

```{r, out.width = '400px', fig.align = 'center'}
ggplot(data = heart_data, aes(x = Age, y = HeartDisease)) +
  geom_jitter(aes(color = RestingBP)) +
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_smooth(method = "lm", color = "Green") +
  xlim(0, 100)
```

The predicted probability (the value of the line) will go below 0 and above 1 sometimes!

Ideally, we should consider a model that requires this. One option is a logistic regression model.

\newpage

## Logistic Regression Model

- The basic Logistic Regression model uses a *logistic function* to model the success probability:

$$P(Y =1|X) = P(success|X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$$
 
This function never goes below 0 or above 1!

```{r, echo = FALSE, out.width = '400px', fig.align = 'center'}
x <- seq(0, 2, 0.01)
b0 <- -5
b1 <- 11
exp_fun <- function(x, b0, b1){exp(b0+b1*x)}
plot(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), ylim = c(0,1), xlim = c(0,2), xlab = "x", ylab= "P(Success|x)", col = "red", type = "l")
b0 <- -10
b1 <- 11
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "blue")
b0 <- -5
b1 <- 6
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "green")
b0 <- 10
b1 <- -11
lines(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), col = "purple")
legend(x = 1.25, y = 0.75, legend = c("b0 = -5, b1 = 11", "b0 = -10, b1 = 11", "b0 = -5, b1 = 6", "b0 = 10, b1 = -11"),
       col = c("red", "blue", "green", "purple"), lty = "solid", cex = 1)
```

This is actually a non-linear model! It is much harder to fit the model...

- The logistic regression model doesn't have a closed form solution (maximum likelihood often used to fit parameters)

### Interpreting the Model

How can we relate this to our usual linear model?

- Back-solving shows the *logit* or *log-odds* of success is linear in the parameters  

\newpage

- Coefficient interpretation changes greatly from linear regression model!

    - $\frac{P(success|X)}{1-P(success|X)}$ is call the odds of success
    - $\beta_1$ represents a change in the log-odds of success  

Let's plot the logistic regression model that uses `Age` to predict `HeartDisease`:

```{r, echo = FALSE, fig.align='center', out.width = '350px', message = FALSE, warning = FALSE}
log_reg_fit <- glm(HeartDiseaseFac ~ Age, data = heart_data, family = "binomial")
x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 200)
plot_df <- tibble(Age = x, Logistic_Pred = predict(log_reg_fit, newdata = data.frame(Age = x), type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred)) + 
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_line()
```

This is a sigmoid function that looks linear close up! Let's zoom out:

```{r, echo = FALSE, fig.align='center', out.width = '350px', message = FALSE, warning = FALSE}
x <- seq(from = -0, to = 120, length = 2000)
plot_df <- tibble(Age = x, Logistic_Pred = predict(log_reg_fit, newdata = data.frame(Age = x), type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred)) + 
  geom_point(data = heart_data_grouped, aes(x = Age_x, y = HeartDisease_mean, size = counts)) +
  geom_line()
```

\newpage

### Including More Than One Predictor

In the MLR model we could easily add other numeric predictors, interactions, quadratics, and categorical predictors via the use of indicator variables. We can do this here too!

- Adding a dummy variable corresponding to a binary predictor just changes the 'intercept'

    - Call `Age` $X_1$
    - Let's add a predictor for `Sex` ($X_2 = 1$ if 'Male' and $X_2 = 0$ if "Female")

$$log\left(\frac{P(success|X)}{1-P(success|X)}\right) = \beta_0+\beta_1 X_1 + \beta_2X_2$$

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
Age_x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 20)
heart_data2 <- heart_data |>
  mutate(Age_groups = cut(Age, breaks = Age_x))
heart_data_grouped2 <- heart_data2 |>
  group_by(Age_groups, Sex) |>
  summarize(HeartDisease_mean = mean(HeartDisease), counts = n()) 
heart_data_grouped2 <- heart_data_grouped2[-39, ]
heart_data_grouped2$Age_x <- rep(seq(from = Age_x[1]+(Age_x[2]-Age_x[1])/2, to = max(Age_x)-(Age_x[2]-Age_x[1])/2, by = Age_x[2]-Age_x[1]), each = 2)
```


```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
log_reg_fit <- glm(HeartDiseaseFac ~ Age +Sex, data = heart_data, family = "binomial")
x <- seq(from = min(heart_data$Age), to = max(heart_data$Age), length = 200)
plot_df <- tibble(Age = rep(x, times = 2), Sex = factor(rep(c("F", "M"), length(x))),
                  Logistic_Pred = predict(log_reg_fit, 
                                          newdata = data.frame(Age = rep(x, times = 2), 
                                                               Sex = factor(rep(c("F", "M"), length(x)))
                                          ),
                                          type = "response"))
ggplot(plot_df, aes(x = Age, y = Logistic_Pred, color = Sex)) + 
  geom_point(data = heart_data_grouped2, aes(x = Age_x, y = HeartDisease_mean, size = counts, color = Sex)) +
  geom_line()
```

We can see that we don't have parallel lines in this case. Let's investigate the model fit a bit more. 

- Similar to the `lm()` function to fit MLR models, `glm()` with `family = "binomial"` allows us to fit logistic regression models.

```{r}
log_reg_fit <- glm(HeartDiseaseFac ~ Age + Sex, 
                   data = heart_data, family = "binomial")
summary(log_reg_fit)
```

What are our fitted equations for Males? For Females?
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- If we fit an interaction term between `Age` and our `Sex` dummy variable, we essentially fit two separate logistic regression models

```{r}
log_reg_fit_int <- glm(HeartDiseaseFac ~ Age + Sex + Age*Sex, 
                   data = heart_data, family = "binomial")
summary(log_reg_fit_int)
```

- Let's check out the visual of the fits

```{r, echo = FALSE, fig.align='center', out.width = '400px', message = FALSE, warning = FALSE}
plot_df_int <- tibble(Age = rep(x, times = 2), Sex = factor(rep(c("F", "M"), length(x))),
                  Logistic_Pred = predict(log_reg_fit_int, 
                                          newdata = data.frame(Age = rep(x, times = 2), 
                                                               Sex = factor(rep(c("F", "M"), length(x)))
                                          ),
                                          type = "response"))
ggplot(plot_df_int, aes(x = Age, y = Logistic_Pred, color = Sex)) + 
  geom_point(data = heart_data_grouped2, aes(x = Age_x, y = HeartDisease_mean, size = counts, color = Sex)) +
  geom_line()
```

- We can also include more than one numeric predictor and add in polynomial terms increases flexibility as well!

    - The plot below shows a model with a linear and quadratic term included
    
```{r, echo = FALSE, out.width = '200px', fig.align = 'center'}
x <- seq(-1, 5, 0.01)
b0 <- -1
b1 <- 3
b2 <- -1
exp_fun <- function(x, b0, b1, b2){exp(b0+b1*x+b2*x^2)}
plot(x, exp_fun(x, b0, b1, b2)/(1+exp_fun(x, b0, b1, b2)), ylim = c(0,1), xlim = c(-1, 5), xlab = "x", ylab= "P(Success|x)", col = "red", type = "l", main = "True Model: logit = -1 + 3*x -x^2")
```

### General Logistic Regression Model

Suppose we have a set of covariates $X = (X_{i1}, \ldots, X_{ip})^T$ and a binary response.  

- We can model the conditional (posterior) probabilities as follows: 
$$
P(Y_i = 1 | X) = \frac{exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)}{1 + exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)};
$$

- Since we have only two classes for $Y$, we know $P(Y=1|X) + P(Y=0|X) = 1$. This means we can write the following:

$$
P(Y_i = 0 | X) = 1 - P(Y_i = 1 | X) = \frac{1}{1 + exp(\beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p)}.
$$
Another way to write the same model is using the *log-odds*:

$$
log\left[\frac{P(Y_i = 1 | X)}{1-P(Y_i = 1 | X)}\right] = log\left[\frac{P(Y_i = 1 | X)}{P(Y_i = 0 | X)}\right] = \beta_0 +  X_{i1}\beta_1 + \ldots + X_{ip}\beta_p.
$$

\newpage

### Fitting the Logistic Regression Model

For the MLR model we said we could use least squares or maximum likelihood to fit the model and the fit (estimated coefficients) would be the same.

- Here, the use of least squares doesn't make sense
- Maximum likelihood still works! However, we don't get a closed form solution as we did in the MLR case

    + We can show that the equation to find the 'best' $\beta$'s is equivalent to minimizing the negative log loss over the training data!
    + The derivation below is optional!
    
\newpage


# Inference on the Logistic Regression Model

## Classification Using the Logistic Regression Model

We estimate our conditional probabilities using the fitted model:

$$
\widehat P(Y = 1 | X = x) = \frac{exp(\widehat\beta_0 + x_1\widehat\beta_1 + \ldots x_p\widehat\beta_p)}{1 + exp(\widehat\beta_0 + x_1\widehat\beta_1 + \ldots x_p\widehat\beta_p)};
$$
$$
\widehat P(Y = 0 | X = x) = 1 - \widehat P(Y = 1 | x),
$$

As with our $KNN$ model, we can predict the class for a particular observation via the following rule:

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


Logistic regression can be performed using the `glm()` function in base R. **Make sure your response variable is a factor.** Use `family = "binomial"` to specify the logistic regression model.

- We use the same `formula` notation as before!

    + `response ~ predictors`
    + Separate predictor terms with `+`
    + `I()` or `poly()` for polynomial terms
    + `pred1*pred2` for an interaction
    + `pred1:pred2` for main effects and an interaction
    + `glm()` automatically creates dummy variables

Let's fit a logistic regression model for our `HeartDisease` response. We'll use `Age`, `RestingBP`, and `Sex` as predictors.

```{r}
# Logistic regression
heart_glm =  glm(HeartDiseaseFac ~ Age + RestingBP + Sex, 
               family = "binomial", 
               data = heart_data)
```

The estimated coefficients are as follows.
```{r}
heart_coef <- heart_glm$coefficients
heart_coef
```

```{r, echo=FALSE}
cc <- heart_coef
```

We can interpret $\widehat\beta_0$ as the *log-odds* when both `Age` and `RestingBP` are zero and `Sex` is Female. 

- We have $`r exp(cc[1])`$ odds of heart disease vs not having heart disease in this case. 
- Of course, zero values for `Age` and `RestingBP` don't make sense!

The estimated value of $\beta_1$ can be interpreted as the amount the log-odds will *change* due to one unit *increase*  in `Age` while *keeping the `RestingBP` level fixed* for a `Sex = Female`. 

- We expect a $`r cc[2]`$ unit change in log-odds. 
- Equivalently, our odds will change by a multiplicative factor of $`r exp(cc[2])`$ ($e^{\beta_1}$) 

    + In other words, increase by $`r round(100*(exp(cc[2]) - 1), 3)`$ percent ($100(e^{\beta_1}-1)$) 


Suppose we have a new sample with `Age = 60` and `RestingBP = 130`. We can compute the estimated conditional (posterior) probabilities of heart disease for Males and Females as follows:

$$
\mbox{Males: }\widehat P(Y = 1 | x = (60, 130, 1)) = \frac{e^{(`r cc[1]` + `r cc[2]`*60 + `r cc[3]`*130 + `r cc[4]`)}}{1 + e^{(`r cc[1]` + `r cc[2]`*60 + `r cc[3]`*130 + `r cc[4]`)}} = \frac{exp(`r cc[1] +  cc[2]*60 + cc[3]*130 +cc[4]`)}{1 + exp(`r cc[1] +  cc[2]*60 + cc[3]*130 +cc[4]`)} = `r exp( cc[1] +  cc[2]*60 + cc[3]*130 +cc[4])/(1 + exp(cc[1] +  cc[2]*60 + cc[3]*130 +cc[4]))`,
$$
$$
\mbox{Females: }\widehat P(Y = 1 | x = (60, 130, 0)) = \frac{e^{(`r cc[1]` + `r cc[2]`*60 + `r cc[3]`*130)}}{1 + e^{(`r cc[1]` + `r cc[2]`*60 + `r cc[3]`*130)}} = \frac{exp(`r cc[1] +  cc[2]*60 + cc[3]*130`)}{1 + exp(`r cc[1] +  cc[2]*60 + cc[3]*130`)} = `r exp( cc[1] +  cc[2]*60 + cc[3]*130)/(1 + exp(cc[1] +  cc[2]*60 + cc[3]*130))`,
$$
In R, we can simply use the `predict()` function to compute the probabilities shown above. 

```{r}
newx <- data.frame(Age = c(60, 60),
                   RestingBP = c(130, 130),
                   Sex = c("M", "F"))
predict(heart_glm, 
        newdata = newx,
        type = "response")
```

- We can see that a `Male` with these values of `Age` and `RestingBP` is classified as having heart disease but a female would not.

\newpage

Let's check out our confusion matrix. 

```{r}
# Confusion matrix
confusionMatrix(table(predicted = ifelse(predict(heart_glm, type = "response") > 0.5,
                                            1, 0),
                      heart_data$HeartDiseaseFac))
```

As we have only a couple of predictors, we can plot the decision boundary of the classifier!

- Here we need to find all values of `Age` and `RestingBP` that have the same conditional (posterior) probability of being heart disease or not (once for males and then once for females).

- The boundary (for females) is given by all the solutions of the linear equation (the estimated formula of log-odds):

$$
`r cc[1]` + `r cc[2]`*Age + `r cc[3]`*RestingBP = 0.
$$

```{r lgdb, echo=FALSE, fig.cap="Decision boundary of a 2-class logistic regression based classifier."}
cc <- heart_coef
newx <- expand.grid(Age = seq(28, 77, len = 201),
                    RestingBP = seq(80, 200, len = 201))
newx$Sex <- "F"
pp <- predict(heart_glm, newdata = newx, type = "response")
pp[newx$Age == 77 & newx$RestingBP == 200] <- 0.92
pp[newx$Age == 28 & newx$RestingBP == 80] <- 0.04
ggplot() + 
  geom_raster(aes(newx$Age, 
                  newx$RestingBP, 
                  fill = pp))  + 
  theme_bw(base_size = 12) + 
#  geom_contour(aes(newx$Age, 
#                  newx$RestingBP,
#                   z = pp), lwd=1.2, bins=2, 
#               col = "white") +  
  geom_point(aes(Age, RestingBP,
                 col = HeartDiseaseFac, shape = HeartDiseaseFac), size = 2,
             data = heart_data |> dplyr::filter(Sex == "F")) +
  labs(fill = NULL, 
       x = "Age",
       y = "RestingBP") +
  scale_color_manual( name = "Heart Disease",
                      labels = c("Yes", "No"),
                      values = c("#619CFF", "#F8766D")) +
  scale_shape_manual( name = "Heart Disease",
                      labels = c("Yes", "No"),
                      values = c(16, 17)) +
  scale_fill_viridis_c(option = "B", guide = guide_colorbar(title = "Predicted Probability")) + 
  geom_abline(slope = -cc[2]/cc[3], 
              intercept = -cc[1]/cc[3],
              col = "white")

```

Now the confusion matrix for Females:

```{r}
confusionMatrix(table(predicted = ifelse(predict(heart_glm, 
                                                 newdata = heart_data |> filter(Sex == "F"), 
                                                 type = "response") > 0.5,
                                         1, 0),
                      heart_data |> 
                        filter(Sex == "F") |> 
                        pull(HeartDiseaseFac)
                      ))
```

- The boundary (for males) is given by all the solutions of the linear equation (the estimated formula of log-odds):

$$
`r cc[1]` + `r cc[2]`*Age + `r cc[3]`*RestingBP + `r cc[4]`= 0.
$$

```{r , echo=FALSE, fig.cap="Decision boundary of a 2-class logistic regression based classifier."}
cc <- heart_coef
newx <- expand.grid(Age = seq(28, 77, len = 201),
                    RestingBP = seq(80, 200, len = 201))
newx$Sex <- "M"
pp <- predict(heart_glm, newdata = newx, type = "response")
pp[newx$Age == 77 & newx$RestingBP == 200] <- 0.92
pp[newx$Age == 28 & newx$RestingBP == 80] <- 0.04
ggplot() + 
  geom_raster(aes(newx$Age, 
                  newx$RestingBP, 
                  fill = pp))  + 
  theme_bw(base_size = 12) + 
#  geom_contour(aes(newx$Age, 
#                  newx$RestingBP,
#                   z = pp), lwd=1.2, bins=2, 
#               col = "white") +  
  geom_point(aes(Age, RestingBP,
                 col = HeartDiseaseFac, shape = HeartDiseaseFac), size = 2,
             data = heart_data |> dplyr::filter(Sex == "M")) +
  labs(fill = NULL, 
       x = "Age",
       y = "RestingBP") +
  scale_color_manual( name = "Heart Disease",
                      labels = c("Yes", "No"),
                      values = c("#619CFF", "#F8766D")) +
  scale_shape_manual( name = "Heart Disease",
                      labels = c("Yes", "No"),
                      values = c(16, 17)) +
  scale_fill_viridis_c(option = "B", guide = guide_colorbar(title = "Predicted Probability")) + 
  geom_abline(slope = -cc[2]/cc[3], 
              intercept = -(cc[1]+cc[4])/cc[3],
              col = "white")
```

Now the confusion matrix for Males:

```{r}
confusionMatrix(table(predicted = ifelse(predict(heart_glm, 
                                                 newdata = heart_data |> filter(Sex == "M"), 
                                                 type = "response") > 0.5,
                                         1, 0),
                      heart_data |> 
                        filter(Sex == "M") |> 
                        pull(HeartDiseaseFac)
                      ))
```

- Perhaps an interaction term would help us here!

Note: **Ideally, we should use cross-validation, training-testing sets to estimate the accuracy, as we have learned before!**


## Odds and log-odds

Let's investigate the concepts of *odds* and *log-odds* in more detail. 

Reconsider the heart disease data with $K=2$ classes (1 and 0) and only one covariate, $X =$ `Age`. 

- The odds are defined as$$
\frac{P(Y = 1 | X)}{P(Y = 0 | X)} = \frac{P(Y = 1 | X)}{1 - P(Y = 1 | X)}.$$

    - If $P(Y = 1 | X) = 0.1$ then the odds of class 1 are $0.1/0.9 = 1/9$. 
    - In contrast, if $P(Y = 1 | X) = 0.9$ then the odds are $0.9/0.1 = 9$

- Thus greater odds relate to higher posterior probability of class 1.

- In terms of log-odds,
$$
log\left[\frac{P(Y = 1 | X)}{P(Y = 0 | X)}\right] = \beta_0 +  X\beta_1.
$$
    + Model implies that for a one unit increase in $X$ (Age), the log-odds will change by $\beta_1$ units
    + This change in log-odds does *not* depend on the value of $X$, just that it increases by 1
    + Equivalently, due to one unit increase in $X$, the *odds* gets multiplied by $e^{\beta_1}$.
    
$$
\frac{P(Y = 1 | X = x+1)}{P(Y = 0 | X = x+1)} = e^{\beta_1}\frac{P(Y = 1 | X = x)}{P(Y = 0 | X = x)}
$$

## Hypothesis testing and confidence intervals

Reconsider our logistic regression model with `Age`, `RestingBP`, and `Sex`. We can get a summary of the fit as follows.

```{r}
summary(heart_glm)
```

- `summary()` of the fit produces $z$-tests for coefficient of each covariate

    + This is an approximate (large sample) test
    + We test whether $H_0: \beta_j = 0$ vs $H_A:\beta_j\neq 0$ after accounting for the other predictors in the model
    + The test statistic is 
$$
Z = \frac{\widehat\beta_j - 0}{\widehat{SE}(\widehat\beta_j)}.
$$
    + This statistics $Z$ approximately follows a $N(0,1)$ distribution under $H_0$. We reject for large (in absolute value) values.
    + P-value is
$$
p-value = 2*P(Z > |z|),
$$

```{r, echo=FALSE}
logss <- summary(heart_glm)
se <- logss$coefficients[,2]
int <- cc[2] + c(-1,1)*1.96*se[2]
```
We can produce large sample $100(1-\alpha)\%$ confidence intervals for $\beta_j$ as
$$
[\widehat\beta_j \pm z_{1 - \alpha/2} SE(\widehat\beta_j)],
$$
where $z_{1 - \alpha/2}$ is the $(1 - \alpha/2)$ quantile of the $N(0,1)$ distribution. For example, a $95\%$ confidence intervals for $\beta_1$ is
$$
[`r cc[2]` \pm 1.96*`r se[2]`] = [`r int[1]`, `r int[2]`].
$$

We can interpret this intervals as follows: 

- with every unit increase in the value of $X_1$ (`Age` in our example), we can expect an *increase in log-odds* by an amount of `r int[1]` to `r int[2]` (while holding `RestingBP` and `Sex` constant)
- Equivalently, every unit increase in the value of $X_1$, the *odds* will be changed by a factor of `r exp(int[1])` to `r exp(int[2])` (while holding `RestingBP` and `Sex` constant)

    + In other words, the increase in odds will be between `r round(100*(exp(int[1])-1),3)` percent and `r  round(100*(exp(int[2])-1),3)` percent

\newpage

# Logistic regression with multiple classes

We can extend logistic regression presented for two classes to the case of multiple classes.

- We call this a *Multinomial Logistic Regression* model. 

- Suppose we have $K$ classes, and we take the $K$-th class as the reference. The log-odds of classes vs the reference class $K$ are modeled as follows:

$$
log\left[\frac{P(Y = 1 | X_i)}{P(Y = K | X_i)}\right] = \beta_{10} + X_{i1}\beta_{11} + \ldots + X_{ip}\beta_{1p}.
$$
$$
log\left[\frac{P(Y = 2 | X_i)}{P(Y = K | X_i)}\right] = \beta_{20} + X_{i1}\beta_{21} + \ldots + X_{ip}\beta_{2p}.
$$
$$
\vdots
$$
$$
log\left[\frac{P(Y = K-1 | X_i)}{P(Y = K | X_i)}\right] = \beta_{K-1,0} + X_{i1}\beta_{K-1,1} + \ldots + X_{ip}\beta_{K-1,p}.
$$
Some algebra shows that the corresponding conditional (posterior) probabilities are as follows:
$$
P(Y = k | X_i) = \frac{exp(\beta_{k0} + X_{i1}\beta_{k1} + \ldots + X_{ip}\beta_{kp})}{1 + \sum_{\ell = 1}^{K-1}exp(\beta_{\ell 0} + X_{i1}\beta_{\ell 1} + \ldots + X_{ip}\beta_{\ell p})}, \;\;\; k = 1, 2, \ldots, K-1,
$$
$$
P(Y = K | X_i) = \frac{1}{1 + \sum_{\ell=1}^{K-1}exp(\beta_{\ell 0} + X_{i1}\beta_{\ell 1} + \ldots + X_{ip}\beta_{\ell p})}.
$$


We can similarly build a classification rule as follows.

> An item with covariate $x$ is predicted to be in class $k$ if the estimated probability $\hat P(Y = k | x)$ is larger than the other posterior probabilities.

\newpage

# Issues to consider

## Separation of Data

There are some situations where logistic regression might not perform well. One such situation is *complete (or quasi-complete) separation* of the data.

This situation happens when the outcome variable separates a predictor completely. This leads to perfect prediction of the outcome by the predictor. 

- Consider the following data set with binary response $Y$ and two predictors $X_1$ and $X_2$. 
- The figure below shows relationship between $Y$ and the two predictors. 
- In such a case, logistic regression may produce unreasonable over-inflated estimates of regression coefficients. 

```{r septwo, echo=FALSE, fig.cap="Simulated data set. The response separates the data completely -- the boundary (dashed line) is $X_1 + X_2 = 0$. Negative values of $X_1 + X_2$ corresponds to $Y=2$, and positive values corresponds to $Y = 1$."}
set.seed(1001)
x1 <- rnorm(200)
x2 <- rnorm(200)
y <- as.factor((x1 + x2 < 0) + 1)
ggplot() + 
  geom_point(aes(x1, x2, col = y, pch = y), size = 2) + 
  theme_bw(base_size = 18) +
  geom_abline(intercept = 0, slope=-1, lty=2)
```

```{r}
glm(y ~ x1 + x2, family = binomial())
```

In general, if there is a linear combination $Z = aX_1 + bX_2$ that is completely separated by $Y$, logistic regression will fail to produce reasonable results! Figure \ref{fig:septwo} shows one such example where the data is completely separated by the line $X_1 + X_2 = 0$. 

- Although the examples above shows complete separation using continuous predictors, it is more like to happen when using categorical predictors coded by dummy variables. Small sample size might contribute to this problem as well.  

    + In such situations, applying other classification methods (e.g., LDA) is preferred. 

## Sample Size

Since logistic regression usually deals with binary outcome, often it requires a larger sample size that linear regression. 

Multinomial logistic regression requires even more sample size than binary logistic regression due to the fact that it estimates parameters for multiple classes. 


## Combinations Present in the Data

In the presence of categorical predictors, it might happen that there are some combinations of predictor and response values that are not present in the data. 

In such a case, logistic fit may become unstable, or might even fail to converge. 

\newpage

# High-Dimensional Problems

When the number of predictors $p$ is larger than (or close to) the sample size $n$, the methods described in this section suffer from numerical instability or simply can not be applied to the data. We saw these issues in the MLR case as well!

- We can apply similar strategies discussed for linear regression here as well: regularization/shrinkage and dimension reduction methods. 

- Like linear regression, we can develop ridge, lasso and elastic net methods for logistic regression. 
- All these methods are available in  `glmnet()` package. 

    + As before, these methods shrink the regression coefficients towards zero and can be used in high-dimensional setting. 
    + We show the lasso based logistic regression fit of the heart disease data with the penalty parameter $\lambda$ chosen by CV below.
    + We only include numeric predictors here but could add in our own dummy variables for each categorical predictor
    + Note: **`glmnet()` automatically scales the predictors before estimating the regression coefficients, and then outputs the coefficients in the original scale**

```{r, warning=FALSE, message=FALSE}
set.seed(1102)
# CV to choose lambda
logit_cv <- cv.glmnet(x = as.matrix(heart_data |> 
                                      dplyr::select(-starts_with("Heart"), -Sex, -ChestPainType, -RestingECG, -ExerciseAngina, -ST_Slope)), 
             y = heart_data$HeartDiseaseFac,
             family = binomial(),
             alpha = 1)
```

```{r, echo=FALSE, fig.cap="CV results for logistic regression of wines data with lasso penalty."}
plot(logit_cv)
```

```{r}
# Final fit with lambda chosen by 1-SE rule
heart_lasso <- glmnet(x = as.matrix(heart_data |> 
                                      dplyr::select(-starts_with("Heart"), -Sex, -ChestPainType, -RestingECG, -ExerciseAngina, -ST_Slope)),
             y = heart_data$HeartDiseaseFac,
             family = binomial(),
             alpha = 1,
             lambda = logit_cv$lambda.1se)
```

```{r}
# Estimated coefs
coef(heart_lasso)
```


Dimension reduction method like PCA can still be applied to the predictors before building classifiers!