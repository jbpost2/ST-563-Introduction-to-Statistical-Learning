---
title: "Support Vector Machines"
author: "Arnab Maity (modified by Justin Post)"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---


```{r setup, include=FALSE, message=FALSE}
#library(MASS)
#library(klaR)
library(tidyverse)
library(caret)
#library(rsample)
library(ISLR2)
library(knitr)
#library(AppliedPredictiveModeling)
#library(kableExtra)
library(klaR)
library(kernlab)
library(e1071)
```

```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\newpage

The support vector machine (SVM) is a family of classification rules that contain both parametric (e.g., linear) and nonparametric (e.g., kernel based) methods. It is often considered as one of the ready-to-use classifiers. It can be viewed as a generalization of linear decision boundaries for classification. The method can also be applied to regression problems. 

SVM produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space. We will mainly discuss three approaches: 

- maximal margin classifiers and the support vector classifiers
- support vector machines

People often loosely refer to any of these methods collectively as *support vector machines*.


# Maximal Margin Classifiers & Support Vector Classifiers

The maximal margin classifier is a special case of the support vector classifier. It provides a useful starting point to understand SVMs generally.

## Maximal Margin Classifier

We know that LDA and logistic regression both estimate linear decision boundaries (using different approaches) in classification problems. SVMs are built to try to separate data into classes in an optimal way.

Consider a simulated dataset that contains two predictors, $X_1$ and $X_2$, and observations come from one of two classes (blue circles and orange triangles).

```{r svm1, echo=F, fig.height=4.5, fig.width=5, fig.margin=TRUE, fig.cap="Scatterplot between two simulated variables x1 and x2 where the data points are colored by our response, the class type. The classes are completely separable by a line between x1 and x2."}
set.seed(1)
n <- 10
div <- 0.5

x1 <- runif(10, min=0, max=div-0.1)
y1 <- runif(10)

x2 <- runif(10, min = div + 0.1, max = 1)
y2 <- runif(10)

all <- data.frame(x1 = c(x1, x2), x2 = c(y1, y2), group = rep(1:2, each=n))

th <- pi/3
rotmat <- cbind( c(cos(th), sin(th)), c(-sin(th), cos(th)) )
dd <- cbind(all$x1, all$x2)%*%t(rotmat)  
all$x1 <- dd[,1]
all$x2 <- dd[,2]
  
plot(all$x1, all$x2, pch=c(19, 17)[all$group], 
     col = c("steelblue", "darkorange")[all$group], cex=2,
     xlab = "X1", ylab = "X2")
```

The two classes are well separated, and a straight line can be used for classification. This situation is called *linearly separable*. In the two predictor case that means:

There exists $\beta_0, \beta_1, \beta_2$ such that the line
$$
\beta_0 + X_1\beta_1 + X_2\beta_2 = 0
$$
perfectly separate the two classes. 

Here, this line is not unique!

For SVM, we actually define our binary response by coding it as a -1 or 1 variable. With separable classes, this implies:

for a data point with predictor values $(X_{i1}, X_{i2})$, we have the relation

$$
Y_i = \begin{cases} -1  & \mbox{ if } \beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 < 0\\
 1  & \mbox{  if } \beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 > 0 
 \end{cases}
$$
We can therefore define our $Y_i$ by 

$$
Y_i = sign(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2).
$$

- $\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2$ is called a **hyperplane** thus the name **separating hyperplane**.
- Note that $Y_i(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2) > 0$
- Once we fit our $\beta$ terms, we can classify a new observation via
$$\hat{Y} = sign(\hat{\beta}_0 + X_{i1}\hat{\beta}_1 + X_{i2}\hat{\beta}_2)$$


### Fitting the Hyperplane

When the problem is **linearly separable**, there are infinitely many such separating hyperplanes. Two are shown in the plot of our simulated data below.

```{r, echo=FALSE, fig.cap="Simulated two class data with two possible separating hyperplanes (lines) that perfectly separate the data.", fig.height = 4.5, fig.width = 5}
plot(all$x1, all$x2, pch=c(19, 17)[all$group], 
     col = c("steelblue", "darkorange")[all$group], cex=2,
     xlab = "X1", ylab = "X2")
abline(a = 0.65, b = -0.15, lwd=2)
abline(a = 0.65, b = -0.45, lwd=2, col = "red")
```

How to determine the 'optimal' line in this case? 

- Given any separating hyperplane, we can define the *margin* as the minimal distance from each observation to the hyperplane.
- Define $M_i =$ distance between the hyperplane and $i$-th training point. 
- The `margin` is defined as $M = min(M_1,\ldots, M_n)$. 

The optimal classification rule is the line that *maximizes the margin* around the separating line.  Such a classifier is called **the maximal margin classifier**. 

Formally, we consider the optimization problem:
$$
\underset{\beta_0, \beta_1, \beta_2, M}{max} M \mbox{ subject to } \left\{ \begin{matrix}Y_i(\beta_0 + X_{i1}\beta_1 +  X_{i2}\beta_2) \geq M, \; i = 1, \ldots, n, \\ \beta_1^2 + \beta_2^2 = 1 \end{matrix}\right.
$$
The two conditions ensure that, when $M > 0$, each observation will remain on the correct side of the boundary and is at least distance $M$ from the boundary. 

It can be shown that the optimization problem above is equivalent to the following optimization problem:
$$
\underset{\beta_0,\beta_1,\beta_2}{min} (\beta_1^2 + \beta_2^2) \mbox{ subject to } Y_i(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2) \geq 1, \;  i = 1, \ldots, n,
$$

To understand the geometry behind the optimization problem above, we note that for perfectly separable data, 

- $Y_i(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_{i2}) > 0$ for each $i$. 
- The constraint $Y_i(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2) \geq 1$ implies $\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 \geq 1$ if $Y_i = 1$, and that $\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 \leq -1$ if $Y_i = -1$.  

The constraints therefore form an **empty slab** or **margin** around the linear decision boundary. The thickness of the margin is $1/(\beta_1^2 + \beta_2^2)^{1/2}$. 

- Hence we are choosing $\hat{\beta}_0, \hat{\beta}_1,$ and $\hat{\beta}_2$ to maximize the thickness
- Equivalently this is just minimizing $(\hat{\beta}_1^2 + \hat{\beta}_2^2)$. 

The boundaries of this empty slab/margin are determined by the points that exactly satisfies the condition
$$
\hat{\beta}_0 + X_1\hat{\beta}_1 + X_2\hat{\beta}_2 = \pm 1. 
$$

Define the *optimal* separating hyperplane as $\widehat f(x) = \widehat\beta_0 + x_1 \widehat\beta_1 + x_2\widehat\beta_2$. Thus our classification rule is as follows: 

For a new observation $x = (x_1, x_2)$, 
$$
\widehat Y = sign\{\widehat f(x)\}.
$$

In figure below, the optimal separating line is shown as the solid red line, the closest points to the line are circled, and the separation between the classes is shown using the dashed black lines.

```{r, echo=F, fig.height=4.5, fig.width=5, fig.margin=TRUE, fig.cap="The scatterplot of our linearly separable simulated data is shown with the optimal line between the observations. Dashed black lines running parallel to the optimal line show the thickness of the margin. Three points fall directly on these dashed black lines. One point from the first class touches one dashed line and two points from the other class touch the other dashed line."}
set.seed(1)
n <- 10
div <- 0.5

x1 <- runif(10, min=0, max=div-0.1)
y1 <- runif(10)

x2 <- runif(10, min = div + 0.1, max = 1)
y2 <- runif(10)

all <- data.frame(x1 = c(x1, x2), x2 = c(y1, y2), group = rep(1:2, each=n))

th <- pi/3
rotmat <- cbind( c(cos(th), sin(th)), c(-sin(th), cos(th)) )
dd <- cbind(all$x1, all$x2)%*%t(rotmat)  
all$x1 <- dd[,1]
all$x2 <- dd[,2]

#plot(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group])
#abline(v=0.5, lwd=2, lty=2)


sv <- ksvm(group ~., data = all, type = "C-svc", kernel = "polydot", 
           kpar = list(offset = 0), C = 40)

nbp <- 400;
PredA <- seq(min(all$x1), max(all$x1), length = nbp)
PredB <- seq(min(all$x2), max(all$x2), length = nbp)
Grid <- expand.grid(x1 = PredA, x2 = PredB)
out <- predict(sv, Grid)

contour(PredA, PredB, matrix(as.numeric(out), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 1, lwd=2,
        xlab = "X1", ylab = "X2")
points(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group], cex=2)

vec <- alphaindex(sv)[[1]]
points(all$x1[vec], all$x2[vec], cex=3, col="red")
points(all$x1[vec], all$x2[vec], pch = c(19, 17)[all[vec, 3]], col = c("steelblue", "darkorange")[all[vec, 3]], cex=2)

sl <- (all$x2[17] - all$x2[14])/(all$x1[17] - all$x1[14])
int <- (all$x2[14] - sl*all$x1[14])
abline(a = int, b = sl, lwd=2, lty=2)

newint <- all$x2[7] - sl*all$x1[7]
abline(a = newint, b = sl, lwd=2, lty=2)

#abline(a = int-.25, b = sl-0.4, lwd=2, lty=1, col="darkgray")
#abline(a = int-.36, b = sl-0.4, lwd=2, lty=2, col="darkgray")
#abline(a = int-0.2+0.05, b = sl-0.4, lwd=2, lty=2, col="darkgray")
```


Essentially, the separating line corresponding to the maximal margin classifier represents the middle line of the widest space that we can fit between the two classes.  

Although none of the training observations fall in the margin (by construction), this will not necessarily be the case for test observations. The intuition is that a large margin on the training data will lead to good separation on the test data. 

### Support Vectors

In the plots, we noticed that there were three points that are closest, and equidistant, to the optimal separating line. 

These points exactly satisfy the condition
$$
Y_i(\beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2) = 1,
$$

These points are called the **support vectors** for this problem. 

- It can be shown that *only the support vectors are enough to define the optimal classification rule fully*. 
- If we move the support vectors, the optimal separating line also changes. 
- A movement to any of the other observations would not affect the separating hyperplane, provided that the observationâ€™s movement does not cause it to cross into this margin area. 
- This property of the maximal margin classifier is important in development of support vector classifier and support vector machine!

### $p$ Predictors

Everything above generalizes to the case where we have $p$ predictors!


## Support vector classifiers

Of course, our data isn't usually going to be linearly separable! Consider a more realistic example.

```{r svmns, echo=F, fig.height=4.5, fig.width=5, fig.margin=TRUE, fig.cap="Scatterplot of simulated data that are not completely separable. The optimal separating hyperplane (line) is shown with some observations from the first class on both sides of the line. Similarly, some observations from the other class are on both sides of the line."}
library(kernlab)
set.seed(1)
n <- 10
div <- 0.5

x1 <- runif(n, min=0, max=div+0.1)
y1 <- runif(n)

x2 <- runif(n, min = div - 0.1, max = 1)
y2 <- runif(n)

all <- data.frame(x1 = c(x1, x2), x2 = c(y1, y2), group = rep(1:2, each=n))

th <- pi/3
rotmat <- cbind( c(cos(th), sin(th)), c(-sin(th), cos(th)) )
dd <- cbind(all$x1, all$x2)%*%t(rotmat)  
all$x1 <- dd[,1]
all$x2 <- dd[,2]
  
#plot(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group])
#abline(v=0.5, lwd=2, lty=2)


sv <- ksvm(group ~., data = all, type = "C-svc", kernel = "polydot", 
           kpar = list(offset = 0), C = 100)

nbp <- 400;
PredA <- seq(min(all$x1), max(all$x1), length = nbp)
PredB <- seq(min(all$x2), max(all$x2), length = nbp)
Grid <- expand.grid(x1 = PredA, x2 = PredB)
out <- predict(sv, Grid)

contour(PredA, PredB, matrix(as.numeric(out), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 1, lwd=2,
        xlab = "X1", ylab = "X2")
points(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group], cex=2)
vec <- alphaindex(sv)[[1]]
#points(all$x1[vec], all$x2[vec], cex=3, col="red")
#points(all$x1[vec], all$x2[vec], pch = c(19, 17)[all[vec, 3]], col = c("steelblue", "darkorange")[all[vec, 3]], cex=2)

#sl <- (all$x2[17] - all$x2[14])/(all$x1[17] - all$x1[14])
#int <- (all$x2[14] - sl*all$x1[14])
#abline(a = int, b = sl, lwd=2, lty=2)

#newint <- all$x2[7] - sl*all$x1[7]
#abline(a = newint, b = sl, lwd=2, lty=2)
```

Here we will not be able to find a line that entirely separates the groups. That is, the maximal margin classifier can not be computed. 

However, we can generalize the ideas to develop a classification rule that *almost* separates the classes. To do so, 

- we allow a few points to fall on the wrong side of the margin or separating hyperplane
- the classifier is called a *support vector classifier* or a *soft-margin classifier*
- Even in the completely separable case, we still may consider such soft margin classifier for robustness. The graph below shows the drastic change of the maximal margin classifier from the addition of a single observation.

```{r svm95, echo=FALSE, fig.cap="Two classes of observations are shown in blue and in purple, along with the maximal margin hyperplane (left panel).  An additional blue observation has been added, leading to a dramatic shift in the maximal margin hyperplane shown as a solid line (right panel). The dashed line indicates the maximal margin hyperplane that was obtained in the absence of this additional point."}
knitr::include_graphics("img/9_5.jpg")
```

### Slack Variables

In the support vector classifier, each data point $i$ is given a **slack variable** $e_i$ that allows individual data points to be on the wrong side of the margin or the separating hyperplane. 

$e_i$ quantifies where the $i$-th observation is located relative to the hyperplane and the margin: 

+ If $e_i = 0$ then the $i$-th data point is on the correct side of the margin; 

+ If $e_i > 0$ then the $i$-th observation is on the wrong side of the margin (the data point has *violated the margin*),

+ If $e_i > 1$ then it is on the wrong side of the hyperplane.

```{r svmns2, echo=F, fig.height=4.5, fig.width=5, fig.margin=TRUE, fig.cap="A scatterplots showing a simulatd two-class data set is shown. The support vector, separating hyperplane, and margin are highlighted. Points that fall outside the Margin that are correctly classified have their slack variable at 0. Those within the margin but still on the correct side have their slack variable between 0 and 1. Those on the wrong side of the margin have their slack variable larger than 1."}
library(kernlab)
set.seed(1)
n <- 10
div <- 0.5

x1 <- runif(n, min=0, max=div+0.1)
y1 <- runif(n)

x2 <- runif(n, min = div - 0.1, max = 1)
y2 <- runif(n)

all <- data.frame(x1 = c(x1, x2), x2 = c(y1, y2), group = rep(1:2, each=n))

th <- pi/3
rotmat <- cbind( c(cos(th), sin(th)), c(-sin(th), cos(th)) )
dd <- cbind(all$x1, all$x2)%*%t(rotmat)  
all$x1 <- dd[,1]
all$x2 <- dd[,2]
  
#plot(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group])
#abline(v=0.5, lwd=2, lty=2)


sv <- svm(group ~ ., data = all, type = "C-classification", kernel = "linear", cost=10000, scale = FALSE)


plot(x2 ~ x1, data = all,
     pch=c(19, 17)[all$group], 
     col = c("steelblue", "darkorange")[all$group], 
     cex=2) 
cf <- coef(sv)
abline(a = -cf[1]/cf[3], b = -cf[2]/cf[3], lwd = 2, col = "red")
abline(a = (1-cf[1])/cf[3], b = -cf[2]/cf[3], lwd = 2, col = "black", lty=2)
abline(a = (-1-cf[1])/cf[3], b = -cf[2]/cf[3], lwd = 2, col = "black", lty=2)

ind <- sv$index
points(x2 ~ x1, data = all[ind,], cex = 3, col = "red")
```


The support vector classifiers then attempt to maximize the margin such that 

$$\sum_i e_i \leq \mbox{L}$$

for a pre-specified constant $L$.

For the two predictor case, we solve the optimization problem:
$$
\underset{\beta_0, \beta_1, \beta_2, e_i}{max} M \mbox{ subject to } \left\{ \begin{matrix}Y_i(\beta_0 + X_{i1}\beta_1 +  X_{i2}\beta_2) \geq M(1- e_i), i = 1, \ldots, n,\\ 
\beta_1^2 + \beta_2^2 = 1,\\
e_i \geq 0, e_1 + \ldots + e_n \leq L,\end{matrix}\right.
$$
where $L$ is a nonnegative tuning parameter. 

- Conceptually, the value $e_i$ in the constraint $Y_i(\beta_0 + X_{i1}\beta_1 +X_{i2}\beta_2) \geq M(1- e_i)$ is the proportional amount by which the quantity $f(X_i) = \beta_0 + X_{i1}\beta_1 +  X_{i2}\beta_2$ is on the wrong side of its margin. 

- By bounding the sum $e_1 + \ldots + e_n$, we bound the total proportional amount by which $f(X_i)$ falls on the wrong side of their margin. 

- Misclassifications occur when $e_i > 1$, so bounding $e_1 + \ldots + e_n$ at a value $L$ bounds the total number of training misclassifications at $L$. 

- Specifying $L = 0$ gives us the maximal margin classifier, if it exists. 

- In contrast, as $L$ increases we become more tolerant of violations to the margin, and so the margin will widen. 

**In practice, we need to choose $L$ via cross-validation or other appropriate method.**

As before, we can write an equivalent optimization problem:
$$
\underset{\beta_1, \beta_2}{min} (\beta_1^2 + \beta_2^2)
$$
subject to the constraints
$$
Y_i(\beta_0 + X_{i1}\beta_1 +  X_{i2}\beta_2) \geq 1 - e_i, i = 1, \ldots, n,
$$
$$
e_i \geq 0, e_1 + \ldots + e_n \leq L.
$$

This is the usual way the support vector classifier is defined for the nonseparable case (and the equations generalize for $p$ predictors).

As before, once we obtain the estimators $\widehat\beta_0, \widehat\beta_1,$ and $\widehat\beta_2$, the estimated hyperplane is $\widehat f(x) = \widehat\beta_0 + x_1 \widehat\beta_1 + x_2\widehat\beta_2$, and our classification rule is: 

- for a new observation $x = (x_1, x_2)$, 
$$
\widehat Y = sign\{\widehat f(x)\}.
$$
The classification boundary, and the two margins are 
$$
\widehat f(x) = 0,\;\; \mbox{and} \;\; \widehat f(x) = \pm 1,
$$
respectively.

As with the maximal margin classifier, *the classifier is affected only by the observations that lie on the margin or violates the margin*. 

- Data points that fall directly on the margin, or on the wrong side of the margin for their class, are known as *support vectors*. 

### Support Vector Classifier Example

Let us now revisit the wines data. We can use the `svm()` function in the `e1071` library to implement support vector classifier. 

We will only use two classes to begin with (1 and 2) and two covariates, `Alcohol` and `Proline`, so that we can plot the results. Support vector classifiers however can be implemented for much larger number of predictors.

- While it is technically not needed, we often standardive the predictors beforehand -- `svm()` does standardization by default. 

    + We perform standardization manually so that we can compare the coefficients of the resulting hyperplane.

```{r}
# Read wine data
wines <- read_table("https://www4.stat.ncsu.edu/online/datasets/Wines.txt")
# Subset to two classes (1 and 2)
wine_twoclass <- wines[wines$Class < 3, ]
#make the response a factor
wine_twoclass$Class <- as.factor(wine_twoclass$Class)

# Standardize the predictors (excluding Class) 
wine_twoclass[,-1] <- scale(wine_twoclass[,-1], 
                            center = TRUE, 
                            scale = TRUE)
# SVC with linear boundary
sv.wine <- svm(Class ~ Proline + Alcohol, 
                data = wine_twoclass, 
                type = "C-classification", 
                kernel = "linear", 
                cost = 1)
```

- The argument `type = "C-classification"` specifies that we want to perform the support vector classification.

- The argument `kernel = "linear"` ensures that we are using support vector classifier. 

    + Later, when we learn *support vector machine*, we will specify different nonlinear kernels. 
    
- The parameter `cost` takes the role of $L$, however, the `svm()` function uses a different mathematical formulation than what we discussed above. 

    + Thus `cost` is not exactly same as $L$. The main concepts, however, remain the same. When the `cost` argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. 
    + When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.
    + The case `cost` $= \infty$ corresponds to the perfectly separable case. 

Now let's plot the support vector classifier.

```{r, echo=FALSE, fig.width=5, fig.height=4.5, fig.margin=TRUE, fig.cap="A scatter plots between Alcohol (x) and Proline (y) is shown. The points separate reasonably well with most of one class in the bottom left of the graph and most of the other class in the top right. There is some overlap of points in the middle region. The Support vector classifier is shown. This line has a negative slope and splits the points well. The margin around the line is shown and all points within the margin are highlighted as they are the support vectors. Three of of the first class are misclassified and two of the second class."}
nbp <- 200
PredA <- seq(min(wine_twoclass$Alcohol), 
             max(wine_twoclass$Alcohol), 
             length = nbp)
PredB <- seq(min(wine_twoclass$Proline), 
             max(wine_twoclass$Proline), 
             length = nbp)
Grid <- expand.grid(Alcohol = PredA, Proline = PredB)
wine.pred <- predict(sv.wine, Grid)
contour(PredA, PredB, 
        matrix(as.numeric(wine.pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=3,
        xlab = "Alcohol", ylab = "Proline")
points(wine_twoclass$Alcohol, wine_twoclass$Proline, 
       pch=c(19, 17, 12)[wine_twoclass$Class], 
       col = c("steelblue", "darkorange", "#990000")[wine_twoclass$Class], cex=1)
vec <- sv.wine$index
points(wine_twoclass$Alcohol[vec],
       wine_twoclass$Proline[vec],
       pch = 21, cex = 2)

cf <- coef(sv.wine)
abline(a = -cf[1]/cf[2], b = -cf[3]/cf[2])
abline(a = -(cf[1]-1)/cf[2], b = -cf[3]/cf[2], lty=2)
abline(a = -(cf[1]+1)/cf[2], b = -cf[3]/cf[2], lty=2)
```

The coefficients of the separating hyperplane can be extracted using the `coef()` function.

```{r, echo=FALSE}
cf <- round(coef(sv.wine),3)
```

```{r}
beta_hat <- coef(sv.wine)
beta_hat
```

The hyperplane is 
$$
`r cf[1]` + `r cf['Proline']`*\mbox{Proline} + `r cf['Alcohol']`*\mbox{Alcohol} = 0.
$$

The lines indicating the two margins are
$$
`r cf[1]` + `r cf['Proline']`*\mbox{Proline} + `r cf['Alcohol']`*\mbox{Alcohol} = 1,
$$

and

$$
`r cf[1]` + `r cf['Proline']`*\mbox{Proline} + `r cf['Alcohol']`*\mbox{Alcohol} = -1,
$$
respectively. 

The points that are on the margin or violate the margin are support vectors!

`cost` of course is a tuning parameter. Let's use the `caret` package to employ cross-validation to choose `cost`.  

```{r}
# Set up repeated cv option
set.seed(1001)
tr <- trainControl(method = "repeatedcv", 
                   number = 5, repeats = 10)
# Tuning grid
tune_grid <- expand.grid(cost = exp(seq(-5,3,len=30)))
# Train the model
sv_caret <- train(Class ~ Proline + Alcohol,
                  data = wine_twoclass,
                  method = "svmLinear2",
                  tuneGrid = tune_grid,
                  trControl = tr)
```

We can plot the accuracy as a function of the `cost` parameter. 

```{r, echo=FALSE, fig.cap="Results from repeated CV using support vector classifier on two-class wines data. A line chart showing cost (x) and accuracy (y) is shown. The accuracy starts off lower and then jumps to a higher value quickly. The accuracy then drops off for larger values of cost.", fig.width=6}
plot(sv_caret)
```

We can get our optimal cost value using the `bestTune` argument.

```{r}
# Best C
sv_caret$bestTune
```

Now let's refit using the `svm()` function with this optimal value.

```{r}
# Final fit
wine_sv_final <- svm(Class ~ Proline + Alcohol, 
                      data = wine_twoclass, 
                      type = "C-classification", 
                      kernel = "linear", 
                      cost = sv_caret$bestTune$cost)
```

To obtain the predicted classes, we can use the `predict()` function. Let's check out our confusion matrix on the training data.

```{r}
pred.class <- predict(object = wine_sv_final, 
                      newdata = wine_twoclass, 
                      type = "response")
err <- klaR::errormatrix(true = wine_twoclass$Class, 
                   predicted = pred.class)
#rows are true values and columns predicted
round(err, 3) |>
  kable()
```


## More than two classes

What do we do when our response isn't binary? We have two options:

- one-versus-one approach
- one-versus-all approach

### one-versus-one

Here we compute all pair-wise classifiers. 

- That is, compute all $C^K_2 = K(K-1)/2$ classification rules. 
- Given a test observation, we classify it using *each* of the $C^K_2$ classifiers, and record the number of times that the test observation is assigned to each of the $K$ classes. 
- The final classification is performed taking a *majority vote*.

### one-versus-all

Alternatively, we can compare class $k$ with the remaining classes pooled together. 

- That is, class $k$ vs. not class $k$. 
- Let $\widehat\beta_{0k}, \ldots, \widehat\beta_{pk}$ denote the parameters that result from fitting an SVM comparing the $k$-th class (coded as $+1$) to the others (coded as $-1$). 
- Given a test observation, $x$, we assign the observation to the class for which $\widehat\beta_{0k} + x_1\widehat\beta_{1k} + \ldots + x_p\widehat\beta_{pk}$ is largest, as this amounts to a high level of confidence that the test observation belongs to the $k$th class rather than to any of the other classes.

### Example

The `svm()` function uses the *one-versus-one* approach.

Let us use the full `wines` data with three classes, but with two predictors `Alcohol` and `Proline` for plotting purposes. 

```{r}
# Pre-process wines data
wines$Class <- as.factor(wines$Class)
wines[, -1] <- scale(wines[,-1], 
                     center = TRUE, scale = TRUE)
# Train the model
set.seed(1001)
tr <- trainControl(method = "repeatedcv", 
                   number = 5, repeats = 10)
tune_grid <- expand.grid(cost = exp(seq(-5,3,len=30)))
sv_caret <- train(as.factor(Class) ~ Proline + Alcohol,
                  data = wines,
                  method = "svmLinear2",
                  tuneGrid = tune_grid,
                  trControl = tr)
```

Now we can plot the accuracy as a function of cost.

```{r, echo=FALSE, fig.cap="CV results for three class wines data. A line plot is shown with cost (x) and accuracy (y). The accuracy starts low and jumps up quickly. It then drops a bit and levels off."}
plot(sv_caret)
```

Let's refit with the optimal tuning value.

```{r}
# SVC with optimal cost
sv.wine <- svm(Class ~ Proline + Alcohol, 
                data = wines,
                type = "C-classification", 
                kernel = "linear", 
                cost = sv_caret$bestTune$cost)
```

Now we can plot our SVM with three classes.

```{r, echo=FALSE, fig.width=6, fig.height=6, fig.margin=TRUE, fig.cap="A scatterplot between alcohol (x) and proline (y) is shown with points displaying the three different classes. One class mostly falls in the bottom left of the graph, another in the bottom right, and the other in the top right. The SVM is plotted and shows lines that roughly separate these three classes. Many points close to the decision boundary are highlighted and represent the support vectors."}
nbp <- 200
PredA <- seq(min(wines$Alcohol), 
             max(wines$Alcohol), 
             length = nbp)
PredB <- seq(min(wines$Proline), 
             max(wines$Proline), 
             length = nbp)
Grid <- expand.grid(Alcohol = PredA, Proline = PredB)
wine.pred <- predict(sv.wine, Grid)
contour(PredA, PredB, 
        matrix(as.numeric(wine.pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=3,
        xlab = "Alcohol", ylab = "Proline")
points(wines$Alcohol, wines$Proline, 
       pch=c(19, 17, 12)[wines$Class], 
       col = c("steelblue", "darkorange", "#990000")[wines$Class], cex=1)
vec <- sv.wine$index
points(wines$Alcohol[vec],
       wines$Proline[vec],
       pch = 21, cex = 2)
```

# Support vector machines

The support vector classifier described so far finds linear boundaries in the input feature space. Often linear effects of the covariates are not enough for a classification problem. 

Thus we might want to incorporate nonlinear terms (e.g., square or cubic terms). 

- With two covariates $X_1$ and $X_2$, we might include $X_1, X_2, X_1^2, X_2^2$ and $X_1X_2$ in our classifier.

- If we run the support vector classifier, the decision boundary would be a quadratic polynomial. 

- In general, we can incorporate other nonlinear transformations $h_1(X), \ldots, h_M(X)$ as features! 

    + This makes the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines.  
    + Linear boundaries in the enlarged space tend to achieve better training-class separation, and translate to nonlinear boundaries in the original space. 
    + Once the basis functions $h_m(x), m = 1, \ldots, ,M$ are selected, the procedure is the same as before.
    + We fit the support vector classifier using input features $h_1(X_i), h_2(X_i), \ldots , h_M(X_i)$, $i = 1, \ldots, N$, and produce the (nonlinear) function 
$$
\widehat f(x) = \widehat \beta_0  + h_1(x)\widehat \beta_1 + \ldots + h_M(x)\widehat \beta_M.
$$ 
The classifier is $\widehat Y = sign(\widehat f(x))$ as before.

## Example SVM

The following code includes natural cubic spline terms with four degrees of freedom for `Alcohol` and `Proline` with `cost` $= 0.2$. 

```{r, message=FALSE, warning=FALSE}
library(splines)
sv.wine <- svm(Class ~ ns(Proline, df=4) + ns(Alcohol, df=4), 
                data = wines,
                type = "C-classification", 
                kernel = "linear", 
                cost = 0.2)
```

The decision boundaries are quadratic, as shown below.

```{r svmquad, echo=FALSE, fig.width=6, fig.height=6, fig.margin=TRUE, fig.cap="Classification of wine data using support vector classifier with natural cubic splines. The decision boundaries here are non-linear."}
nbp <- 100
PredA <- seq(min(wines$Alcohol), 
             max(wines$Alcohol), 
             length = nbp)
PredB <- seq(min(wines$Proline), 
             max(wines$Proline), 
             length = nbp)
Grid <- expand.grid(Alcohol = PredA, Proline = PredB)
wine.pred <- predict(sv.wine, Grid)
contour(PredA, PredB, 
        matrix(as.numeric(wine.pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=3,
        xlab = "Alcohol", ylab = "Proline")
points(Grid$Alcohol, 
       Grid$Proline, 
       col=c("steelblue", "darkorange", "#990000")[as.numeric(wine.pred)], cex = 0.05)
points(wines$Alcohol, wines$Proline, 
       pch=c(19, 17, 12)[wines$Class], 
       col = c("steelblue", "darkorange", "#990000")[wines$Class], cex=1)
#vec <- sv.wine$index
#points(wines$Alcohol[vec],
#       wines$Proline[vec],
#       pch = 21, cex = 2)
```


The core idea is that even though the two classes are not separable in the original feature space, **they may be separable in transformed, and/or expanded feature space**. 

## Simulated Example

Consider the simulated data in the figure below.

```{r, echo=FALSE, fig.height=4.5, fig.width=5, warning=FALSE, message=FALSE, fig.cap="Simulated example with a non-linear relationship. A scatterplot is shown between two variables. All points in the middle of the plots are from one class. Points along the outer parts of the graph come from the other class. These classes are completely separable using a circle. "}
n <- 300
df <- data.frame(X1 = 4*runif(n)-2,
                 X2 = 4*runif(n)-2)
df$Z = df$X1^2 + 2*df$X2^2# + 2*df$X1*df$X2
df <- df[df$Z <2 | df$Z >4,]
df$Class <- ifelse(df$Z > 3, 1, 2)
colors <- c("#E69F00", "#56B4E9") #")#, )
colors <- colors[as.numeric(df$Class)]
plot(df$X1, df$X2, col = colors, pch = 19,
     xlab = "X1", ylab = "X2")
```

If we consider this data on a transformed scale, we can see that they are linearly separable in that space.

```{r, echo=FALSE, fig.height=4.5, fig.width=5, warning=FALSE, message=FALSE, fig.cap="Simulated example of transformed and enlarged feature space. THe x-axis is given by X1 squared and the y-axis is given by X2 squared. The points are linearly separable in this space."}
plot(df$X1^2, df$X2^2, col = colors, pch = 19,
     xlab = "X1^2", ylab = "X2^2")
```

- We see that there is a linear combination of the form $aX_1^2 + bX_2^2 + c = 0$ that separates the two classes. 

- By including basis functions that allow for this relationship, we can achieve highly accurate predictions.

Consider another simulated example given below. 

```{r, echo=FALSE, fig.height=4.5, fig.width=5, warning=FALSE, message=FALSE, fig.cap="A scatterplot of simulated data between x1 and x2 is shown. Two lines would perfectly separate the data by class with all of one class falling between the lines and the other class outside of the lines."}
df <- data.frame(X1 = 4*runif(n)-2,
                 X2 = 4*runif(n)-2)
df$Z = df$X1^2 + 2*df$X2^2 + 2*df$X1*df$X2
df <- df[df$Z <2 | df$Z >4,]
df$Class <- ifelse(df$Z > 3, 1, 2)
colors <- c("#E69F00", "#56B4E9") #")#, )
colors <- colors[as.numeric(df$Class)]
plot(df$X1, df$X2, col = colors, pch = 19,
     xlab = "X1", ylab = "X2")
```

We could consider a similar transformed space of $X_1^2$ and $X_2^2$.

```{r echo=FALSE, fig.height=4.5, fig.width=5, warning=FALSE, message=FALSE, fig.cap="A scatterplot of simulated data is shown between x1 squared and x2 squared. The data are nearly separable in this case with some overlap."}
plot(df$X1^2, df$X2^2, col = colors, pch = 19,
     xlab = "X1^2", ylab = "X2^2")
```

Here we almost have separability but there is still a good bit of overlap. 

Instead, we could consider a third transformation, an interaction between $X_1$ and $X_2$. Plotting the three variables, $X_1^2$, $X_2^2$, and $X_1X_2$ requires us to use three dimensions.

```{r echo=FALSE, fig.height=4.5, fig.width=5, warning=FALSE, message=FALSE, fig.cap="A three dimensional scatterplot between x1 squared, x2 squared, and x1*x2 is shown. Here we get complete separation of our classes in this enlarged feature space."}
library(scatterplot3d)
scatterplot3d(df$X1^2, df$X2^2, df$X1*df$X2, pch = 16, color=colors, grid=TRUE, box=FALSE,
              xlab = "X1^2", ylab = "X2^2", zlab = "X1*X2")
```

- We see that there is a linear combination of the form $aX_1^2 + bX_2^2 + cX_1X2 + d = 0$ that separates the two classes. 

- In general, we can include other nonlinear transformations $h_1(X), \ldots, h_M(X)$ as features in support vector classifiers (we then call them support vector machines)!

## Kernel Functions

We've seen that SVMs generalize support vector classifiers by including nonlinear features. 

- The dimension of the enlarged space is allowed to get very large, infinite in some cases.  

How? Without going into mathematics, SVM does so using the so called *kernel trick*, that is, by specifying a *kernel function* that controls which nonlinear features to include in the classifier. 

- Define $\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)^T$. It turns out that 
$$
\widehat{\boldsymbol{\beta}} = \sum_{i=1}^n \widehat \alpha_i Y_i X_i,
$$
for some weights $\widehat \alpha_i$. 

- Thus the solution to the support vector classifier problem can be represented as
$$
\widehat f(x) = \widehat \beta_0 + x^T\boldsymbol{\beta} = \widehat \beta_0 + \sum_{i=1}^n \widehat \alpha_i x^TX_i \, Y_i = \widehat \beta_0 + \sum_{i=1}^n \widehat \alpha_i \langle x, X_i\rangle \, Y_i.
$$

- To estimate $\widehat\beta_0$ and $\widehat\alpha_1, \ldots, \widehat\alpha_n$, it can be shown that we only need the all *pair-wise* inner products of the training data $\langle X_i, X_{i'}\rangle$. 

- Many of the resulting solutions $\widehat\alpha_i$ are zero. The observations for which $\widehat\alpha_i$ are nonzero are called the *support vectors*. 

> To summarize, in representing the linear classifier $f(x)$, and in computing its coefficients, all we need are inner products: $\langle X_i, X_{i'}\rangle, i,i' = 1, \ldots, n$, and $\langle x, X_{i}\rangle$.

For general nonlinear features $$h(X_i) = [h_1(X_i), h_2(X_i), \ldots , h_M(X_i)]^T,$$ the classifier $\widehat f(x)$ can be computed using the inner products: $\langle h(X_i), h(X_{i'})\rangle$ and $\langle h(x), h(X_{i})\rangle$. 

- In fact, we need not specify the transformation $h(x)$ at all, but require only knowledge of the *kernel function*
$$
K(x, x') = \langle h(x), h(x') \rangle,
$$
that computes inner products in the transformed space. 

- In general, the kernel function $K(\cdot,\cdot)$ should be a symmetric positive (semi-) definite function. 

    + Some popular choices for $K(\cdot,\cdot)$ in the SVM are
$$
\mbox{Linear}: K(x, x') = \langle x, x' \rangle,
$$
$$
d\mbox{-th degree polynomial}: K(x, x') = (1 + \langle x, x' \rangle)^d,
$$
$$
\mbox{Radial basis}: K(x, x') =  exp(-\gamma \|x - x'\|^2),
$$
$$
\mbox{Neural network}: K(x, x') = tanh(\kappa_1 \langle x, x' \rangle + \kappa_2).
$$

- Using the "linear" or "quadratic" ($d = 2$ degree polynomial) kernel will result in linear or quadratic classification boundaries, respectively. 

- Using a "radial basis kernel" captures other nonlinear features. 

As an example, the plot below shows three classification rules corresponding to linear, quadratic and radial kernels based on a simulated data set. 

```{r svmgauss, echo=F, fig.height=6, fig.width=6, fig.margin=TRUE, fig.cap="A scatterplot of simulated two-class data is shown. A linear boundary (blue dashed), a quadratic boundary (black dash-dotted), and a radial-basis based boundary (red solid) classification rules are shown. We see the linear boundary is the simplest, the quadratic more flexible, and the radial much more flexible."}
library(kernlab)
set.seed(1)
n <- 10
div <- 0.5

x1 <- runif(n, min=0, max=div+0.1)
y1 <- runif(n)

x2 <- runif(n, min = div - 0.1, max = 1)
y2 <- runif(n)

all <- data.frame(x1 = c(x1, x2), x2 = c(y1, y2), group = rep(1:2, each=n))

th <- pi/3
rotmat <- cbind( c(cos(th), sin(th)), c(-sin(th), cos(th)) )
dd <- cbind(all$x1, all$x2)%*%t(rotmat)  
all$x1 <- dd[,1]
all$x2 <- dd[,2]
  
#plot(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group])
#abline(v=0.5, lwd=2, lty=2)


sv <- ksvm(group ~., data = all, type = "C-svc", kernel = "rbfdot", C = 100)
svpoly <- ksvm(group ~., data = all, type = "C-svc", kernel = "polydot",C = 100, kpar=list(degree = 2))

nbp <- 400;
PredA <- seq(min(all$x1), max(all$x1), length = nbp)
PredB <- seq(min(all$x2), max(all$x2), length = nbp)
Grid <- expand.grid(x1 = PredA, x2 = PredB)
outg <- predict(sv, Grid)
outpoly <- predict(svpoly, Grid)
contour(PredA, PredB, matrix(as.numeric(outg), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 1, lwd=3,
        xlab = "X1", ylab = "X2")
contour(PredA, PredB, matrix(as.numeric(out), ncol = nbp), 
        drawlabels = F, col="blue", nlevels = 1, lwd=3,
        xlab = "X1", ylab = "X2", add=TRUE, lty=2)
contour(PredA, PredB, matrix(as.numeric(outpoly), ncol = nbp), 
        drawlabels = F, col="black", nlevels = 1, lwd=3,
        xlab = "X1", ylab = "X2", add=TRUE, lty=6)
points(all$x1, all$x2, pch=c(19, 17)[all$group], col = c("steelblue", "darkorange")[all$group], cex=2)
#vec <- alphaindex(sv)[[1]]
#points(all$x1[vec], all$x2[vec], cex=3, col="red")
#points(all$x1[vec], all$x2[vec], pch = c(19, 17)[all[vec, 3]], col = c("steelblue", "darkorange")[all[vec, 3]], cex=2)

#sl <- (all$x2[17] - all$x2[14])/(all$x1[17] - all$x1[14])
#int <- (all$x2[14] - sl*all$x1[14])
#abline(a = int, b = sl, lwd=2, lty=2)

#newint <- all$x2[7] - sl*all$x1[7]
#abline(a = newint, b = sl, lwd=2, lty=2)
```

To see the correspondence between kernels and original features, consider an example a feature space with two inputs $X_1$ and $X_2$, and a polynomial kernel of degree 2. 
$$
K(X,X') = (1 + \langle X, X'\rangle)^2 = 1 + 2X_1X_1' + 2X_2X_2' + (X_1X_1')^2 + (X_2X_2')^2 + 2X_1X_1'X_2X_2'.
$$

- Here, $M = 6$.
- We can define (not unique) $h_1(X) = 1, h_2(X) = \sqrt{2}X_1, h_3(X) = \sqrt{2}X_2, h_4(X) = X_1^2, h_5(X) = X_2^2, h_6(X) = \sqrt{2}X_1X_2$
- Then we have  $K(X,X') = \langle h(X), h(X') \rangle$. 
## Another SVM Example

Let us now revisit the wines data. The figure below shows the decision boundaries using the radial basis kernel SVM. 

```{r}
sv.wine <- svm(Class ~ Proline + Alcohol, 
                data = wines,
                type = "C-classification", 
                kernel = "radial",
                gamma = 1,
                cost = 0.2)
```

- Here we set $\gamma = 1$. 
- In practice, we need to explore a few values of $\gamma$ to select an optimal value based on test performance.    

```{r svmradial, echo=FALSE, fig.width=6, fig.height=6, fig.margin=TRUE, fig.cap="A scatterplot between alcohol (x) and proline (y) is shown with points differentiated by wine class (3 classes). The decision boundaries for the radial basis SVM are shown. These boundaries are smooth but nonlinear in this case."}
nbp <- 200
PredA <- seq(min(wines$Alcohol), 
             max(wines$Alcohol), 
             length = nbp)
PredB <- seq(min(wines$Proline), 
             max(wines$Proline), 
             length = nbp)
Grid <- expand.grid(Alcohol = PredA, Proline = PredB)
wine.pred <- predict(sv.wine, Grid)
contour(PredA, PredB, 
        matrix(as.numeric(wine.pred), ncol = nbp), 
        drawlabels = F, col="red", nlevels = 2, lwd=3,
        xlab = "Alcohol", ylab = "Proline")
points(Grid$Alcohol, 
       Grid$Proline, 
       col=c("steelblue", "darkorange", "#990000")[as.numeric(wine.pred)], cex = 0.05)
points(wines$Alcohol, wines$Proline, 
       pch=c(19, 17, 12)[wines$Class], 
       col = c("steelblue", "darkorange", "#990000")[wines$Class], cex=1)
vec <- sv.wine$index
#points(wines$Alcohol[vec],
#       wines$Proline[vec],
#       pch = 21, cex = 2)
```

- In general, non-linear kernels are better in capturing nonlinear boundaries compared to linear methods like LDA, Logistic regression etc.  

One advantage of using a kernel rather than simply enlarging the feature space using transformation of the original features is ease of computation. 

- Using kernels, we only need to compute inner products distinct pairs of observations $\langle X_i, X_{i'}\rangle$. 
- This can be done without explicitly working in the enlarged feature space. 
- This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. 
- For some kernels, e.g., radial kernel, the feature space is implicit and infinite-dimensional, so it is not feasible to specify the entire feature space using explicit basis functions. 

# SVM as a Penalized Method 

Suppose we define $f(x) = \beta_0 + h_1(x)\beta_1 + \ldots + h_M(x)\beta_M$, where $h_m(x)$ are transformations of the original functions. 

- The SVM problem can be shown to be equivalent to 
$$
\underset{\beta_0, \beta_1, \cdots, \beta_p}{min} \; \sum_{i=1}^n [1 - Y_if(X_i)]_+ + \frac{\lambda}{2}\sum_{m=1}^M \beta_m^2, 
$$
where $t_+$ denotes the positive part, that is,$t_+ = tI(t > 0)$, and $\lambda$ is a penalty parameter. 

- The loss function above is also called the *hinge loss*. 
- The optimization problem has the typical form of `loss` + `penalty` that we have encountered in our discussion of regularized regression. 

The penalty above is the ridge penalty. Some recent work (Yi, C. and Huang, J. (2017) Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression, Journal of Computational and Graphical Statistics, 547-557.) replaces the ridge penalty with the lasso and elastic net penalty to obtain sparse solution of the coefficients. 

- The `sparseSVM` library performs such a procedure in `R`. For the two-class wines data, let us run the support vector classifier based on all 13 predictors.

```{r}
# Set up repeated cv option
set.seed(1001)
tr <- trainControl(method = "repeatedcv",
                   number = 5, repeats = 10)
# Tuning grid
tune_grid <- expand.grid(cost = exp(seq(-5,3,len=30)))
```

Let us first fit an optimized support vector classifier.

- Recall that we have already standardized the two-class wines data. 

```{r}
# Train the model
sv_caret <- train(Class ~ .,
                  data = wine_twoclass,
                  method = "svmLinear2",
                  tuneGrid = tune_grid,
                  trControl = tr)
# Final model
wine_sv_final <- svm(Class ~ .,
                     data = wine_twoclass, 
                     type = "C-classification",
                     kernel = "linear",
                     cost = sv_caret$bestTune$cost)
sv_caret$bestTune
```

The coefficients of this fit are given below.

```{r}
beta_hat <- coef(wine_sv_final)
beta_hat |>
  kable()
```

- Notice that the standard support vector classifier does not set any of the small coefficients to exactly zero. 

- We may suspect predictors with smaller values of coefficients (e.g., `Mg`, `Phenol`, `Flav` etc) may not contribute to the classification rule as much as other variables like  `Alcohol`, `Proline`, `Ash`, `Alcal`, which have relatively large coefficients.

- Let us run the same classifier with a LASSO penalty.

    + We will use CV to choose $\lambda$. By default, the `R` function uses 10-fold CV. 
```{r, message=FALSE, warning=FALSE}
library(sparseSVM)
set.seed(1001)
X <- as.matrix(wine_twoclass[,-1])
y <- wine_twoclass$Class
# Cross validation to choose lambda 
spr.cv <- cv.sparseSVM(X = X, y = y, alpha = 1)
spr.cv$lambda.min
```

The coefficients for the two fits are given below for comparison.

```{r}
# Final fit
beta_sparse <- coef(spr.cv, 
                    lambda = spr.cv$lambda.min)
cbind(beta_sparse, beta_hat) |>
  kable()
```

- Notice that the LASSO penalty is setting some coefficients to 0.

# SVM Regression

SVM can be applied to regression problems with quantitative response as well. 

Let us start with a linear regression problem
$$
E(Y_i|X_i) = f(X_i),
$$
where 
$$
f(X_i) = \beta_0 + X_{i1}\beta_1 + \ldots + X_{ip}\beta_p = \beta_0 + X_i^T\boldsymbol{\beta}
$$

Support vector regression solves the following problem:
$$
\underset{\beta_0,\beta_1,\cdots,\beta_p}{min} \; \sum_{i=1}^n V_\epsilon(Y_i - f(X_i)) + \frac{\lambda}{2}\sum_{j=1}^p \beta_j^2, 
$$
where the loss function $V_\epsilon(\cdot)$ has the form
$$
V_\epsilon(r) = \left\{ \begin{matrix}0 \mbox{ if } |r| < \epsilon, \\ |r| - \epsilon  \mbox{ otherwise. }\end{matrix}\right.
$$
- This loss function ignores errors of size less than $\epsilon$.

- It can be shown that the solution function has the form
$$
\widehat f(x) = \sum_{i=1}^n (\widehat\alpha_i^* - \widehat\alpha_i) \langle x, X_i \rangle + \widehat\beta_0,
$$
where $\widehat\alpha_i^*$ and $\widehat\alpha_i$ are constants. 

- Typically only a subset of the values $(\widehat\alpha_i^* - \widehat\alpha_i)$ are nonzero, and the associated data values are called the support vectors. 

- As was the case in the support vector classification, the solution depends on the input values only through the inner products $\langle X_i, X_{i'} \rangle$.  

- Thus we can generalize the methods to richer spaces by defining an appropriate inner product and corresponding kernel function.

## SVM Regression in R

In `R`, the `svm()` functions can perform regression as well. 

- Here we have two parameters to tune: $\epsilon$ and `cost`. 
- This can be done using usual methods such as CV. 

Here we demonstrate support vector regression for a specific value of $\epsilon = 0.1$ and `cost = 1`, using the `Boston` housing data. 

- We use radial kernel with $\gamma = 1$.

```{r, warning=FALSE, message=FALSE}
library(ISLR2)
svr <- svm(medv ~ lstat, data = Boston,
           type = "eps-regression",
           kernel = "radial", gamma = 1,
           cost = 1, epsilon = 0.1
           )
```

With only one predictor we can visualize the fit easily.

```{r, fig.height = 4.5, fig.width=5, echo = FALSE, fig.cap = "A scatterplot between lstat and medv is shown. The data generally start in the top left and decrease, leveling off for larger values of lstat. The SVM fit is overlayed and it roughly flows through the middle of the data points in a non-linear fashion."}
# Prediction
xnew <- data.frame(lstat = seq(2, 37, len=51))
pred <- predict(svr, newdata = xnew)
# Plot
plot(medv ~ lstat, data = Boston, pch=19, col = "gray")
lines(xnew$lstat, pred, col = "red", lwd=2)
```

Overall, SVMs are quite useful in practice, and extend beyond what we discussed in this chapter. 

- For example, the penalized loss function formulation, as described in the classification and regression context can be used for  *any* convex loss function with *any* kernel function. 

- This enables us to use kernel methods in function estimation in linear, generalized linear (e.g., logistic), and other (e.g., least absolute deviation) regression models.     

