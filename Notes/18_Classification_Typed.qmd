---
title: "Classification Tasks"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(tidyverse)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(AppliedPredictiveModeling)
library(kableExtra)
library(nnet)
library(glmnet)
```


```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\pagebreak

# Classification Tasks

The problems of separating two or more groups/classes, and allocating new objects in previously defined classes are called classification and discrimination.

+ **Classification:** developing a rule to *allocate* a new object into one of a number of known groups. We use such classification rules to classify objects into pre-defined classes. Here the emphasis is on defining the rule to optimally assigning objects to classes. 

+ **Discrimination:** finding the features that *separate* known groups in a multivariate sample. This can be either done graphically or algebraically. We try to find *discriminants* (features) whose numeric values can separate the classes as much as possible.

\noindent A classification rule is based on the features that separate the groups, so the two goals often (but not always) overlap. 

## Classification Example

To give some context, let's consider a data set and the familiar model KNN model used as a classifier.

Consider the `wines` data set available at the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/wine). The dataset contains quantities of 13 constituents found in each of the three types (cultivars) of wines.

Our response variable here is `Class` (`1`, `2`, or `3`), representing the three cultivars. Note that is no ordering to the cultivars. The values 1, 2, and 3 are just category placeholders.

```{r, message = FALSE, warning = FALSE}
# Read the data
wines <- read_table("data/Wines.txt")
#convert our response to a factor
wines$Class <- as.factor(wines$Class)
```

A snapshot of the full data is shown below. The goal is to find a *rule* that can assign a specimen of wine to its cultivar. In other words, we want to predict the classes (1, 2, or 3) based on the predictors (13 variables).  

```{r}
#frequency of each class of wine
freq <- table(wines$Class) |>
  data.frame()
names(freq) <- c("Cultivar", "Freq")
freq |>
  kable()
```

For this demonstration, we will only consider two predictor variables, `Alcohol` and `Malic`. However, the techniques discussed hereafter can be applied to any number of predictors. The figure below shows the three classes on a scatterplot of `Alcohol` vs. `Malic`.  

```{r winetwo, fig.width=4, fig.height=4, fig.cap="Scatterplot of Alcohol vs. Malic in the wine data.", fig.margin=T, echo=FALSE, fig.cap = 'The image is a scatter plot chart with axes labeled "Alcohol" (x-axis) and "Malic" (y-axis). The chart plots data points using three different colored shapes representing different classes. Light blue squares represent Class 1, yellow triangles represent Class 2, and red circles represent Class 3. The data points are spread across the chart with varying concentrations and patterns for each class. Class 1, represented by squares, appears concentrated mainly along the lower part of the y-axis between x-values 12 and 14. Class 2, represented by triangles, is scattered mostly between x-values 11 and 13.5 along the lower y-values. Class 3, represented by circles, is more dispersed over the plot, with several data points extending to higher y-values.'}
# pairs plot
#colors <- c("darkgreen", "darkorange", "#990000")[wines$Class]
#plot(wines$Alcohol, wines$Proline, pch = 16, cex = .5, col = colors, #xaxt = "n", yaxt = "n", xlab = "Alcohol", ylab = "Proline")

ggplot(wines) + 
  geom_point(aes(Alcohol, Malic, 
                 col=as.factor(Class),
                 shape=as.factor(Class)),
             size = 3) + 
  scale_color_manual(name = "Class",
                     labels = c(1,2,3),
                     values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
  scale_shape_manual(name = "Class",
                     labels = c(1,2,3),
                     values = c(15, 17, 19)) +
  theme_bw(base_size = 18) + 
  theme(legend.position = "top")
```

### $K$-Nearest Neighbors 

Suppose we have training data $(Y_i, X_i)$ for $i=1,\ldots,n$, where $Y_i$ is categorical variable denoting class label of $X_i$. For a given predictor $x_0$, KNN classifier predicts the class label as follows:

+  Identify the $K$ observations in the training data such the their $X$ values are "nearest" to $x_0$.

+ Predict the class label corresponding to $x_0$ as the class having the majority vote, that is, having the most number of points among the $K$ neighbors obtained form previous step.

The process and concepts around train/test sets, tuning, etc. all follow through here. We'll go through the example more explicitly soon. First, let's just look at some KNN decision boundaries.

Let us start with a KNN classifier with $K = 30$. We can use the `knn()` function in the `class` package, or use `caret` with `method = "knn"`. Note that `caret` does both regression and classification. It automatically determines the problem depending on whether the response is numeric or categorical (`factor`). We have already converted the `Class` variable in the `wines` data to a factor.

```{r}
## 30-NN Classifier / with no tuning done
fit <- train(Class ~ Alcohol + Malic,
             data = wines,
             method = "knn",
             tuneGrid = data.frame(k = 30),
             trControl = trainControl(method = "none"))
```

Now let's plot the classifications made by the model:

```{r knnk20, echo=FALSE, cache=TRUE, fig.height=4, fig.width=4, fig.cap="Decision boundary of 30-NN classifier of the wines data.", fig.margin = TRUE, fig.alt = 'The image is a scatter plot chart with axes labeled "Alcohol" (x-axis) and "Malic" (y-axis). The chart plots data points using three different colored shapes representing different classes. Light blue squares represent Class 1, yellow triangles represent Class 2, and red circles represent Class 3. The data points are spread across the chart with varying concentrations and patterns for each class. Class 1, represented by squares, appears concentrated mainly along the lower part of the y-axis between x-values 12 and 14. Class 2, represented by triangles, is scattered mostly between x-values 11 and 13.5 along the lower y-values. Class 3, represented by circles, is more dispersed over the plot, with several data points extending to higher y-values. Three regions are indicated by the plot representing the classification for the data points. The region that classifies points into Class 1 roughly corresponds to Alcohol values of 13 to 15 and Malic values of 1 to 2.5. The region that classifies points into class 2 roughly corresponds to Alcohol from 11 to 13 and Malic values of 1 to 3.5. The remaining region classifies points to class 3.'}

nbp <- 200
PredA <- seq(min(wines$Alcohol), max(wines$Alcohol), length = nbp)
PredB <- seq(min(wines$Malic), max(wines$Malic), length = nbp)
Grid <- expand.grid(Alcohol = PredA, Malic = PredB)

pred_reg <- predict(fit, newdata = Grid)

ggplot() + 
  geom_point(aes(Alcohol, Malic, 
                 col = Class, 
                 shape = Class), 
             data = wines,
             size = 2) + 
  geom_raster(aes(Alcohol, Malic, fill = pred_reg), 
              data = Grid,
              alpha = 0.1) + 
  geom_contour(mapping = aes(Alcohol, Malic, 
                             z = as.numeric(pred_reg)), data = Grid,
               col="black") + 
   scale_color_manual(name = "Class",
                      labels = c("1",'2',"3"),
                      values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
   scale_shape_manual(name = "Class",
                      labels = c("1","2","3"),
                      values = c(15, 17, 19)) +
   scale_fill_manual(name = "Prediction",
                      labels = c("1",'2',"3"),
                      values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
   theme_bw(base_size = 18) + 
   theme(legend.position = "top") +
   guides(color = guide_legend(override.aes = list(size = 5)))
```

The lines in the figure are called the decision boundaries of this classifier!

Formally, we can think of the KNN process as estimating the conditional probability of Y given X, $P(Y | X)$. 

Suppose that $Y$ has $J$ categorical classes. Denote the values $Y$ can take on as $1, 2, \ldots, J$.

Suppose $S_K(x_0)$ denotes the indices of the $K$ points whose $X$ values are nearest to $x_0$. Then for a data point $x_0$, KNN estimates the conditional probability that the class label is $j$ given $X = x_0$ as
$$
\widehat P(Y = j | X = x_0) = \frac{1}{K}\sum_{i \in S_K(x_0)} I(Y_i = j), 
$$
for each $j = 1, \ldots, J$. Thus, for each of the $J$ classes, we compute the proportion of the $K$ neighbors belonging to that class. We classify $x_0$ to the class that has the highest estimated probability.

Here we can see that the KNN classifier gives us a rule to allocate objects to classes, but **does not give us any discriminants**. 

Now that we've seen an example, we need to understand how evaluation of a classifier differs from evaluation of a regression model.

# Bayes classifier

The motivation behind estimating the conditional probabilities $P(Y = j | X)$ is from minimizing test error rate. Similar to regression, given a new independent test point $X$ with label $Y$, we can define expected prediction error for classification as
$$
E[I(Y \neq \widehat Y)],
$$
where $\widehat Y$ is the prediction from a classifier. Notice that $Y$ depends on $X$, and $\widehat Y$ depends on both $X$ and the training set. We want a classifier that minimizes the expected prediction error. It can be shown that the optimal classifier is the one that predicts a new observation $x_0$ by $\widehat Y$ such that 

>> $\widehat Y = j$ if $P(Y = j | X = x_0)$ is maximum among $P(Y = 1 | X = x_0), \ldots, P(Y = J | X = x_0).$

\noindent The optimal classifier is called the *Bayes classifier*.

**Bayes Classifier:** Classifies an observation to the most probable class using the discrete conditional distribution of $P(Y | X)$

**Bayes error rate:** misclassification error rate of the Bayes classifier. For a given $x_0$, Bayes error is $1 - max_\ell P(Y = \ell | X = x_0)$. The overall Bayes error rate is $1 - E[max_\ell P(Y = \ell | X)]$.

Every classification problem has these two (unknown) items. 

The Bayes rate is analogous to the irreducible error that we encountered in the regression setting.  Even if we knew everything about the relationships between our $X$'s and $Y$, we can't ever beat this error rate!

Unfortunately, we can not directly use the Bayes classifier since we do not know the distribution of $Y|X$. Different classifiers use different estimators of such conditional distributions -- KNN uses proportion of points in the $K$ nearest neighbors belonging to each class as the estimator.

Thus, it is natural to try to estimate/model the conditional probabilities $P(Y = k | X)$ using the data, and use them to create classifiers. 

There are two major approaches for obtaining estimates of $P(Y = k | X)$:

+ *Directly estimating/modeling $P(Y = k | X)$*

    - An example of direct estimation of $P(Y = k | X)$ is the KNN classification technique, where the conditional probability is estimated by taking a majority vote from $K$ nearest point to $x_0$. 
    - Another example is *logistic* regression model, where the conditional probability is modeled using transformations of linear combinations of $X$ of the form:
    $$
    P(Y = k | X) = \frac{e^{\beta_0 + X_1\beta_1 + \ldots + X_p\beta_p}}{1 + e^{\beta_0 + X_1\beta_1 + \ldots + X_p\beta_p}}.
    $$
      Therefore it is sufficient to estimate the coefficients $\beta_0,\beta_1, \ldots, \beta_p$ to obtain estimates of $P(Y = k | X)$.

+ *Generative models* where we model the distribution of $X | Y = k$ for $k = 1, \ldots, K$ and use **Bayes Theorem** to obtain a classifier

## kNN and the Bayes Classifier

As with KNN regression, the hyperparameter $K$ determines how flexible the KNN method is. However, the idea of flexibility is subtle in this case. The boundary that separates the regions of the predictor space is effectively the classification rule for that classifier. The *decision boundary*, is the boundary of the regions.

```{r, echo=FALSE, eval = FALSE, cache=TRUE, fig.height=4, fig.width=4, fig.cap="Decision boundary of 30-NN classifier of the wines data.", fig.margin = TRUE}
ggplot() + 
  geom_point(aes(Alcohol, Malic, 
                 col = Class, 
                 shape = Class), 
             data = wines,
             size = 2) + 
  geom_raster(aes(Alcohol, Malic, fill = pred_reg), 
              data = Grid,
              alpha = 0.1) + 
  geom_contour(mapping = aes(Alcohol, Malic, 
                             z = as.numeric(pred_reg)), data = Grid,
               col="black") + 
   scale_color_manual(name = "Class",
                      labels = c("1",'2',"3"),
                      values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
   scale_shape_manual(name = "Class",
                      labels = c("1","2","3"),
                      values = c(15, 17, 19)) +
   scale_fill_manual(name = "Prediction",
                      labels = c("1",'2',"3"),
                      values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
   theme_bw(base_size = 18) + 
   theme(legend.position = "top") +
   guides(color = guide_legend(override.aes = list(size = 5)))
```


How would the decision boundary look like for the Bayes classifier? For a two class problem, the Bayes classifier assigns the most probable class to a new data point. Thus for a new $x_0$, it will be assigned to class 1 if $P(Y = 1 | X = x_0) > P(Y = 2 | X = x_0)$ and assigned to class 2 otherwise. Equivalently, $x_0$ will be assigned to class 1 if $P(Y = 1 | X = x_0) > 0.5$. Thus the decision boundary of the Bayes classifier is the set of all $x$ such that $P(Y = 1 | X = x) = 0.5$.

The plots below are taken from the book and show the KNN decision boundary and the Bayes classifier decision boundary on some simulated data (hence the ability to know the Bayes classifier!).

```{r knnthree, echo=FALSE, fig.fullwidth = FALSE, fig.cap="Impact of K on decision boundaries of KNN classifiers. The Bayes dicision boundary is shown using purple dashed line. Image adapted from *Introduction to Statistical Learning*.", out.height="40%", fig.alt = "Two scatterplots between simulated data points are shown. Roughly, the points in the bottom right of the graph represent one group and points in the top left represent another group. Where the points overlap a Bayes decision boundary is shown. This represents the optimal boundary for classifying the observations. The left hand graph shows the decision boundary of a KNN model with K = 1. This boundary follows the Bayes decision boundary roughly but is too wiggly and follows the data a little too closely in some places. The right hand plot shows the decision boundary for a KNN model with K = 100. This decision boundary is too inflexible and doesn't match well with the Bayes decision boundary as it smooths over too many points."}

knitr::include_graphics(c("img/2_16.png"))

```

The value of $K$ in a KNN classifier determines how smooth or rough the decision boundary is. For a small value of $K$ (in this example,  $K=1$), the boundary is extremely rough. Although it follows the Bayes boundary closely, it is overly flexible (uses local features) and tries to discover patterns that do not conform to the Bayes boundary. This is an example of overfitting a classification problem.  

In contrast, for a large value of $K$ (such as $K=100$), the decision boundary is much smoother but does not capture the shape of the Bayes boundary. (Again we see the bias-variance trade-off here, even though we are in the classification setting.) Large values of $K$ result in a non-flexible (uses global features but averages over local ones) classifier that perhaps captures the overall trend of the Bayes boundary, but misses the details. In fact, as $K$ grows, the decision boundary will get closer to a straight line. 

Therefore, we need to tune $K$ so that the "optimal" $K$ will result in a decision boundary that is not too rough but also sufficiently captures the shape of the Bayes boundary.  The figure below shows one such example with $K=10$. In practice, we might choose $K$ by minimizing the test error rate or equivalently maximizing test accuracy. 

```{r knnk10, echo=FALSE, fig.fullwidth = FALSE, fig.margin = TRUE, fig.cap="Decision boundary for $K=10$ using simulated data presented in the previous figure. The Bayes decision boundary is shown using purple dashed line. Image adapted from *Introduction to Statistical Learning*.", out.height="40%", fig.alt = "A scatterplots of simulated data points is shown. Roughly, the points in the bottom right of the graph represent one group and points in the top left represent another group. Where the points overlap a Bayes decision boundary is shown. This represents the optimal boundary for classifying the observations. The decision boundary of a KNN model with K = 10 is overlayed. This boundary follows the Bayes decision boundary very well, only missing in a few places."}

knitr::include_graphics(c("img/2_15.png"))

```


# Example: Building a KNN Classifier

Consider the `wines` data set available at the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/wine). The dataset contains quantities of 13 constituents found in each of the three types (cultivars) of wines.

```{r, message = FALSE, warning = FALSE}
# Read the data
wines <- read_table("data/Wines.txt")
wines$Class <- as.factor(wines$Class)
```

A snapshot of the full data is shown below. 

```{r}
wines[1:4, ] |>
  kable()
```

The goal is to find a *rule* that can assign a specimen of wine to its cultivar. In other words, we want to predict the classes (cultivar) based on the predictors (13 variables).  

```{r}
# classes of wine
freq |>
  kable()
```

We can tune $K$ as we did in the regression setting. The code below searches odd values of $K$ (to avoid ties) for the optimal value with largest test accuracy. We use 50 times repeated 5-fold CV for tuning.

```{r knnloocv, cache=TRUE, fig.height=4, fig.width=4, fig.cap="Results for repeated 5-fold CV tuning.", fig.margin = TRUE, fig.alt = 'The image is a line graph depicting the relationship between the accuracy of a model, measured through repeated cross-validation, and the number of neighbors in a k-nearest neighbors algorithm. The x-axis represents the number of neighbors, ranging from 0 to 50, while the y-axis shows accuracy values from 0.74 to 0.82. The accuracy increases sharply, peaking around 0.81 at approximately 10 neighbors, and then gradually decreases as the number of neighbors continues to increase.'}
set.seed(1001)
## K values for tuning
kgrid <- expand.grid(k = seq(1,51, by=2))
## 5-fold CV, repeated, tuning
tr <- trainControl(method = "repeatedcv",
                   number = 5,
                   repeats = 50)
## Train the classifier
fit <- train(Class ~ Alcohol + Malic,
             data = wines,
             method = "knn",
             tuneGrid = kgrid,
             trControl = tr)
plot(fit)
#best value of k
fit$bestTune$k
```

```{r}
## Refit the model with best K
tuned_knn_class <- train(Class ~ Alcohol + Malic,
             data = wines,
             method = "knn",
             tuneGrid = expand.grid(k = fit$bestTune$k),
             trControl = trainControl(method = "none"))
```


To estimate the prediction error of the tuned model, we can use any of the methods discussed previously. For example, we could just look at prediction on a test set or we could use the bootstrap as the 'outer' loop. 

Consider two new unlabeled points. The first with `Alcohol` = 13 and `Malic` = 3, and the second with `Alcohol` = 12.78 and `Malic` = 2.

```{r again, echo=FALSE, cache=TRUE, fig.height=5, fig.width=5, fig.cap="Decision boundary of 21-NN classifier of the wines data with two new unlabeled points.", fig.margin = TRUE, fig.alt = 'The image is a scatter plot chart with axes labeled "Alcohol" (x-axis) and "Malic" (y-axis). The chart plots data points using three different colored shapes representing different classes. Light blue squares represent Class 1, yellow triangles represent Class 2, and red circles represent Class 3. The data points are spread across the chart with varying concentrations and patterns for each class. Class 1, represented by squares, appears concentrated mainly along the lower part of the y-axis between x-values 12 and 14. Class 2, represented by triangles, is scattered mostly between x-values 11 and 13.5 along the lower y-values. Class 3, represented by circles, is more dispersed over the plot, with several data points extending to higher y-values. Three regions are indicated by the plot representing the classification for the data points. The region that classifies points into Class 1 roughly corresponds to Alcohol values of 13 to 15 and Malic values of 1 to 2.5. The region that classifies points into class 2 roughly corresponds to Alcohol from 11 to 13 and Malic values of 1 to 3.5. The remaining region classifies points to class 3. Two "new" points are plotted. One at Alcohol of 13 and Malic of 3 is clearly in the region classified to Class 3. Another at Alcohol of 3 and Malic of 2 is classified as Class 2 but is very close to the decision boundary.'}
new_dat <- data.frame(Alcohol = c(13, 12.78),
                   Malic = c(3,2))

nbp <- 200;
PredA <- seq(min(wines$Alcohol), max(wines$Alcohol), length = nbp)
PredB <- seq(min(wines$Malic), max(wines$Malic), length = nbp)
Grid <- expand.grid(Alcohol = PredA, Malic = PredB)

pred_reg <- predict(tuned_knn_class, newdata = Grid)

ggplot() + 
  geom_point(aes(Alcohol, Malic, 
                 col = Class, 
                 shape = Class), 
             data = wines,
             size = 2) + 
  geom_point(aes(Alcohol, Malic), 
             data = new_dat,
             size = 5, shape = 5) + 
  geom_text(aes(Alcohol, Malic, label = rownames(new_dat)),
            data = new_dat, size = 5) + 
  geom_raster(aes(Alcohol, Malic, fill = pred_reg), 
              data = Grid,
              alpha = 0.2) + 
  scale_color_manual(name = "Class",
                     labels = c(1,2,3),
                     values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
  scale_shape_manual(name = "Class",
                     labels = c(1,2,3),
                     values = c(15, 17, 19)) +
  scale_fill_manual(name = "Prediction",
                     labels = c(1,2,3),
                     values = c("#00AFBB", "#E7B800", "#FC4E07")) + 
  theme_bw(base_size = 18) + 
  theme(legend.position = "top") +
  guides(color = guide_legend(override.aes = list(size = 5)))

```

We can use the model to predict a class or a class probability with predict.

The default is to give a predicted class.

```{r}
pred_class <- predict(tuned_knn_class,
                      newdata = new_dat)
pred_class
```

We can specify `type = "prob"` to do obtain predicted class probabilities.

```{r}
pred_prob <- predict(tuned_knn_class,
                     newdata = new_dat,
                     type = "prob")
names(pred_prob) <- c("Class 1", "Class 2", "Class 3")
rownames(pred_prob) <- c("New Obs 1", "New Obs 2")
pred_prob |>
  kable()
```

\noindent Note that for the first point, has about an $80\%$ probability associated with class `3`, and hence we are quite confident about out final class prediction of `3`. However, for the second point, probabilities associated with classes `1` and `2` are quite similar ($38\%$ vs $43\%$). So while we are quite confident about the predicted class of the first data point, there is some uncertainlty about the second prediction.  


# Evaluating a Classifier

## Basic Measures

To evaluate the performance of the classifier, instead of test MSE, we can use *classification accuracy* or *misclassification error rate*. In the definitions below, $|S|$ denotes the *cardinality* of $S$, that is, the number of observations in $S$. 

**Accuracy:** the proportion of points correctly classified to their respective classes. With a set of observations $S$, 

$$
\mbox{Accuracy} = \frac{\mbox{Total correct classification}}{\mbox{
Total number of points}} = \frac{1}{|S|}\sum_{i \in S} I(Y_i = \widehat Y_i).
$$
We can compute *training* and *test accuracy* depending on whether $S$ is the training or testing set.

**Misclassification error rate:** The proportion of points wrongly classified.

$$
\mbox{Error rate} = \frac{\mbox{Total incorrect classification}}{\mbox{Total number of points}} = \frac{1}{|S|}\sum_{i \in S} I(Y_i \neq \widehat Y_i).
$$

As before, we can compute *training* and *test error rate*.


As with regression setting, here too we aim to maximize *test* accuracy or minimize test error. Minimizing training error is undesirable since it will lead to overfitting the data. 

- For example, consider $K=1$, a 1-NN classifier. Since each $X_i$ is the closest neighbor to itself, the training error would be zero. 
- The figure below shows training and test error rates from a simulation study (figure adapted from the textbook *Introduction to Statistical Learning*). 


```{r knncerr, echo=FALSE, fig.margin = TRUE, fig.cap="Training and test error rates for a KNN classifier based on 200 training and 5000 test observations. The error rates are plotted against 1/K. The black dashed line shows the Bayes error rate. Figure adapted from Introduction to Statistical Learning.", fig.height = 4, fig.width=4, fig.alt = 'The image is a line graph depicting error rates as a function of the reciprocal of K (1/K) for training and test data. The x-axis is labeled "1/K," ranging from 0.01 to 1.00, while the y-axis is labeled "Error Rate," ranging from 0.00 to 0.20. There are two lines on the graph: a solid line representing "Training Errors" and a dotted line representing "Test Errors." The training error line decreases gradually with increasing 1/K, showing a downward trend and reaching near zero. In contrast, the test error line initially decreases but then rises, creating a U shape. A horizontal dashed line crosses the graph at the 0.15 error rate, suggesting a threshold or benchmark.'}
knitr::include_graphics("img/2_17.png")

```


## Other Metrics and the No Information Rate

There are many other metrics to evaluate a classification technique other than error rate and accuracy. The main criticism of these two criteria are that they provide a global measure, but do not provide much insight into how individual classes are correctly identified. For example, $80\%$ accuracy of a classifier does *not* guarantee that it will correctly classify items from *both* the classes correctly $80\%$ of the time. Such a criticism is even more relevant when there is class imbalance in the data: say we have a situation where $80\%$ of observations belong to class A, and rest in class B. If we employ a classifier that classifies *every point into class A* regardless of their predictor values. This classifier will have $80\%$ accuracy! This is called the *no information rate (NIR)* of the classification problem.

**No information rate (NIR)**
The percentage of the largest class in the training set.

\noindent The NIR represents the accuracy that can be obtained without using any model. Thus for any classifier, the NIR should be the minimum accuracy it should have. Any classifier having accuracy better than NIR might be considered viable. 

Most other measures to evaluate a classifier can be obtained by cross-tabulating the true and predicted classes of a test set. Such a table is called *confusion matrix*. An example is shown in the table below.

```{r}
#split the original data
index <- createDataPartition(wines$Class, p = 0.7, list = FALSE)
#get the train and test sets
train <- wines[index,]
test <- wines[-index,]

## K values for tuning
kgrid <- expand.grid(k = seq(1,51, by=2))
## 5-fold CV tuning
tr <- trainControl(method = "cv",
                   number = 5)
## Train the classifier
fit <- train(Class ~ Alcohol + Malic,
             data = train,
             method = "knn",
             tuneGrid = kgrid,
             trControl = tr)
#check the best tuning parameter
fit$bestTune$k
## Refit the model with best K
tuned_knn_class <- train(Class ~ Alcohol + Malic,
             data = train,
             method = "knn",
             tuneGrid = expand.grid(k = fit$bestTune$k),
             trControl = trainControl(method = "none"))
#predict on the test set
preds <- predict(tuned_knn_class, test)
```

With `caret` the `confusionMatrix()` function can now be applied to the true labels in the test set and the predictions given by the model.

```{r}
conf_output <- confusionMatrix(test$Class, preds)
```

The columns of the confusion matrix produced represent the 'truth' and the rows represent the predicted value by the model. 

```{r}
conf_output$table |>
  kable(row.names = TRUE)
```

Some metrics on the data are given in the `$overall` list element.

```{r}
data.frame(value = conf_output$overall) |>
  kable()
```


\noindent Some measures we might look at are as follows:

+ *sensitivity* (Also called "true positive rate" or "recall")$$\frac{\text{number of positive cases classified as positive}}{\text{Total number of positive samples}} = \frac{TP}{TP + FN}$$

+ *specificity* (Also called "true negative rate")$$\frac{\text{number of negative cases classified as negative}}{\text{Total number of negative samples}}  = \frac{TN}{TN + FP}$$

+ *Precision*$$\frac{\text{number of positive cases classified as positive}}{\text{Total number of predicted positive cases}}  = \frac{TP}{TP + FP}$$

\noindent We can also examine: 

+ *Cohen's kappa*: measures the agreement of the classifier to the sample data taking into account any class imbalances, and how much agreement is by chance.  Values close to 1 are considered good. The R function to do so is `cohen.kappa()` in `psych` library.

+ *McNemar's test*: hypothesis test for agreement between the predictions from an classifier to the observed data using a Chi-squared test. The R function to do so is `mcnemar.test()`.

For a multi-class problem, we can create these measures using a "one-vs-all" approach, that is, by comparing each class vs the remaining combined (class `1` vs not class `1`, and so on). 


Often, we want a single measure of performance of the classifier rather than the multitude of measures shown above. There are many such options, such as *Youdenâ€™s $J$ Index*, 
$$J = \mbox{Sensitivity} + \mbox{Specificity} - 1,$$
which measures the proportions of correct predictions for both the positive and negative events.
