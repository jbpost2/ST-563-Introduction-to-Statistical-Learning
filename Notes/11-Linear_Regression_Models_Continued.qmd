---
title: "Linear Regression Continued"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

Packages used in this set of notes:

```{r setup, message=FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(tufte)
library(tidyverse)
library(lubridate)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(AppliedPredictiveModeling)
library(kableExtra)
library(robustbase)
```

```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)

#read in data from previous set of notes
bike_share <- read_csv("https://www4.stat.ncsu.edu/online/datasets/SeoulBikeData.csv",
                       local = locale(encoding = "latin1"))

bike_share <- bike_share |>
  rename("date" = "Date",
         "rented_bike_count" = `Rented Bike Count`,
         "hour" = "Hour",
         "temperature" = `Temperature(°C)`,
         "humidity" = `Humidity(%)`,
         "wind_speed" = `Wind speed (m/s)`,
         "visibility" = `Visibility (10m)`,
         "dew_point_temperature" = `Dew point temperature(°C)`,
         "solar_radiation" = `Solar Radiation (MJ/m2)`,
         "rainfall" = `Rainfall(mm)`,
         "snowfall" = `Snowfall (cm)`,
         "seasons" = "Seasons",
         "holiday" = "Holiday",
         "functioning_day" = "Functioning Day" 
         ) |>
  mutate(date = dmy(date), #convert the date variable from character
         seasons = factor(seasons),
         holiday = factor(holiday),
         functioning_day = factor(functioning_day),
         log_rented_bike_count = log(rented_bike_count)) |>
  filter(functioning_day == "Yes")
```

\newpage

# Methods for Selecting Variables & Evaluating Model Performance

Like any other learner, we need ways to evaluate the model performance and, in this case, determine which variables we want to include in our model. We can use

- inference-based methods
- model fitting criteria that do not require a test set
- training/test set ideas with a metric

Before we get into those, let's first recap how we would use our fitted linear regression model to do prediction.

## Prediction

As mentioned before, we can predict the response associated with a set of predictors $x_1, \ldots, x_p$ as 

&nbsp;  
&nbsp;  
&nbsp;  

This is our prediction for two separate quantities:

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

To visualize this, we can consider an SLR model. Recall our `bike_share` data set. Here we fit a model using `temperature` to predict `log_rented_bike_count`. 

**Note: I've removed observations where `functioning_day` was "No" as there were no bike rentals on these days. These were the set of points that always looked weird in our diagnostic plots!**

```{r}
SLR_fit <- lm(log_rented_bike_count ~ temperature, data = bike_share)
summary(SLR_fit)$coefficients
```

If we are interested in predicting the **mean rented bike count at a temperature of 22.22 degrees** (72 degree Fahrenheit), we'd plug 22.22 into our equation:

```{r}
summary(SLR_fit)$coefficients[1, 1] + summary(SLR_fit)$coefficients[2, 1]*22.22
#or use predict()
predict(SLR_fit, 
        newdata = data.frame(temperature = 22.22))
```

Likewise, if we wanted to predict a **future rented bike count at a temperature of 22.22 degrees**, we'd simply plug 22.22 into our equation! So what's the difference?

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Consider the scatterplot with an SLR fit below. If the graph is inspected very closely, one may notice a 'confidence band' around the line. This is a confidence interval that attempts to capture the **mean response** at a given temperature.

- With this type of interval, we are trying to capture average `log_rented_bike_count` (the point on the true line), across repeated samples, when the temperature is 22.22 degrees.

```{r, message = FALSE, out.width='350px', fig.cap="Scatterplot with fitted SLR model overlayed", fig.alt = "A scatterplot between temperature on the x-axis and log rented bike count on the y-axis. Temperature goes from -20 degrees to 40 degrees and log rented bike count goes from 1 to 8. The plots shows a general upward trend with a number of smaller log rented bike count values for temperature between 0 and 25. The overlayed SLR line has a positive slope and shows a strong linear relationship. There is a very small 'band' around the line indicating a very narrow confidence interval for the mean log rented bike count for each temperature value."}
bike_share |>
  ggplot(aes(x = temperature, y = log_rented_bike_count)) +
  geom_point(size = 0.5) +
  geom_smooth(method = "lm")
```

Alternatively, we can try to capture a **new observation** when temperature is 22.22. 

- We can produce a prediction interval using the `predict()` function in `R` and adding that to our graph

```{r}
predictions <- predict(SLR_fit,
                       newdata = bike_share,
                       interval="prediction")
predictions[1:4, 1:3]
#add the predicitons to the data frame
bike_share_preds <- cbind(bike_share, predictions)
```

- With this type of interval, we are trying to capture an observation about the true line, across repeated samples, when the temperature is 22.22 degrees. This includes the irreducible error discussed previously.

```{r, message = FALSE, out.width='350px', fig.cap="Scatterplot with fitted SLR model overlayed and prediction intervals.", fig.alt = "A scatterplot between temperature on the x-axis and log rented bike count on the y-axis. Temperature goes from -20 degrees to 40 degrees and log rented bike count goes from 1 to 8. The plots shows a general upward trend with a number of smaller log rented bike count values for temperature between 0 and 25. The overlayed SLR line has a positive slope and shows a strong linear relationship. There is a wide 'band' around the line that captures most of the data points indicating a very wide prediction interval for a future log rented bike count for each temperature value."}
bike_share |>
  ggplot(aes(x = temperature, y = log_rented_bike_count)) +
  geom_point(size = 0.5) +
  geom_smooth(method = "lm", se = FALSE) + 
  geom_line(data = bike_share_preds,
            aes(y = lwr), 
            color = "red", 
            linetype = "dashed") + 
    geom_line(data = bike_share_preds,
            aes(y = upr), 
            color = "red", 
            linetype = "dashed") 
```

While we produced confidence bands above, we can get these individual predictions using `predict()` in `R`:

```{r}
conf_for_mean <- predict(SLR_fit, 
                         newdata = data.frame(temperature = 22.22), 
                        interval = "confidence")
conf_for_mean 
pred_for_future <- predict(SLR_fit,
                           data.frame(temperature = 22.22), 
                           interval = "prediction")
pred_for_future 
```

We can interpret these intervals with statements such as

\newpage

# Model Selection

We now move into how to quantify model performance and, ultimately, how to choose which predictors, interactions, polynomial terms, etc. we should have in our model.

We've seen the use of hypothesis testing to understand the importance of predictors. We'll investigate a few other options now:

- Comparing differing MLR models using CV or a train/test split
- Considering variable selection methods such as best subset selection, forward/backward selection, or combinations of these methods

    + With those methods, we can use p-values or other model performance metrics ($R^2$, Adjusted $R^2$, AIC, BIC, etc.) to choose the model form which don't require a test set!
    + However, we could use the CV or train/test set idea to do these methods as well
    
- Utilizing penalized or regularized regression methods to choose fit our model

    + Some of these methods lead to automatic variable selection!
    + These methods require tuning of parameters
    
- Using dimension reduction techniques prior to model specification, or in conjunction with our model fitting

\newpage

## Test Set Performance

As we discussed earlier, we can use the data splitting methods (CV, Bootstrap, holdout, etc.) to evaluate model performance on unseen test data. 

This gives us a way to compare and choose between models when prediction performance is our major goal.

For example, the code below uses 5-fold CV, repeated 10 times, to estimate the test error for three competing models for predicting `rented_bike_count`. By repeating the CV process 10 times, we get a more stable estimate of prediction error.

- A model using only main effects. 

```{r}
set.seed(1001)
# control params
cv <- trainControl(method = "repeatedcv", 
                   number = 5, 
                   repeats = 10)
# training main effects
res_main <- train(log_rented_bike_count ~ hour + temperature + humidity + 
               wind_speed + visibility + rainfall + 
               snowfall + seasons + holiday, 
             data = bike_share, 
             method = "lm", 
             trControl = cv)
```

- A model with fewer variables with all main effects and their interactions.

    + In our `R` formula, we can pass `(pred + pred2)^2` to do this concisely!

```{r}
#training interaction model
res_interaction <- train(log_rented_bike_count ~ (hour + temperature + wind_speed + rainfall +  snowfall + holiday)^2, 
             data = bike_share, 
             method = "lm", 
             trControl = cv)
```

- Now a simpler model that may be more interpretable

```{r}
#training simpler model
res_simple <- train(log_rented_bike_count ~ hour + temperature + wind_speed + rainfall + seasons, 
             data = bike_share, 
             method = "lm", 
             trControl = cv)
```

Now we can investigate their repeated CV error to compare their fits.

```{r}
rbind(c("Main effect", round(res_main$results, 3)),
      c("Interaction", round(res_interaction$results, 3)),
      c("Simple", round(res_simple$results, 3)))
```

The main effects only model wins here! (This won't always be the case across problems you consider.)

\newpage

## Metrics Used with Traditional Variable Selection Methods

In this section, we discuss methods to select a *subset* of the available covariates that we believe to be related to the response. We look at more traditional methods that don't focus on predictive accuracy but use other model metrics or p-value based approaches to do so.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


### Metrics That Don't Adjust for Complexity

We can measure how well the model fits the training data by using the
following measures:

-   Residual squared error (RSE) (Not a good choice!)

-   Coefficient of determination, $R^2$ (Not a good choice!)

These aren't good choices. Let's discuss them and then see why.

#### RSE

We have seen RSE as the estimator of $\sigma$ in the previous sections.
In general, RSE quantifies the uncertainty in prediction on $Y$ from $X$
*even if the true regression parameters were known.* 

&nbsp;  
&nbsp;  
&nbsp;  

A small RSE would indicate a good regression fit. In the `bike_share`
data example with only `temperature` as predictor of `log_rented_bike_count` described above, we have
$RSE = `r round(sigma(SLR_fit),2)`$. 

Thus, even if we knew the true
regression line (assuming that the linear model is correct), a prediction of `log_rented_bike_count` based on `temperature` would still be off by
$`r round(sigma(SLR_fit),2)`$ units on average. 

In the `bike_share` data,
the mean value of `log_rented_bike_count` over all values of `temperature` is
`r round(mean(bike_share$log_rented_bike_count), 2)`. Thus we are making an error in the amount of `r round(sigma(SLR_fit)/mean(bike_share$log_rented_bike_count), 2)*100` percent.

The RSE is considered a measure of the **lack of fit** of the model. 

- Small values of RSE imply the predictions are close to the observed values which indicate good model fit. 
- Large values of RSE would indicate that the model did not fit the data well. 

However, it is often not clear what values of RSE are acceptable. 

The coefficient of determination ($R^2$) is another option to measure goodness of fit.

#### Coefficient of determination: $R^2$

- Define the *total sum of squares (TSS)* as $\sum_i(Y_i - \bar Y)^2$. Recall RSS is the residual sum of squares. Then

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- We can think of TSS as the amount of variability inherent in the response before the regression is performed. 
- RSS measures the amount of variability that is left unexplained after performing the regression.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Another way to interpret $R^2$ is that 

$$
R^2 = (\mbox{correlation coefficient between observed and predicted values})^2.
$$

\newpage

#### Issues with RSE and $R^2$

Usage of RSE and $R^2$ from the training set in model selection is **undesirable** as they will almost always choose the largest model possible.

That is, minimum RSE and maximum $R^2$ will almost always occur when number of predictors is largest.

- $RSE = \sqrt{RSS/(n-p-1)}$. $RSS$ will always decrease with the addition of more predictors.

- Likewise, $R^2$ will always increase with the addition of more predictors.

- To show this, let's just do a silly example. We'll fit an SLR model to the iris data with `Petal.Length` as a predictor and `Sepal.Width` as our response.

```{r}
quick_SLR <- lm(Petal.Width ~ Sepal.Length, data = iris)
#RSE
sigma(quick_SLR)
#RSS
(150-2)*(sigma(quick_SLR))^2
#R^2
cor(iris$Petal.Width,
    quick_SLR$fitted.values)
```

- Now we'll add in a non-sense predictor that has nothing to do with `Petal.Width`

```{r}
iris_extra <- mutate(iris, nonsense = rnorm(150, sd = 3))
#fit the model
quick_SLR_2 <- lm(Petal.Width ~ Sepal.Length + nonsense, 
                  data = iris_extra)
#RSE
sigma(quick_SLR_2)
#RSS
(150-3)*(sigma(quick_SLR_2))^2
#R^2
cor(iris$Petal.Width,
    quick_SLR_2$fitted.values)
```

- Notice that the value of $R^2$ is larger and the value of $RSS$ is smaller with the nonsense predictor! This shows we don't want to use these in selecting our model without considering the number of predictors or by using a test set.

### Metrics That Adjust for Complexity

Alternatively, there are metrics available that adjusts training performance metrics to balance both goodness of fit and model complexity/size, so that a separate test set is not needed for model comparison.

These approaches can be used to select among a set of models with different numbers of variables. Four such
metrics are:

\newpage


#### Adjusted $R^2$

Suppose we have a model with $d$ predictors. Recall that $R^2 = 1 - RSS/TSS$. Adjusted $R^2$ is defined as 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

where $d$ is the number of predictors in the model. Maximizing the adjusted $R^2$ is equivalent to minimizing $RSS/(n - d - 1)$. 

- Unlike $RSS$, which monotonically decreases as $d$ increases, $RSS/(n - d - 1)$ will increase and decrease as $d$ changes. 
- We choose the model with maximum adjusted $R^2$.

#### Information Criteria

AIC, BIC and $C_p$ all have the form for a model with $d$ predictors: 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

where $P(n, d, \widehat\sigma^2)$ is a penalty term involving sample size, number of predictors in the model and estimated error variance using the full model containing all predictors. 

The three metrics use the following form of $P$:
$$
P(n, d, \widehat\sigma^2) = \left\{ \begin{matrix} 2d\widehat\sigma^2,\;\; \mbox{ for } C_p, AIC \\ log(n)\,d\,\widehat\sigma^2, \;\; \mbox{ for } BIC \end{matrix}\right.
$$ 

We choose the model which gives **minimum** AIC/BIC values.

```{r aic, echo=FALSE, fig.cap="Example of model selection using AIC/$C_p$, BIC and adjusted $R^2$.", fig.width=9, fig.height=3, fig.margin = FALSE, fig.alt = 'The image contains three side-by-side line graphs, each showing a relationship between "Number of Predictors" (x-axis) and a different statistical measure (y-axis). The first graph has y-axis is labeled "Cp." It displays a general trend of decreasing values from left to right, then leveling out after a significant drop around 3 predictors. The second graph has y-axis is labeled "BIC." It similarly shows a sharp decline as the number of predictors increases, stabilizing after approximately 3 predictors. The third graph has its y-axis labeled "Adjusted R squared." This graph depicts an increasing trend that levels out after a visible rise at around 3 predictors, with values approaching a peak.'}
knitr::include_graphics("img/6_2.png")
```

It seems AIC and $C_p$ are equivalent from the formula above -- this
happens for linear regression model using least squares and normal
errors. However, AIC and BIC both have general forms involving
*log-likelihood* values, and can be computed for general regression
problems.

We can see from the penalty terms that BIC tend to have a higher penalty
than AIC/$C_p$ as $n$ increases. Thus BIC tends to produce smaller
models compared to AIC/$C_p$. The figure above shows an example of
model selection using AIC/$C_p$, BIC and adjusted $R^2$.


\newpage

## Traditional Variable Selection Methods

### Best subset selection

In this approach, we need to fit a separate least square model to *each*
of the possible combination of the $p$ predictors in the dataset, that
is, we need to fit all $2^p$ possible models. 

- We can either use CV/holdout or AIC/BIC to choose the best model. The following algorithm shows the best subset selection procedure.

1.  Start with the model with only intercept, and no other predictor.
    Denote the model by $M_0$.

2.  For $k = 1, \ldots, p$, fit all $C^p_k$ models with $k$ predictors,
    and pick the best model (smallest RSE, largest $R^2$ etc.). Denote
    the resulting model as $M_k$.

3.  Among the models $M_0, M_1, \ldots, M_p$, choose the best model
    using AIC, BIC, adjusted $R^2$ or CV.


\newpage


In `R`, we can use `regsubsets()` in the `leaps` package to perform best
subset selection. We demonstrate this procedure using `bike_share` data.

Note the usage of the argument `nvmax = 11`. This ensures that we will
search of subsets up to size 11 (Since `bike_share` data has 11 predictors - not including `date`).

```{r, warning=FALSE, message=FALSE}
library(leaps)
# Best model for each model size
bestmod <- regsubsets(log_rented_bike_count ~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature + solar_radiation + rainfall + snowfall + seasons + holiday, 
                      data = bike_share,
                      nvmax = 11)
# summary
mod_summary <- summary(bestmod)
```

If you look at the `summary()` of our `bestmod` object we can see which predictors were included in the subset of each size. The output is not fun to look at though!

Now we can use either AIC/BIC or adjusted $R^2$ to choose the best model among these 11 models. We can pull of the model criterion for each model.

```{r}
metrics <- data.frame(aic = mod_summary$cp,
                      bic = mod_summary$bic,
                      adjR2 = mod_summary$adjr2)
metrics |>
  round(3)
```

```{r, echo=FALSE, fig.cap="AIC, BIC and Adjusted $R^2$ for best subset selection in our bike share data.", fig.margin = TRUE, fig.width=6, fig.height=3, fig.alt = 'The image consists of three line graphs displayed side by side, labeled "AIC," "BIC," and "Adjusted R2." Each graph is plotted against the same horizontal axis labeled "size," with numerical tick marks ranging from 1 to 11.The first graph, labeled "AIC," has the vertical axis labeled "aic." It shows a downward-sloping curve starting around 6000 at size 1 and decreasing to just below 2000 by size 5, then leveling off as size increases to 11. The second graph, labeled "BIC," has the vertical axis labeled "bic." It presents a similar downward trend, beginning at -3000 at size 1, descending sharply to around -8000 by size 5, and flattening from there on. The third graph, labeled "Adjusted R2," has the vertical axis labeled "adjR2." It depicts an upward-sloping curve, starting around 0.3 at size 1 and rising steeply to approximately 0.6 by size 5, thereafter remaining flat through size 11.'}
dd <- cbind(size = 1:11, metrics)
p1 <- ggplot(dd, aes(size, aic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 13) + 
  ggtitle("AIC") + 
  scale_x_discrete(limits=as.factor(1:11))
p2 <- ggplot(dd, aes(size, bic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 13) + 
  ggtitle("BIC")+ 
  scale_x_discrete(limits=as.factor(1:11))
p3 <- ggplot(dd, aes(size, adjR2)) + geom_point() + 
  geom_line() + theme_bw(base_size = 13) + 
  ggtitle("Adjusted R2")+ 
  scale_x_discrete(limits=as.factor(1:11))

gridExtra::grid.arrange(p1,p2,p3, nrow=1)
```

The minimum AIC and adjusted $R^2$ occurs for model size $10$. For BIC it occurs for the model of size $9$. These two fitted models are below.

```{r}
#BIC best model
round(coef(bestmod, 9), 3) |>
  round(3) 
#AIC and adjusted R2 model
round(coef(bestmod, 10), 3) |>
  round(3)
```

As mentioned before, investigating all off the $2^p$ models can be
computationally intensive for large values of $p$. The following two
approaches provide computationally efficient alternatives using
*stepwise subset selection*.

\newpage

### Forward Stepwise Selection

*Forward stepwise selection* considers a much smaller set of models as compared to best subset selection. The
algorithm as as follows:

1.  Start with the model with only intercept, and no other predictor.
    Denote the model by $M_0$.

2.  For $k = 0, \ldots, p-1$,

    -   consider all $p-k$ models that adds one more predictor to the
        existing predictors in $M_k$.
    -   choose the best among these $p-k$ models; denote this model by
        $M_{k+1}$

3.  Among the models $M_0, M_1, \ldots, M_p$, choose the best model
    using AIC, BIC, adjusted $R^2$ or CV.


\newpage


The following code performs forward stepwise selection for `bike_share` data example.

```{r}
forward <- regsubsets(log_rented_bike_count ~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature +  solar_radiation + rainfall + snowfall + seasons + holiday, 
                      data = bike_share,
                      nvmax = 11,
                      method = "forward")
mod_summary <- summary(forward)
```

As before, we could look at the `summary()` of this object but it isn't nice to look at. Just as before, we can choose the
best model among these 11 models by looking at our criteria.

```{r}
metrics <- data.frame(aic = mod_summary$cp,
                      bic = mod_summary$bic,
                      adjR2 = mod_summary$adjr2)
```

```{r, echo=FALSE, fig.cap="AIC, BIC and Adjusted $R^2$ for forward stepwise selection in bike share data.", fig.margin = FALSE, fig.width=10, fig.height=4, fig.alt = 'The image consists of three side-by-side line graphs, each labeled at the top. The first graph, titled "AIC," shows a line that steeply decreases from above 6000 on the y-axis to just above zero, as the x-axis values increase from 1 to 11. The second graph, titled "BIC," displays a line that similarly decreases from above -3000 on the y-axis to near -8000, following the same x-axis range as the first. The third graph, labeled "Adjusted R2," depicts a line that gradually increases from around 0.3 to about 0.6 as the x-axis extends from 1 to 11.'}
dd <- cbind(size = 1:11, metrics)
p1 <- ggplot(dd, aes(size, aic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("AIC") + 
  scale_x_discrete(limits=as.factor(1:11))
p2 <- ggplot(dd, aes(size, bic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("BIC")+ 
  scale_x_discrete(limits=as.factor(1:11))
p3 <- ggplot(dd, aes(size, adjR2)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("Adjusted R2")+ 
  scale_x_discrete(limits=as.factor(1:11))

gridExtra::grid.arrange(p1,p2,p3, nrow=1)
```

In this case, we get the same exact results as with best subset selection (the same predictors are chosen in the models of size 9 and 10).

\newpage

### Backward Stepwise Selection

Like forward selection, backward selection also considers a smaller set
of models. It start from including all the predictors, and gradually
removes one predictor at a time. The following algorithm performs
backward stepwise selection.

1.  Start with the model with all the predictors included. Denote the
    model by $M_p$.

2.  For $k = p, p-1 \ldots, 1$,

    -   consider all $k$ models that contain all but one of the
        predictors in $M_k$, for a total of $k-1$ predictors.
    -   choose the best among these $k$ models; denote this model by
        $M_{k-1}$

3.  Among the models $M_0, M_1, \ldots, M_p$, choose the best model
    using AIC, BIC, adjusted $R^2$ or CV.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


The following code performs backward stepwise selection for
the `bike_share` data example.

```{r}
backward <- regsubsets(log_rented_bike_count ~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature +solar_radiation + rainfall + snowfall + seasons + holiday, 
                      data = bike_share,
                      nvmax = 11,
                      method = "backward")
# summary
mod_summary <- summary(backward)
```

As before, the summary shows which predictors give the best model (based
on training set performance) for each model size. Next we can choose the
best model among these 11 models.

```{r}
metrics <- data.frame(aic = mod_summary$cp,
                      bic = mod_summary$bic,
                      adjR2 = mod_summary$adjr2)
```

```{r, echo=FALSE, fig.cap="AIC, BIC and Adjusted $R^2$ for backward stepwise selection in bike share data.", fig.margin = FALSE, fig.width=10, fig.height=4, fig.alt = 'The image displays three side-by-side line graphs comparing different statistical metrics: AIC, BIC, and Adjusted R2. Each graph contains a horizontal x-axis labeled "size," ranging from 1 to 11, and a vertical y-axis representing the corresponding metric values. The first graph, labeled "AIC," shows a line decreasing sharply from a value above 10000 at size 1 to below 2500 at size 5, then gradually leveling off to near 0 by size 11. The second graph, labeled "BIC," shows a similar trend with the line starting above 0, decreasing steeply past -6000 at size 5, and then leveling off just above -8000 by size 11. The third graph, labeled "Adjusted R2," begins just above 0.2 at size 1, rising steadily to just above 0.6 by size 6, and remaining relatively flat through size 11.'}
dd <- cbind(size = 1:11, metrics)
p1 <- ggplot(dd, aes(size, aic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("AIC") + 
  scale_x_discrete(limits=as.factor(1:11))
p2 <- ggplot(dd, aes(size, bic)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("BIC")+ 
  scale_x_discrete(limits=as.factor(1:11))
p3 <- ggplot(dd, aes(size, adjR2)) + geom_point() + 
  geom_line() + theme_bw(base_size = 18) + 
  ggtitle("Adjusted R2")+ 
  scale_x_discrete(limits=as.factor(1:11))

gridExtra::grid.arrange(p1,p2,p3, nrow=1)

#gg <- gather(cbind(size = 1:12, metrics), 
#             key = metric,
#             value = value,
#             -size)
#ggplot(gg, aes(size, value)) + 
#  geom_point() + 
#  facet_wrap(vars(metric))
```

In the `bike_share` data example seen so far, the results match regardless of method we used. This is generally not going to hold!

As the model chosen by AIC, adjusted $R^2$, and BIC differ, we can pick the criteria we like the most (e.g. BIC for typically giving smaller models), and go with the corresponding best model.

\newpage

### Using the Holdout and Cross-Validation for Subset Selection

As mentioned before, apart from AIC/BIC/adjusted $R^2$, it is also
possible to use data splitting techniques such as a holdout set or CV for
model selection. 

Ideally, we can run CV for each of the $2^p$ models, and choose the one with best test error. However, such an approach can be computationally expensive.

Alternatively, we can use the algorithms presented above and use CV on
them. It is important to recall our discussion in the previous chapters
about proper implementation of CV: 

- the entire model building process, including any tuning, has to be applied to the training set. 
- We **can not** simply use steps 1 and 2 on the full data to get $M_0, \ldots, M_p$ and then just use CV on the final models. 
- The following paragraph is quoted verbatim from the textbook to emphasize
this important point (page 271).

> In order for these approaches to yield accurate estimates of the test error, we must use *only the training observations* to perform all aspects of model-fitting---including variable selection. Therefore, the determination of which model of a given size is best must be made using *only the training observations*. This point is subtle but  important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.

Thus we can think the **model size** as a tuning parameter here, since each training set might yield different models even if the size (number of predictors) remains the same. We use holdout/CV to choose the best model size, and then choose the best model of that size using the full data.

The algorithm of subset selection using a style *holdout method* is as follows:

-   Split the observations into training and test sets.

-   Apply best/forward/backward selection method on the training set.

-   For *each model size*, pick the best model, and compute test error using the test set.

-   Choose the optimal model size that has minimum test error.

-   Finally, perform best/forward/backward subset selection on the **full data set**, and select the best model of the size chosen in the previous step.

Let's illustrate this.

- Obtain the train/test split.

```{r}
set.seed(1001)
## Create test and training sets
data_split <- createDataPartition(bike_share$log_rented_bike_count, 
                                  p = 0.8, 
                                  list = FALSE)

test_set <- bike_share[-data_split, ]
train_set <- bike_share[data_split, ]
```

- Apply best subsets on the training data

```{r}
## Best subset selection on the training data
best_train <- regsubsets(log_rented_bike_count ~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature +solar_radiation + rainfall + snowfall + seasons + holiday,
                         data = train_set,
                         nvmax = 11)

train_sum <- summary(best_train)
```

- For each model size, estimate the test performance

    + A function to help us
    
```{r}
#We'll write a function to predict and estimate the error on the test set. 
#Inputs are 
#- model size (mod_size),
#- summary output of the selection process (reg_summary)
#- model matrix of the test data (test_model)
#- test set response (test_resp)
test_err <- function(mod_size, 
                     reg_summary, 
                     test_model,
                     test_resp){
  # get regression coefs
  betahat <- coef(reg_summary$obj, mod_size)
  # get best subset of the specified size
  sub <- reg_summary$which[mod_size, ]
  # Create test model matrix, prediction, test error
  model <- test_model[, sub]
  yhat <- model %*% betahat
  err <- mean((test_resp - yhat)^2)
  return(err)
}
```

- Apply the function to each model size

```{r}
#define the test model
test_model <- model.matrix(~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature +solar_radiation + rainfall + snowfall + seasons + holiday,
                           data = test_set)

#define the test response
test_resp <- test_set$log_rented_bike_count

#apply the function to each of the model sizes
hold_err <- sapply(1:11, #apply the function to these 
                  FUN = test_err, 
                  reg_summary = train_sum,
                   test_model = test_model, 
                   test_resp = test_resp)
```

- Now let's plot the errors
```{r, fig.cap = "Holdout error as a function of model size", fig.alt = 'The image depicts a line graph displaying a decreasing trend. The x-axis is the model size, ranging from 1 to 11. The y-axis is the holdout error, with values decreasing from 0.9 to below 0.6. The graph shows a steep decline initially, which gradually levels off as the model size increases.', fig.margin = FALSE, fig.width=5, fig.height=3}
plot(hold_err, type = 'b', pch=19, lwd=2)
```

- Choose the optimal model size and use that model size on a model fit to the full data set

```{r}
size_opt <- which.min(hold_err)
size_opt
#fit on the full data set
bestmod <- regsubsets(log_rented_bike_count ~ hour + temperature + humidity + wind_speed + visibility + dew_point_temperature +solar_radiation + rainfall + snowfall + seasons + holiday,
                      data = bike_share,
                      nvmax = 11)
#Use the optimal size
coef(bestmod, size_opt) |>
  round(3) 
```

In this particular example, we chose a 9-variable model. We refit to the full data set in order to obtain more accurate
estimates of the regression coefficient estimates. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


We can similarly use $V$-fold cross-validation as follows:

-   Split the data into $V$ equally sized folds.

-   For $v = 1, \ldots, V$:

    -   Set $v$-th fold as test set, and the remaining folds as training set.
    -   Apply best/forward/backward selection method on the training set.
    -   For *each model size*, pick the best model, and compute test error using test set.

-   Choose the optimal model size that has minimum average test error over $V$ folds.

-   Finally, perform best/forward/backward subset selection on the full data set, and select the best model of the size chosen in the previous step.

As a final note on correctly implementing cross-validation in general, we quote the following paragraph verbatim from *Elements of Statistical Learning*, **Section 7.10.2: The Wrong and Right Way to Do Cross-validation**:

> Consider a classification problem with a large number of predictors, as may arise, for example, in genomic or proteomic applications. A typical strategy for analysis might be as follows:

> 1.  Screen the predictors: find a subset of "good" predictors that show fairly strong (univariate) correlation with the class labels

> 2.  Using just this subset of predictors, build a multivariate classifier.

> 3.  Use cross-validation to estimate the unknown tuning parameters and to estimate the prediction error of the final model.

> Is this a correct application of cross-validation? Consider a scenario with N = 50 samples in two equal-sized classes, and p = 5000 quantitative predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%.

> We carried out the above recipe, choosing in step (1) the 100 predictors having highest correlation with the class labels, and then using a 1-nearest neighbor classifier, based on just these 100 predictors, in step (2). Over 50 simulations from this setting, the average CV error rate was 3%. This is far lower than the true error rate of 50%.

> What has happened? The problem is that the predictors have an unfair advantage, as they were chosen in step (1) on the basis of all of the samples. Leaving samples out after the variables have been selected does not correctly mimic the application of the classifier to a completely independent test set, since these predictors "have already seen" the left out samples.

Even though the discussion above is in the context of classification, the idea still applies to regression problems. Instead of misclassification error rate, we will be concerned about test MSE.

If we do need to screen predictors for a specific regression model, we need to do so *without involving response*, that is, using *unsupervised* methods. This should be done *before splitting data*. Again we quote a paragraph from *Elements of Statistical Learning*:

> In general, with a multistep modeling procedure, cross-validation must be applied to the entire sequence of modeling steps. In particular, samples must be "left out" before any selection or filtering steps are applied. There is one qualification: initial unsupervised screening steps can be done before samples are left out. For example, we could select the 1000 predictors with highest variance across all 50 samples, before starting cross-validation. Since this filtering does not involve the class labels, it does not give the predictors an unfair advantage.

\newpage

## Regularization/Shrinkage Methods

Another approach to selecting relevant predictors is to fit a model with
all $p$ predictors but put *constraints* on the regression coefficients.
This is called *regularization* of the estimates. It is done in such a
way that the resulting estimates are pulled towards zero -- this is
called *shrinkage*. Without going into mathematical details, it can be
shown that shrinking the coefficients towards zero in this manner
increases their bias but significantly reduces their variance.

A common regularization method is to add an extra *penalty term* to the
usual least squares criterion. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


### Ridge regression

Ridge regression shrinks the regression coefficients towards zero by
imposing a *quadratic penalty* or $L2$ penalty. The ridge regression coefficient
estimates are obtained by minimizing

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

where $\lambda \geq 0$ is a tuning parameter. (Note that the intercept $\beta_0$ is not penalized.) The penalty term
$\lambda\sum_{j=1}^p\beta_j^2$ is called a *shrinkage penalty*.

Here $\lambda$ controls the relative impact of the two terms on the
regression coefficient estimates. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Recall that $\mathbf{X}$ denotes the model matrix of the regression
problem. We can show that ridge regression solutions have a closed form
expression (if we also penalize the intercept):

\newpage

Recall the `Boston` data set from the `ISLR` package. Here we tried to predict `medv` using different predictors in the data. The figure below shows the estimated ridge regression coefficients for different values of $log(\lambda)$ for the model with `medv` as the response, and many *standardized* predictors. 

- The left most part of the plot corresponds to $\lambda = 0$, and shows the least squares estimates. 
- The right extreme of the plot represents a large value of $\lambda$, and we see that all the coefficients are very close to zero.

```{r ridge, echo=FALSE, fig.cap="Ridge regression coefficients for different values of lambda (log10 scale) for MLR model from the Boston data.", fig.alt = 'The image is a line graph illustrating the relationship between log(lambda) on the x-axis and estimated beta values on the y-axis. Lines represent different variables, each labeled on the left side with their respective names such as "rm", "zn", "chas", "nox", etc. The graph shows how each variables estimated beta changes as log(lambda) increases from 0 to approximately 9. The lines start at various points on the y-axis and generally converge towards zero as log(lambda) increases.', message=FALSE, warning=FALSE, fig.width=9, fig.height=6, cache=TRUE}

<<boston_prep>>

<<boston_ridge_fit>>

betahat <- t(as.matrix(betahat)[-1,])
lam <- boston_ridge$lambda
df <- data.frame(lam = lam, betahat)
cc <- coef(boston_ridge, s = min(lam))
dflab <- data.frame(lab = rownames(cc)[-1],
                    ypos = cc[-1],
                    xpos = -2.5)
gg <- gather(df, "Variable", "Beta", -lam)
ggplot() + 
  geom_line(aes(log(lam, base = 10), Beta, col = Variable), lwd = 1.2, data = gg) + 
  #geom_point(aes(log(lam), Beta, shape = Variable), lwd = 1.2) +
  theme_bw(base_size = 18) + 
  scale_color_viridis_d(option = "magma") + 
  xlab("log(lambda)") + 
  ylab("Estimated beta") +
  geom_text(aes(xpos, ypos, label = lab), 
            data = dflab, size = 5) + 
  theme(legend.position = "None")
```

We can also view the ridge regression problem as a *constrained
minimization problem*, $$
\mbox{minimize } \sum_{i}(Y_i - \beta_0 - X_{i1}\beta_1 - \ldots - X_{ip}\beta_p)^2 
$$ subject to the constraint $$
\sum_{j=1}^p \beta_j^2 \leq t,
$$ for some $t > 0$. The second formulation of ridge regression
explicitely puts constraint on the size of the regression coefficients.
The parameters $\lambda$ in the penalized formulation and $t$ in the
constraint formulation are connected via an one-to-one relationship.

Based on the second formulation, we can think of ridge regression as
minimizing RSS of a linear regression while preventing the regression
coefficients from getting too large or small. The parameter $t$
determines how large/small regression coefficients can become. If $t$ is
set to very large, then we are effectively allowing $\beta$'s to take
any value (equivalent to setting a small $\lambda$). On the other hand,
a small $t$ will force the $\beta$'s to be smaller and closer to zero
(equivalent to setting large $\lambda$).

In presence of multicollinearity, the corresponding $\beta$'s can become
wildly variable. A very large positive $\beta$ on one variable can be
canceled by a similarly large negative $\beta$ on another predictor
correlated to the first one. A size constraint imposed by $t$, fixes
this issue.

Before fitting the ridge regression model, we need to aware that scaling
the predictors is often needed. In least squares estimation,
scaling/standardizing a predictor does *not* change the overall
quality of the fit (e.g., $R^2$, $MSE$ etc). If we multiply a predictor
by a constant $c$, then the resulting least square coefficient estimate
will get multiplied by $1/c$. In other words, using least squares, the
quantity $X_j\widehat\beta_j$ will remain the same no matter how we
scale the $j$-th predictor. This is the reason we call least squares estimators *scale
    equivariant*.

Consider the simple example below where we use `lstat` and `5*lstat` as predictors. The coefficient estimate for the `5*lstat` model is simply $1/5$ the value for the `lstat` model.

```{r}
mod1 <- lm(medv ~ lstat, data = Boston)
mod2 <- lm(medv ~ I(5*lstat), data = Boston)
# Coefficients
cbind(original = mod1$coefficients[2],
      scaled = mod2$coefficients[2])
```

\noindent  In contrast, ridge regression estimates can change
substantially depending on scaling of the predictors. In fact, ridge
regression estimators $\widehat\beta^{ridge}_j$ will depend on the
scaling of the $j$-th predictor, the value of the tuning parameter
$\lambda$, *and* the scaling of the *other* predictors as well.
Therefore it is best to apply ridge regression *after we have
standardized each of the predictors*. This way, each predictor has
variance 1, and the final fit will not depend on the scale on which the
predictors are measured.

In addition, the ridge formulation does not penalize the intercept
$\beta_0$. This is due to the fact that the ridge estimates depend on
the center chosen for the responses. Specifically, in least squares
regression, if we add a constant $c$ to each of the responses $Y_i$, the
resulting predictions also shift by the same amount $c$. But this does
not happen in ridge regression if we penalize the intercept -- therefore
we do not penalize $\beta_0$.

It can be shown that, if we center each covariate, that is, we use
$X_{ij} - \bar X_j$ as predictors, then the estimator of the intercept
is simply the sample mean of $Y$: $\widehat\beta_0 = \bar Y$. The
remaining coefficients, $\beta_1, \ldots, \beta_p$, are estimated by a
ridge regression without intercept.

> For simplicity, we will henceforth assume that the model matrix $X$ does not include intercept when we are talking about Ridge Regression, and thus has only $p$ columns, not $p+1$. We will also assume that mean of each column is zero.

\noindent Under this assumption, we still have the same form of the
solution:
$(\widehat\beta_1, \ldots, \widehat\beta_p) = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{Y}$.
Furthermore, if we standardize predictors beforehand and, if they are
orthogonal to each other, it can be shown that
$\widehat\beta_j^{ridge} = \widehat\beta/(1 + \lambda)$.

In R, we can use the `glmnet()` function in the `glmnet` library.
Let us use the `Boston` data for example. Note the usage of `alpha = 0`
(this ensures we are fitting ridge regression as `glmnet()` can fit other
models like LASSO and elastic net as well).

```{r boston_prep, warning=FALSE, message=FALSE}
library(glmnet)
## model matrix (standardized) and response
medv <- Boston$medv
model_mat <- Boston[ , -13]|> 
  scale() |> 
  as.matrix()
```

```{r boston_ridge_fit}
## Fit ridge regression for a grid of lambda
grid <- 10^seq(-2, 10, length = 100)
boston_ridge <- glmnet(y = medv, 
                       x = model_mat,
                       alpha = 0,
                       lambda = grid)
betahat <- coef(boston_ridge)
```

```{r}
dim(betahat)
```

We constructed the model matrix by excluding intercept since it will be
automatically included by `glmnet()`. 

Here we have used a custom grid of $\lambda$ values. For each value of
$\lambda$, the output `betahat` contains the corresponding estimates of
the regression coefficients. The figure below shows the estimated
coefficients for different values of $\lambda$.

```{r ridgebv, echo=FALSE, fig.cap="Bias-variance trade-off of ridge regression. Figure taken from *Introduction to Statistical Learning*. Displayed are squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set. The horizontal dashed lines indicate the minimum possible MSE.", fig.margin = FALSE, fig.width=8, fig.height=4, fig.alt = "The graph is a line graph with lambda on the x-axis and MSE on the y-axis. Three curves are shown and a dashed line. The dashed line represents the minimum possible MSE. One curve represents the squared bias. This starts off with an MSE near 0 and eventually curves up and moves beyond the dashed line for larger values of lambda, indicating that a large lambda causes biased estimates. Another curve represents the variance. This curve starts off near the dashed line but moves down towards zero as lambda increases, indicating that a larger lambda causes less variation in our estimates. The last curve shown is an example test MSE curve from a simulated data set. This test MSe always stays above the dahsed line. It starts off higher, drops down to some optimal value, and then starts to trend further up"}
knitr::include_graphics("img/6_5.png")
```

How do we choose the "optimal" value of $\lambda$? We again come back to the 
*bias-variance trade-off*. Note that the penalty parameter $\lambda$
effectively controls the model complexity: 

- small values of $\lambda$ results in close to least squares fit (lower bias, higher variance)
- arge values of $\lambda$ results in almost an intercept-only model (higher bias, lower variance). 

Ideally, we would like to select $\lambda$ that minimizes test MSE. We
can use data splitting methods such as cross-validation (or holdout) to
do so. We choose a grid of candidate values of $\lambda$, and compute
the cross-validation (or holdout) error for each value. The optimal
$\lambda$ is the one with minimum test error. Finally, we refit the
model to the full data using the optimal $\lambda$.

We can use `glmnet.cv()` function to perform cross-validation. By
default, `glmnet.cv()` uses 10-fold CV.

```{r}
set.seed(1001)
grid <- 10^seq(-2, 10, length = 100)
cv_out <- cv.glmnet(x = model_mat, y = medv, 
                    alpha = 0, 
                    lambda = grid)
```

```{r, echo=FALSE, eval=FALSE}
####
## Function to perform CV to select lambda
## Input:
#         x: model matrix excluding intercept
#         y: response vector
#         lambda_grid: lambda values to search over
## Outout: a list
#         lambda_min: lambda with minimum error
#         cv_out: output object from cv.glmnet()
#         best_fit: fitted model with best lambda
####
reg_cv <- function(x, y, lambda_grid, alpha){
  # Perform cross-validation
  cv.out <- cv.glmnet(x, y, 
                      alpha = alpha, 
                      lambda = grid)
  # lambda with minimum CV error
  bestlam <- cv.out$lambda.min
  # Refit using best lambda
  out <- glmnet(x, y, 
                alpha = alpha,
                lambda = bestlam)
  # Return relevant objects
  return(list(lambda_min = bestlam,
              cv_out = cv.out,
              best_fit = out))
}

## Apply the function above
set.seed(1001)
# lambda values
grid <- 10^seq(-5, 10, length = 200)
# perform cv
out <- reg_cv(x, y,
                lambda_grid = grid,
                alpha = 0)
```

We can plot the results from CV process using the output of
`cv.glmnet()` output. The figure below shows the results.

```{r boscv, fig.cap="Cross-validation results for Boston data using ridge regression.", fig.alt = 'The image is a graph displaying a plot of Mean-Squared Error (MSE) against Log(lambda). The vertical axis represents the Mean-Squared Error, ranging from 20 to 90, while the horizontal axis depicts Log(lambda) with values ranging from -5 to 25. The plot shows a red line with dots that begins horizontally near a Mean-Squared Error of 30, quickly rises between Log(lambda) values 0 to 5, before leveling off again at a Mean-Squared Error close to 90. Error bars, depicted in gray, extend vertically from each red dot, illustrating variability. There are two vertical dashed lines within the plot at Log(lambda) values around 0. The left vertical line represents the "best" lambda chosen by minimizing the CV error. The vertical line a little to the right of this value represents the "one SE" chosen lambda.'}
# Plot cv results
plot(cv_out)
```

The "best" value of $\lambda$ can chosen by minimizing the CV error. The
left vertical line in the figure represents this value. We can see that there are a range of $\lambda$ values that give similar CV errors, and the dip in CV errors is not very
pronounced. This suggests that we might just as well use least squares
estimate in this case. 

Alternatively, we can also us the *one standard error* rule to choose $\lambda$: rather than choosing the $\lambda$ that gives the minimum test MSE, we would pick the largest $\lambda$ (less
model complexity) whose test MSE is within one standard error of the minimum test MSE. 

The right vertical line in the figure represents this value. The two values $\lambda$ are shown below, along with the estimated coefficients and the least squares coefficients for comparison.

```{r}
## lambda with minimum CV error/1 - SE
bestlam <- data.frame(min = cv_out$lambda.min,
             one_se = cv_out$lambda.1se)
bestlam 
## Refit ridge regression
# The cv_out object already has the full data fit
# for each lambda
ridge_min = predict(cv_out$glmnet.fit, 
                    type = "coefficients", 
                    s = bestlam$min)
ridge_1se = predict(cv_out$glmnet.fit, 
                    type = "coefficients", 
                    s = bestlam$one_se)
# Least squares
ols <- coef(lm(medv ~ model_mat))
betahat <- cbind(ridge_min, ridge_1se, ols)
colnames(betahat) <- c("min", "1se", "ols")
rownames <- attributes(betahat)$Dimnames[[1]]
betahat
## norm of betahat
sqrt( colSums(betahat^2) ) 
```

```{r, echo=FALSE, fig.width=5, fig.cap="Predictors arranged by absolute values of their estimated coefficients using 1-SE rule.", fig.alt = ""}
tb <- tibble(pred = rownames(betahat)[-1],
            est = abs(betahat[-1,2]))
tb <- tb %>% arrange(est)

ggplot(tb) + 
  geom_point(aes(y = pred, 
                 x = est),
             stat = "identity") + 
  theme_minimal(base_size = 18) + 
  xlab("abs(coefficient)") + 
  ylab("Predictor") + 
  scale_y_discrete(limits = tb$pred)
```


In general, when the true relationship between predictors and response
is linear, the least squares estimates will have low bias but can have high
variance, especially when $p$ is close to $n$. When $p>n$, least squares
estimates are not unique. 

In contrast, ridge regression will still perform well by trading off a small increase in bias for a large decrease in variance. Thus, ridge regression works best in situations  where the least squares estimates have high variance.

```{r, echo=FALSE, eval=FALSE}
#split <- initial_split(cbind(), prop = 0.8)
#test <- testing(split)
#train <- training(split)

index <- sample(1:nrow(Boston), 
                size = round(0.8*nrow(Boston)), 
                replace = FALSE)

cv.out <- cv.glmnet(x[index, ], 
                    y[index], 
                    alpha = 0, 
                    lambda = grid)
bestlam <- cv.out$lambda.min
train_out <- glmnet(x, y, alpha = 0, lambda = grid)
predict(train_out, type = "coefficients", s = bestlam)
pred <- predict(train_out, s = bestlam, newx = x[-index, ])
MSE <- mean((y[-index] - pred)^2)
#MSE

pred <- predict(train_out, s = 0, newx = x[-index, ], exact = TRUE,
                x = x[index,], y = y[index])
MSE <- mean((y[-index] - pred)^2)
#MSE

```

A major disadvantage of ridge regression is that it does not exclude any
variables from the final fitted model, that is, it always produces
non-zero estimates of the regression coefficients. Ridge regression will
not set any coefficients to exactly zero for any finite value of
$\lambda$. Thus ridge regression can not be considered as a *variable
selection* method. This is not a problem for prediction, but
interpreting of a model fit with many small but non-zero coefficients
can be difficult.

### LASSO regression

LASSO regression is another shrinkage method like ridge regression,
but LASSO uses a penalty term involving sum of the absolute values of
the regression coefficients, instead of sum of their squares. In
particular, LASSO estimates of $\beta_j$ are obtained by minimizing\
$$
\sum_{i}(Y_i - \beta_0 - X_{i1}\beta_1 - \ldots  - X_{ip}\beta_p)^2 + \lambda\sum_{j=1}^p|\beta_j|,
$$ for $\lambda \geq 0$. Due to the $L_1$ penalty term, there is no
closed form solution to the lasso problem. An equivalent way to
write the LASSO problem is in the form of a constrained minimization
problem, $$
\mbox{minimize } \sum_{i}(Y_i - \beta_0 - X_{i1}\beta_1 - \ldots - X_{ip}\beta_p)^2 
$$ subject to the constraint $$
\sum_{j=1}^p |\beta_j| \leq t,
$$ for some $t > 0$.

Much like ridge regression, lasso also shrinks the regression
coefficients towards zero. However, due to the $L_1$ penalty term
$\sum_j|\beta_j|$, some of the coefficients will be shrunk exactly to
zero. It is easier to see if we have standardized the predictors, and if
they are orthogonal to each other. In that case, the explicit lasso
solution is

$$\widehat\beta_j^{lasso} = sign(\widehat\beta_j)(|\widehat\beta_j| - \lambda)_+$$

Thus lasso does perform variable selection. As a result, models
generated from the lasso are generally much easier to interpret than
those produced by ridge regression. In other words, lasso generates
*sparse models* -- some coefficients are estimated to be *exactly zero*.

From the point of view of the constrained formulation, for large values
of $t$, we will effectively get the least squares estimates.
Specifically, it can be shown that if $t$ is chosen larger that
$t_0 = \sum_{j=1}^p |\widehat\beta_j|$ (where $\widehat{\beta_j}$ are the OLS estimates), then lasso estimates are
identical to least squares estimates. 

On the other hand, if we chose
$t = t_0/2$, then the least squares estimates are shrunk, on average, by
about $50\%$. 

The figure below shows the reason some lasso estimates are exactly set to zero while ridge estimates are not. Here $\widehat\beta$ represents least squares solution while while the blue
diamond and circle represent the lasso and ridge regression constraints.
For large values of $t$, the constraint region will contain
$\widehat\beta$ and thus both ridge and lasso estimates will be
identical to least squares (equivalently choosing $\lambda = 0$). For
smaller values of $t$, the least squares estimate may lie outside the
constraint region, like we see in the figure.

```{r lasso, echo=FALSE, fig.cap="Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regions for lasso and ridge, while the red ellipses are the contours of the RSS. Figure taken from \\textit{Introduction to Statistical Learning}.", fig.margin = FALSE, fig.width=8, fig.height=4, fig.alt = "Two plots are shown. On the left a cartesian x, y plane is shown with x-axis representing beta 1 values and the y-axis reprsenting beta 2 values. A blue diamond is shown around the origin with vertices at (1,0), (0,1), (-1,0), and (0, -1). This represents the restricted space allowed for the beta estimates with a LASSO penalty. The least squares solution is plotted in the upper right quadrant. Around this there are ellispes representing the beta 1 and beta 2 values corresponding to the possible LASSO solutions for larger and larger penalty weights. The ellipses naturally touch the diamond at a vertex, indicating the optimal LASSO solution setting a beta coefficient to 0. The second plot is a similar scenario except a circle is plotted around the origin representing the restricted space allowed for the beta estimates with a ridge penalty. The contours shown no longer naturally cross this space at a value where one estimated predictor is set to 0."}
knitr::include_graphics("img/6_7.png")
```

```{r lassohidden, warning=FALSE, message=FALSE, eval=FALSE, echo=FALSE}

<<boston_prep>>

## Fit lasso regression for a grid of lambda
grid <- 10^seq(-3, 7, length = 100)
boston_lasso <- glmnet(x = model_mat, y = medv,
                       alpha = 1,
                       lambda = grid)
beta_hat <- coef(boston_lasso)
```

The ridge and lasso estimates are the points where the contours
(ellipses) of the RSS intersect with the corresponding constraint
region. Since the constraint region of ridge regression is circular with
no sharp points, this intersection will not generally occur on an axis.
Thus ridge regression coefficient estimates will be non-zero. 

On the other hand, the lasso constraint region has corners at each of the axes.
So the ellipse will often intersect the constraint region at an axis.
When this occurs, one of the coefficients will equal zero. In higher
dimensions, many of the coefficient estimates may equal zero
simultaneously.

In `R`, we can use `glmnet()` with argument `alpha=1` to fit lasso
regression. The code presented in the ridge regression section will work
here with only change being `alpha=1`. The lasso fit for `Boston` data
is done below. The estimated regression
coefficients as $\lambda$ changes are shown in the plot below. The left extreme of the plot
corresponds to least squares fit ($\lambda = 0$).

```{r lassopath, echo=FALSE, fig.cap="Lasso regression coefficients for different values of lambda (log10 scale) for Boston data.", message=FALSE, warning=FALSE, fig.width=9, fig.height=7, cache=TRUE, fig.alt = 'The image is a line graph illustrating the relationship between log(lambda) on the x-axis and estimated beta values on the y-axis. Lines represent different variables, each labeled on the left side with their respective names such as "rm", "zn", "chas", "nox", etc. The graph shows how each variables estimated beta changes as log(lambda) increases from 0 to approximately 9. The lines start at various points on the y-axis and some become 0 as log(lambda) increases. As log(lambda) continues to increase, all of the estimates are eventually set exactly to 0.'}
<<lassohidden>>
  
betahat <- t(as.matrix(beta_hat)[-1,])
lam <- boston_lasso$lambda
df <- data.frame(lam = lam, betahat)
cc <- coef(boston_lasso, s = min(lam))
dflab <- data.frame(lab = rownames(cc)[-1],
                    ypos = cc[-1],
                    xpos = -3.5)
gg <- gather(df, "Variable", "Beta", -lam)
ggplot() + 
  geom_line(aes(log(lam, base = 10), Beta, col = Variable), lwd = 1.2, data = gg) + 
  #geom_point(aes(log(lam), Beta, shape = Variable), lwd = 1.2) +
  theme_bw(base_size = 18) + 
  scale_color_viridis_d(option = "magma") + 
  xlab("log(lambda)") + 
  ylab("Estimated beta") +
  geom_text(aes(xpos, ypos, label = lab), 
            data = dflab, size = 5) + 
  theme(legend.position = "None")
```

```{r, warning=FALSE, message=FALSE}
<<lassohidden>>
dim(beta_hat)
```

Like ridge regression, we need to carefully select $\lambda$. We can use
cross-validation (or holdout) methods to do so, as before.

```{r}
##  Lasso cross-validation
set.seed(1001)
grid <- 10^seq(-3, 7, length = 100)
cv_out <- cv.glmnet(x = model_mat, y = medv, 
                    alpha = 1,
                    lambda = grid)
```

```{r lassocv, fig.cap="Cross-validation results for Boston data using lasso regression.", fig.alt = 'The image is a graph displaying a plot of Mean-Squared Error (MSE) against Log(lambda). The vertical axis represents the Mean-Squared Error, ranging from 20 to 90, while the horizontal axis depicts Log(lambda) with values ranging from -8 to 15. The plot shows a red line with dots that begins horizontally near a Mean-Squared Error of 25, quickly rises between Log(lambda) values -1 to 2, before leveling off again at a Mean-Squared Error close to 90. Error bars, depicted in gray, extend vertically from each red dot, illustrating variability. There are two vertical dashed lines within the plot at Log(lambda) values around -4. The left vertical line represents the "best" lambda chosen by minimizing the CV error. The vertical line a little to the right of this value represents the "one SE" chosen lambda.'}
# Plot cv results
plot(cv_out)
```

The figure above shows the results of selection of $\lambda$
using 10-fold cross-validation. The $\lambda$ values with minimum CV
error and chosen by the one standard rule are shown below, along with
the corresponding coefficient estimates.

```{r}
## lambda with minimum CV error/1 - SE
bestlam <- data.frame(min = cv_out$lambda.min,
                      one_se = cv_out$lambda.1se)
bestlam 
```

```{r, fig.width=5, fig.cap="Predictors arranged by absolute values of their estimated coefficients using 1-SE rule from a lasso fit.", echo=FALSE}

<<gagan>>
  
<<chegi>>

```

```{r gagan}
## ## Refit lasso regression
# The cv_out object already has the full data fit
# for each lambda
lasso_min = predict(cv_out$glmnet.fit,
                    type = "coefficients",
                    s = bestlam$min)
lasso_1se = predict(cv_out$glmnet.fit,
                    type = "coefficients",
                    s = bestlam$one_se)
# Least squares
ols <- coef(lm(medv ~ model_mat))
betahat_lasso <- cbind(lasso_min,
                       lasso_1se,
                       ols)
colnames(betahat_lasso) <- c("min", "1se", "ols")
```

```{r}
rownames <- attributes(betahat_lasso)$Dimnames[[1]]
betahat_lasso
```

```{r chegi, echo=FALSE, eval=FALSE, fig.alt = ""}
tb <- tibble(pred = rownames(betahat_lasso)[-1],
            est = abs(betahat_lasso[-1,2]))
tb <- tb %>% arrange(est)

ggplot(tb) + 
  geom_point(aes(y = pred, 
                 x = est),
             stat = "identity") + 
  theme_minimal(base_size = 18) + 
  xlab("abs(coefficient)") + 
  ylab("Predictor") + 
  scale_y_discrete(limits = tb$pred)
```


\noindent Notice that the coefficient of `indus` is exactly set to zero,
and is thus excluded from the final model, when we choose $\lambda$ by
minimizing CV error. The one standard error rule gives a much larger
$\lambda$, and thus a sparser fit, excluding `indus`, `age` and `rad`
from the final model.

### Elastic net

A generalization of lasso and ridge is *elastic net*, which
minimizes $$  
\sum_{i}(Y_i - \beta_0 - X_{i1}\beta_1 - \ldots  - X_{ip}\beta_p)^2 + \lambda((1-\alpha)\sum_{j=1}^p\beta_j^2 + \alpha\sum_{j=1}^p|\beta_j|),
$$ for $\lambda \geq 0$ and $\alpha \in [0,1]$. Note that lasso and
ridge regressions are special cases of elastic net for $\alpha = 1$ and
$\alpha = 0$, respectively. 

The creators of this method, Zhou and Hastie (2005), suggest that
elastic net deals with correlated predictors more effectively than lasso
or ridge. The ridge penalty tends to shrink coefficients of correlated
variables towards each other, while lasso tends to pick one predictor to
be kept in the model while ignoring the rest. The elastic net
penalty is a compromise between these two phenomena. The first term the
the penalty encourages the correlated features to be averaged, while the
second penalty term encourages sparsity in the estimated coefficients of
the averaged features.

Elastic net often finds application in genomics (high-dimensional
problems) where $p>n$, and predictors (genes) are often have high
correlation among them.

As usual, we need to tune both $\lambda$ and $\alpha$ in this case. We
can use `glmnet()` to fit elastic net as well.

### Other variable selection methods

There are *many* other variable selection models in literature,
including several variations of lasso, such as

-   *adaptive lasso*: for estimation with less bias than ordinary
    lasso. It requires an initial estimate of the coefficients. The
    penalty term for each coefficient is then inversely weighted by the
    corresponding initial estimates. We can use the *penalty.factor*
    argument in glmnet() to do so. (Zou, H (2012). The Adaptive Lasso and Its Oracle Properties,
    JASA, 101, 1418 - 1429)

-   *group lasso*: for variable selection in groups of variables.
    For example, we might have a categorical variable with more than two
    levels. In variable selection, we might exclude/include all the
    dummy variable together. We can use R package `grpreg` for fitting
    group lasso. (Yuan, M. & Lin, Y. (2007), Model selection and estimation in
    regression with grouped variables, Journal of the Royal Statistical
    Society, Series B 68(1), 49 - 67)

-   *fused lasso*: does variable selection when the predictors have
    a natural ordering. For example, the predictors can be genes ordered
    by their chromosome location. Another example is when predictor is a
    function of time (functional data or time series). We can use the
    `genlasso` package here. (Tibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight, K.
    (2005), "Sparsity and smoothness via the fused lasso", Journal of
    the Royal Statistics Society: Series B 67(1), 91 - 108.)

-   *Smoothly clipped absolute deviations (SCAD)* and *Minimax
    concave penalty (MCP)*: produce sparse set of solution and
    approximately unbiased coefficients for large coefficients. Both
    methods are available in the `ncvreg` package. (Fan J and Li R. (2001). Variable Selection via Nonconcave
    Penalized Likelihood and its Oracle Properties. Journal of American
    Statistical Association, 96:1348 - 1360.)

There are many other methods available in literature. Readers are encouraged to explore according to their needs.

\newpage

## Dimension Reduction Methods

The variable selection and shrinkage methods discussed so far attempts
to reduce model variance in two ways: 

- by reducing the number of variables in the model (subset selection, lasso) 
- by shrinking regression coefficients toward zero (ridge, lasso). 

Another method to control model variance is to transform the original predictors to obtain new ones, and use them as covariates in the regression model. Typically, the number of new variables are less than the number of the original predictors. Thus these methods are called *dimension reduction* techniques.

Suppose our original predictors are $X_{i1}, \ldots, X_{ip}$. A typical
dimension reduction method has two steps:

1.  Create new predictors $Z_{i1}, \ldots, Z_{iM}$ by
    transforming/combining the original predictors. Usually we choose
    $M < p$, and thus reducing the dimension of the problem.

2.  Fit the regression model with the new $M$ predictors: $$
    Y_i = \theta_0 + Z_{i1}\theta_1 + \ldots + Z_{iM}\theta_M + \epsilon_i.
    $$

Depending on how we construct the new predictors gives rise to different
dimension reduction techniques.

In this section, we will discuss dimension reduction in the context of
building linear regression models. We will discuss dimension reduction
methods as a part of unsupervised learning in a later chapter.

### Principal Components Regression

Principal components regression uses *Principal Components Analysis
(PCA)* to derive new features from the original predictors. 

This essentially means we find linear combinations of the original predictors

$$
Z_{im} = a_{m1}X_{i1} + \ldots + a_{mp}X_{ip}, m = 1, 2, \ldots, M,
$$ 

that explain most of the variability in the data, where each linear combination is uncorrelated to each other. 

Typically we choose $M < p$, and the new variables, $Z_{im}$, are ordered according
to their importance. These can then be used as predictors in an MLR model. This method assumes that where the original predictors, $X_1, \ldots, X_p$ show most variation are in fact the directions associated with the response.

We won't go any further into this topic. You may want to revisit this after we go through the unsupervised learning section at the end of the course.


### Partial Least Squares

Just to give an example of another method that follow this dimension reduction idea, we will briefly discuss Partial Least Squares (PLS). 

PLS is a *supervised* approach that is similr to PCR. That is, PLS determines the linear combinations of the original predictors by making use of the response. Roughly speaking, the
PLS approach attempts to find directions that help explain both the
response and the predictors.

Assuming that the predictors have been standardized, PLS begins by 

- performing a *simple linear regression* of $Y_i$ on the $j$-th original predictor, $X_{ij}$, for each $j = 1, \ldots, p$. 
- The resulting estimates of slopes are denoted as $a_{11}, \ldots, a_{1p}$, respectively. 
- The first PLS component is constructed as 
$$
Z_{i1} = a_{11}X_{i1} + \ldots + a_{1p}X_{ip}.
$$ 
Thus the first PLS component places the highest weight on the
variables that are most strongly related to the response.

To construct the second PLS component, we regress each predictor
variable on the first PLS component, and take the residuals. 

- We can view these residuals as the remaining information that has not been captured
by the first PLS component. 

- The the second PLS component is computed in the same manner as before:

$$
Z_{i2} = a_{21}X_{i1} + \ldots + a_{2p}X_{ip}
$$ 

where $a_{2j}$ is the estimated regression coefficient of $X_{ij}$ from the simple linear
regression of the residuals (obtained above) on $X_{ij}$. 

We continue this process until we have all the $p$ PLS components. As in PCR, we
take the leading $M$ PLS components. A multiple linear regression is
then fitted with $Y$ as response and the $M$ PLS components as
predictors.

We won't go any further with this method.

\newpage

## High-dimensional data

So far, all the methods we discussed assume that the number of
predictors ($p$) is (much) less than the sample size ($n$). The
performance of these methods deteriorate as $p$ gets closer or exceed
$n$. 

Data sets containing more features than observations (or sometimes
number of features slightly smaller than $n$) are often referred to as
*high-dimensional*. 

In many fields, such as genomics and bioinformatics,
such high-dimensional data are common. For example, in genomics we
measure *single nucleotide polymorphisms (SNPs)* (these are individual DNA mutations that are relatively common in the population) and investigate their association with an outcome of interest. Typically, the number of SNPs are in hundred of thousands, but sample size is in hundreds.

When we have $p>n$, the usual least squares regression should not be
performed. This is because as $p>n$, the model matrix will not have full
column rank, and as such least squares does not provide unique
solutions. 

Furthermore, training set measures such as $R^2$ and $RSE$
will keep getting better and better as we add more predictors to the
model *regardless of whether the predictors are actually associated with
the response*. 

In fact, the model evaluation approaches that do not require a test set
(AIC, BIC, adjusted $R^2$), are also not appropriate for in the
high-dimensional setting due to instability of estimation of
$\widehat\sigma^2$ and RSS, both of which will be zero when
$p + 1 \geq n$. Thus we need alternative methods in this situation.


### Regression in high-dimensions

We can still apply *dimension reduction approaches* such as forward
stepwise selection[, ridge regression, the lasso, and principal
components regression. These methods avoid overfitting data using a less
flexible model.



### Interpreting Results in High Dimensions

Another issue in high-dimensional problem is multicollinearity, that is,
when one predictor can be expressed as a linear combination of the
others. When $p + 1 \geq n$, the predictors will *always* have
multicollinearrity -- any predictor can be written as a linear
combination of the others. This implies that we can not identify the
best coefficient in the regression model. At most, we can hope to assign
large regression coefficients to variables that are correlated with the
variables that truly are predictive of the outcome.

We should also be careful in reporting measures of model fit. We quote
the following paragraph from Chapter 6.4 of *Introduction to Statistical
Learning*.

> We have seen that when $p > n$, it is easy to obtain a useless model that has zero residuals. Therefore, one should never use sum of squared errors, p-values, $R^2$ statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting.

> It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or $R^2$ on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.
