---
title: "Linear Regression"
author: "Arnab Maity - Modified by Justin Post"
always_allow_html: yes
format: docx
include-in-header: 
      - header.tex
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

Packages used in this set of notes:

```{r setup, message=FALSE, warning = FALSE}
library(MASS)
library(klaR)
library(tufte)
library(tidyverse)
library(lubridate)
library(caret)
library(rsample)
library(ISLR2)
library(knitr)
library(AppliedPredictiveModeling)
library(kableExtra)
library(robustbase)
```

```{r  include=FALSE, message=FALSE}
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, 
                      cache.extra = packageVersion('tufte'),
                      kable.force.latex = TRUE,
                      fig.margin = TRUE, 
                      fig.height = 5, 
                      fig.width = 6)
options(htmltools.dir.version = FALSE)
```

\newpage

# Big Picture

For now, we stick to the regression task *where we have a quantitative
response*. We need to select a model form to work with. We'll start with
a basic parametric model - *a model with a stronger structural form* -
called the (Multiple) Linear Regression model.

$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \ldots + \beta_pX_{ip} +\epsilon_i,
$$

The model is much simpler compared to other modern techniques; however,
such models are still very useful in developing new methods. In fact,
many flexible nonparametric models can be thought of generalizations of
linear regression model. We could spend an entire course on this topic
if we wanted to!

Recall that we have two major goals:

-   Inference - 

&nbsp;  
&nbsp;  

-   Prediction - 

&nbsp;  
&nbsp;  

We'll see that most of the models we look at in this section can be used
for either of these tasks!

\newpage

# Introduction to the Linear Regression Model

We already investigated the simple linear regression model. This is a
model where we have a single predictor being used to model the response.

Let's reintroduce our data on bike sharing.

```{r}
bike_share <- read_csv("https://www4.stat.ncsu.edu/online/datasets/SeoulBikeData.csv",
                       local = locale(encoding = "latin1"))
bike_share |> 
  select(`Rented Bike Count`, everything())
```

As the variable names are non-standard in R, let's quickly modify them
to make our lives easier (and have better consistency). We'll also make
some variables `factors`, which are special character variables that
only take on a few values (or levels).

```{r}
bike_share <- bike_share |>
  rename("date" = "Date",
         "rented_bike_count" = `Rented Bike Count`,
         "hour" = "Hour",
         "temperature" = `Temperature(°C)`,
         "humidity" = `Humidity(%)`,
         "wind_speed" = `Wind speed (m/s)`,
         "visibility" = `Visibility (10m)`,
         "dew_point_temperature" = `Dew point temperature(°C)`,
         "solar_radiation" = `Solar Radiation (MJ/m2)`,
         "rainfall" = `Rainfall(mm)`,
         "snowfall" = `Snowfall (cm)`,
         "seasons" = "Seasons",
         "holiday" = "Holiday",
         "functioning_day" = "Functioning Day" 
         ) |>
  mutate(date = dmy(date), #convert the date variable from character
         seasons = factor(seasons),
         holiday = factor(holiday),
         functioning_day = factor(functioning_day))
```

Ok, let's graph the relationship the SLR model fits between
`rented_bike_count` and `temperature`.

```{r, message = FALSE, out.width='350px', fig.cap="Scatterplot with fitted SLR model overlayed", fig.alt = "A scatterplot between temperature on the x-axis and rented bike count on the y-axis. The plots shows a general upward trend with more spread in the rented bike count as the temperature increases. The overlayed SLR line has a positive slope and shows a strong linear relationship. The line does not fit the data well for temperatures below -10 degrees as it predicts a negative rented bike count in that region."}
bike_share |>
  ggplot(aes(x = temperature, y = rented_bike_count)) +
  geom_point(size = 0.5) +
  geom_smooth(method = "lm")
```

The equation for the model is fit in the software and given here.

```{r}
SLR_fit <- lm(rented_bike_count ~ temperature, data = bike_share)
summary(SLR_fit)$coefficients
```

\newpage

## Fitting the model

Once we've determined our model structure, we must **fit** or
**estimate** our model. This is generally done by minimizing some
criteria.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

## Conducting Inference Using the Model

If we want to do statistical tests, we usually need to make assumptions
about the error terms of our linear regression model. The assumptions
made to do inference are also dependent on how we fit the model.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

## Using the Model to Predict

Once the model is fit, we often want to predict. This can be done by
plugging in values of our predictor(s) we are interested in. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

Our plan will be to discuss the multiple linear regression model in
detail and then look at extensions to the model such as the LASSO, Ridge
Regression, Principle Component Regression, etc.

\newpage

# Multiple Linear Regression (MLR)

A linear regression model has the form $$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \ldots + \beta_pX_{ip} +\epsilon_i,
$$ where

-   $Y_i$ is a quantitative response

-   $X_{i1}, \ldots, X_{ip}$ are predictor variables 

-   $\epsilon_i$ is unobserved random error

-   $\beta_0$ is the *intercept* 

-   Coefficients $\beta_j, j = 1, \ldots, p$ are 'slope' terms
    associated with predictor $X_{ij}$

    -   Value of $\beta_j$ indicates the strength, and direction, of the
        *linear* relationship between $X_{ij}$ and $Y_i$
    -   $\beta_j$ is the expected change in the response for a unit
        change in $X_ij$, holding all other predictors constant
    -   If interactions or quadratics are included, this interpretation
        changes!

```{r}
MLR_fit <- lm(rented_bike_count ~ temperature + hour + wind_speed, data = bike_share)
summary(MLR_fit)$coefficients
```

## Assumptions on the Errors

### Mean Zero

Typically, we assume that the errors have mean zero, that is
$E(\epsilon_i) = 0$. Thus we can write the mean response as

$$
E(Y_i|X_{i1}, \ldots, X_{ip}) = \beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 + \ldots + X_{ip}\beta_p.
$$

We call the model described above **a linear model** because $E(Y|X)$
is **linear in parameters**, that is, linear in $\beta$.

We can have nonlinear terms of $X$, but the model would be still a
linear model!

-   For example, the so-called polynomial regression,

&nbsp;  
&nbsp;  
&nbsp;  

-   Fractional polynomial regression (introduced by Royston & Sauerbrei
    in their 2004 paper, *A new approach to modelling interactions
    between treatment and continuous covariates in clinical trials by
    using fractional polynomials.* in Stat Med)

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

-   Models with interaction such as

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

A non-linear model, in contrast, is a model such as

&nbsp;  
&nbsp;  
&nbsp;  

Hopefully, you can see that linear models do not just capture linear
effects of $X$, they can capture nonlinear functions of $X$ as well! The
figure below shows a few examples functions that can be captured by
appropriate linear models.

```{r example, echo=FALSE, fig.cap="Examples of functions that can be captured by appropriate linear models.", fig.width=5, fig.margin = TRUE}
x <- seq(0.2,4, len=51)
df <- tibble(x = x,
             Linear = 1 + x,
             Cubic = (1 + (x-2) + (x-2)^2 + (x-2)^3),
             Fractional = (1 - 50*x^2 + 500*(1/x^2))/1500,
             BSpline = c(splines::bs(x,5)%*%c(50,2,3,40,50)/3))

g <- gather(df, "Function", "fx", -x)
ggplot(g) + 
  geom_line(aes(x, fx, col = Function), lwd=1.2) + 
  theme_minimal(base_size = 18) + 
  ylab("f(x)") + 
  theme(legend.position = "top")
  
```

### Common Distributional Assumptions

A normality assumption on the errors, enable us to perform statistical
inference on the coefficients such as:

-   Constructing confidence intervals for $\beta_j$: a set/range of
    values which contain the "true" value of $\beta_j$ with high
    probability (in repeated sampling). This can be investigated by a
    $t$-statistic based confidence interval.

-   Perform hypothesis tests to determine whether the $j$-the predictor
    has any linear association with $Y$, $H_0:\beta_j = 0$ vs.
    $H_1:\beta_j \neq 0$. This can be investigated using a $t$-test.

-   Perform hypothesis tests to determine whether the *any* of the
    predictors has any linear association with $Y$,
    $H_0:\beta_1 = \ldots = \beta_p = 0$ vs. $H_1:$ at least on
    $\beta_j$ is non-zero. We can use $F$-test to answer this question.

Although not the only way to make the above inferences, our common
assumption on the distribution of the errors is

&nbsp;  
&nbsp;  
&nbsp;  

Conceptually, this idea
can be visualized in the simple linear regression model by the graph
below.

```{r, fig.cap = "At each value of the predictor, we assume the values of the response variable are normally distributed about the line. Figure from physicsforums.com", fig.width=4, echo = FALSE}
knitr::include_graphics("img/normal_errors.png")
```

\newpage

## Fitting a Linear Model

We denote the estimated coefficients as $\widehat\beta_j$ and the
estimated response as $\hat{Y}$.

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

How we determine these estimated coefficients depends! Let's go through
common methods for fitting the model.

### Least Squares

A common estimation procedure for the regression coefficients is the
*least squares* technique.

-   The difference between the observed and predicted values are called
    *residuals*,

&nbsp;  
&nbsp;  
&nbsp;  

-   We define the *residual sum of squares*, also known as
    *sum-of-squared errors (SSE)* as

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

We see that MSE is simply RSS divided by the sample size! (This could be
found over the training set or a test set.)

The *ordinary least squares (OLS)* procedure estimates
$\beta_j, j = 0, \ldots, p$ by minimizing the sum-of-squares

$$
\sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_{i1} - \ldots - \beta_pX_{ip})^2,
$$ with respect to $\beta_j$'s (resulting estimates are called the OLS
estimates).

For the *simple linear regression* model the solutions are easy to
derive with calculus.

$$
min_{\widehat{\beta}_0, \widehat{\beta}_1}\sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_{i1})^2,
$$

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

$$
\widehat\beta_1 = \frac{\sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2}
$$ 
$$
\widehat \beta_0 = \bar Y - \bar X \widehat\beta_1.
$$

-   For the general regression model with $p$ predictors, writing closed
    form expression is easier in matrix from. We can convert the
    original regression model in matrix form as $$
    \mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon},
    $$ where

-   $\mathbf{Y} = (Y_1, \ldots, Y_n)^T$ is the column vector of
    responses

-   $\mathbf{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)^T$ is the
    column vector of regression coefficients

-   $\mathbf{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)^T$ is the
    column vector of errors.

-   $\mathbf{X}$ is $n\times (p+1)$ matrix

    -   We call $\mathbf{X}$ the *model matrix* or *design matrix*

    -   The first column of $\mathbf{X}$ has all elements equal to $1$
        (corresponding to the intercept)

    -   For the remaining part of $\mathbf{X}$, each row corresponds to
        an unit/individual and each column corresponds to a predictor.

        -   For the special case of simple linear regression with one
            predictor $X$, the model matrix is of size $n \times 2$,
            $$\mathbf{X} = \begin{bmatrix} 1 & X_1 \\ \vdots & \vdots \\ 1 & X_n\end{bmatrix}$$

        -   In general, we have
            $$\mathbf{X} = \begin{bmatrix} 1 & X_{11} & \ldots & X_{1p}  \\ \vdots & \vdots & \ldots & \vdots \\ 1 & X_{n1} & \ldots & X_{np} \end{bmatrix}$$

For our bike count data, we can write out some of these terms for
clarity. Let's just work with the first 10 observations for brevity.

```{r}
bike_share_first_ten <- bike_share[1:10, ]
y <- bike_share_first_ten$rented_bike_count
X <- bike_share_first_ten |>
  mutate(intercept = rep(1, 10)) |>
  select(intercept, temperature, hour, wind_speed) |>
  as.matrix()
y
X
```

A *unique* minimizer of the sum-of-squares exists under certain
conditions!

$$
\widehat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}.
$$

We can find this in software easily.

```{r}
#%*% is the matrix muliplication operator in R
#solve() gives the matrix inverse
#t() gives the transpose
solve(t(X)%*%X)%*%t(X)%*%y
```

Which is the same as the basic `lm()` fit!

```{r}
MLR_first_ten <- lm(rented_bike_count ~ temperature + hour + wind_speed,
                    data = bike_share_first_ten)
MLR_first_ten$coefficients
```

#### Notes on Matrix Requirements

The estimator above depends on the term $(\mathbf{X}^T\mathbf{X})^{-1}$,
that is, the inverse of $(\mathbf{X}^T\mathbf{X})$. Such an inverse
exists only if $\mathbf{X}$ has full column-rank. Equivalently,
$\mathbf{X}$ must have the following two properties:

> > (C1) The sample size $n$ is larger than the number of regression
> > coefficients in the model, $p+1$.

> > (C2) None of the columns of $\mathbf{X}$ can be written as a
> > weighted sum (called a *linear combination*) of the remaining
> > columns.

\noindent If $\mathbf{X}$ violates either of these conditions, then a
*unique* least squares estimator does not exist. If $\mathbf{X}$
violates (C2) but not (C1), then we can replace
$(\mathbf{X}^T\mathbf{X})^{-1}$ by a *generalized inverse*[^1], but
there will be many estimators that minimize the sum-of-squares.
Interpreting them will be difficult in general. In practice, even if the
predictors are not perfectly correlated, their correlation can be high
enough to cause numerical instability. This issue is known as
*multicollinearity* among predictors. We can avoid this issue by
removing the collinear predictors from the model.

[^1]: Generalized inverse of $\mathbf{A}$ is a matrix $\mathbf{G}$ such
    that $\mathbf{A}\mathbf{G}\mathbf{A} = \mathbf{A}$. Although there
    are other definitions used by various authors.

If $\mathbf{X}$ violates (C1) then (C2) is automatically violated as a matrix can not have full column rank if it has more rows than columns.

In that case, one can take a few steps described below.

\newpage

### Fitting Via Maximum Likelihood (Optional)

If you are familiar with maximum likelihood, the solutions obtained from
the least squares optimization are the same as the solutions from the
model that assumes our iid Normal errors with constant variance.

Consider the SLR model with Normal errors. The likelihood is given by

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

We want to find the values of $\beta_0$, $\beta_1$, and $\sigma^2$ that
maximize this function. We can interpret these values as the 'most
likely values of the parameters to have produced the data we saw.'

Rather than optimize the likelihood directly, we usually optimize the
log-likelihood (natural log) given by

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

We can take the derivatives with respect to each of our parameters, set
the resulting equations equal to zero and solve simultaneously. But
wait! Notice that the maximizing the second term over $\beta_0$ and
$\beta_1$ is equivalent to minimizing our sum of squared errors (i.e.
least squares)!

Therefore, we get the same coefficient estimates for our intercept and
slope terms whether we use least squares or maximum likelihood!

\newpage

### Other Methods for Fitting the Regression Model

A drawback of least squares (and, hence, maximum likelihood with
Normally distributed errors)is that it is susceptible to influential
points. That is, points that have an unduly high impact on the
regression process!

The figure below shows an example of an outlier and its impact of the
regression fit.

```{r outlier, echo=FALSE, fig.cap="Example of an outlier (red point). Shown are two regression lines fit with OLS: before (red dashed) and  after (blue solid) removing the outlier.", message=FALSE, fig.width = 4}
set.seed(1001)
df <- tibble(x = rnorm(100), 
                 y = 0.1*x + rnorm(100)/2)
dfnew <- rbind(df, c(2, 3))
ggplot() + 
  geom_point(aes(x,y), 
             data = dfnew,
             col="red", size = 3, pch=17) + 
  geom_point(aes(x,y), data = df, size = 4) + 
  geom_smooth(aes(x,y), 
              method = "lm", 
              data = dfnew,
              col = "red", lty=2, lwd=2, se = FALSE) +
  geom_smooth(aes(x,y), method = "lm", data = df,
              lwd = 2, se = FALSE) + 
  theme_bw(base_size = 18)
  
```

We could simply remove the outlier from the data set but that must be
done carefully.

We can also use a different minimization criterion that is more robust
to influential points.

-   We can use the *least absolute deviation (LAD)* criterion, $$
    \sum_{i=1}^n |e_i|,
    $$

or

-   *Huber function*, which uses squared residuals when their values are
    small, but uses absolute value for large residuals (above a certain
    cutoff).

$$
\sum_{i=1}^{n}\left[ (1/2)e_i^2I_{|e_i^2|\leq \delta}(e_i) + \delta\left(|e_i|-(1/2)\delta\right)I_{|e_i^2|>\delta}(e_i)\right]
$$

```{r, echo = FALSE, fig.height = 3, fig.width = 3, fig.cap = "Visual of the loss associated with squared error loss and huber loss. Source: Wikipedia"}
knitr::include_graphics("img/huber_loss.png")
```

-   Let's compare the coefficient estimates of two MLR models: one using
    absolute error loss and one using squared error loss.

```{r}
#uses the robustbase package
mlr_ae_fit <- lmrob.lar(x = as.matrix(bike_share |> 
                                        mutate(intercept = rep(1, nrow(bike_share))) |>
                                                 select(intercept, temperature, wind_speed)),
                                      y = bike_share$rented_bike_count)
mlr_ae_fit$coefficients
```

Compare with the MLR fit:

```{r}
mlr_ls_fit <- lm(rented_bike_count ~ temperature + wind_speed, data = bike_share)
mlr_ls_fit$coefficients
```

Let's create a similar plot that showed how the OLS line was affected by
the outlier, but use the absolute error loss for the model.

```{r outlier2, echo=FALSE, fig.cap="Example of an outlier (red point). Shown are two regression lines fit with absolute error loss: before (red dashed) and  after (blue solid) removing the outlier.", message=FALSE, fig.width = 4}
set.seed(1001)
df <- tibble(x = rnorm(100), 
                 y = 0.1*x + rnorm(100)/2)
dfnew <- rbind(df, c(2, 3))
#df and dfnew objects available
mlr_ae_fit_full <- lmrob.lar(x = as.matrix(dfnew |> 
                                        mutate(intercept = rep(1, nrow(dfnew))) |>
                                                 select(intercept, x)),
                                      y = dfnew$y)
mlr_ae_fit <- lmrob.lar(x = as.matrix(df |> 
                                        mutate(intercept = rep(1, nrow(df))) |>
                                                 select(intercept, x)),
                                      y = df$y)
#coef from each
coef_full <- mlr_ae_fit_full$coefficients
coef_ <- mlr_ae_fit$coefficients
#obtain predictions for plotting
pred_full <- c(coef_full[1] + coef_full[2]*min(df$x), 
               coef_full[1] + coef_full[2]*max(df$x))
pred_ <- c(coef_[1] + coef_[2]*min(df$x), 
               coef_[1] + coef_[2]*max(df$x))
df_full <- tibble(x = c(min(df$x), max(df$x)), pred = pred_full)
df_<- tibble(x = c(min(df$x), max(df$x)), pred = pred_)

ggplot() + 
  geom_point(aes(x,y), 
             data = dfnew,
             col="red", size = 3, pch=17) + 
  geom_point(aes(x,y), data = df, size = 4) + 
  geom_line(aes(x, y = pred), data = df_full, col = "red",
            lty =2, lwd =2) +
  geom_line(aes(x, y = pred), data = df_, col = "blue",
            lty =1, lwd =2) + 
  theme_bw(base_size = 18)
  
```

In general, *robust regression* methods are often of interest if data are prone to large outliers or have a heavy tailed distribution.

\newpage

## Fitting non-linear Relationships in the Linear Model

As we mentioned, we can include polynomial terms, interaction terms, and more to allow our model to be more flexible. Let's look at how to do this in R and the implications for interpretation.

The `formula` notation in R is an extremely powerful, concise, way for specifying the structure of a model!

The general syntax is `response ~ predictor terms`

-   The right hand side contains the regression formula.


\newpage

### Including Polynomial Terms

Let's fit a quadratic term for `temperature` to our SLR model. (We'll use OLS as the default unless otherwise specified!)

```{r}
#fit the quadratic relationship
#equivalently, specify poly(temperature, 2, raw = TRUE)
quad_ols <- lm(rented_bike_count ~ temperature + I(temperature^2),
                 data = bike_share)
quad_ols$coefficients
```

```{r quad, echo=FALSE, fig.cap="Least squares fit using a quadratic term."}
dd <- quad_ols$coefficients
ggplot(bike_share) + 
  geom_point(aes(temperature, rented_bike_count)) + 
  geom_smooth(aes(temperature, rented_bike_count), method = "lm", 
              formula = y ~ poly(x,2, raw = TRUE),
              se = FALSE) + 
  theme_bw(base_size = 18)
```

We need to be careful of interpretations now! 

- $\beta_1$ is no longer the (expected) change in bike count for a unit change in temperature!

- Inference can still be done though! We must be careful that the colinearity of `temperature` with `temperature^2` is accounted for (note the correlation between the two is `r round(cor(bike_share$temperature, bike_share$temperature^2), 4)`)

&nbsp;  
&nbsp;  
&nbsp;  

### Including Interaction Terms

An **interaction** between two predictors implies that the effect of one predictor depends on the value of the other predictor.

- For example, consider the effect of `temperature` and `wind_speed` on the `rented_bike_count`

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

In `R`, we can fit interaction terms using the `X1*X2` notation. 

- The formula `Y ~ X1*X2` will include *main effects* of $X_1$ and $X_2$, and the *two-way interaction effect* $X_1X_2$ in the model. - Alternatively, we can explicitly specify the interaction term in the formula: "`Y ~ X1 + X2 + X1:X2`"

```{r}
#fit an interaction relationship
interaction_ols <- lm(rented_bike_count ~ temperature*wind_speed,
                 data = bike_share)
interaction_ols$coefficients
```

```{r echo = FALSE}
saddle_fit <- interaction_ols
```

+ $\hat{y}$ = (`r round(saddle_fit$coef[1],3)`) + (`r  round(saddle_fit$coef[2],3)`)temperature + (`r  round(saddle_fit$coef[3],3)`)wind_speed + (`r  round(saddle_fit$coef[4], 3)`)(temperature)(wind_speed)

+ For `temperature` = 0, the slope on `wind_speed` is (`r  round(saddle_fit$coef[3],3)`)`+0*` (`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3],3)`

+ For `temperature` = 50, the slope on `wind_speed` is (`r  round(saddle_fit$coef[3],3)`)`+50*`(`r  round(saddle_fit$coef[4],3)`) = `r  round(saddle_fit$coef[3]+50*saddle_fit$coef[4],3)`

+ For `temperature` = 100, the slope on `wind_speed` is (`r  round(saddle_fit$coef[3],3)`)`+100*`(`r  round(saddle_fit$coef[4],3)`) = `r round(saddle_fit$coef[3]+100*saddle_fit$coef[4],3)`

- Similarly, the slope on `temperature` depends on `wind_speed`!


```{r,webgl = TRUE, echo = FALSE, eval = FALSE}
# scatter plot with regression plane (rgl package)
library(rgl)
plot3d(x = bike_share$temperature, 
       y = bike_share$wind_speed, 
       z = bike_share$rented_bike_count,
					cex = 0.8, theta = 20, phi = 20, ticktype = "detailed",
                    xlab = "temp", ylab = "wind", zlab = "bike rentals",
					xlim = c(min(bike_share$temperature),max(bike_share$temperature)), 
					ylim = c(min(bike_share$wind_speed),max(bike_share$wind_speed)), 
					zlim = c(0, max(bike_share$rented_bike_count)),
					main = "Data with best saddle fit") 
#add surface
temperature <- seq(min(bike_share$temperature), max(bike_share$temperature), length = 100)
wind_speed <- seq(min(bike_share$wind_speed),max(bike_share$wind_speed), length = 100)
temp_wind <- expand.grid(temperature = temperature, wind_speed = wind_speed)
surface3d(x = temperature, y = wind_speed, z = predict(saddle_fit, newdata = temp_wind), color = "blue")
```

```{r, fig.align='center', fig.width=3, echo = FALSE, fig.cap = "Best saddle surface fit through the data."}
knitr::include_graphics("img/saddle_fit_1.png")
```

Note:

- We usually *retain all lower-order terms corresponding to an
interaction* in the model


\newpage

### Including Qualitative Predictors

So far we have only discussed models where $X$'s are continuous
variables. We can accommodate categorical predictors as well. To do so, we need to create new *binary* predictors representing each of the categories of the original predictors.


#### Main Effects with Qualitative Predictors

As an example, consider the binary variable *`holiday`*. This takes on "Holiday" or "No Holiday". 

- We can create an **indicator variable** to replace this variable
- This takes on 1 (Holiday) or 0 (No holiday)
- Then we can include this as a predictor in our model and our design matrix!
- `lm()` automatically creates the variable for us (not all modeling functions do this)

```{r binary}
MLR_binary_pred <- lm(rented_bike_count ~ temperature + holiday,
                      data = bike_share)
MLR_binary_pred$coefficients
```


&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


```{r echo=FALSE, fig.cap="Regression line for No Holiday (0) and Holiday (1) based on a linear model with main effects for holiday and temperature.", fit.alt = 'The image is a scatter plot graph displaying the relationship between rented bike count and temperature, with an overlay of holiday information. The x-axis represents temperature, ranging from -20 to 40 degrees, while the y-axis shows the rented bike count, ranging from 0 to over 3000. Data points are scattered across the plot, mostly concentrated between 0 and 40 degrees on the x-axis, and 0 to 2500 on the y-axis. Two trend lines are included: a red line indicating the trend for holidays and a blue line showing the trend for non-holidays. The lines are parallel with No holiday have a slightly higher intercept.'}
x_values <- seq(from = min(bike_share$temperature), 
                to = max(bike_share$temperature), 
                length = 2)
pred_df <- data.frame(temperature = rep(x_values, 2), 
                      holiday = factor(c(rep("Holiday", 2),
                                rep("No Holiday", 2))))
pred_df <- pred_df |> 
  mutate(predictions = predict(MLR_binary_pred, 
                               newdata = pred_df))

ggplot(bike_share, aes(temperature, rented_bike_count, color = holiday)) + 
  geom_point(alpha = 0.25, size = 1) + 
    geom_line(data = pred_df, aes(x = temperature, 
                                  y = predictions, 
                                  color = holiday),
              lwd = 2) +
  ggtitle("rented_bike_count ~ temperature + holiday")
```

#### Interaction Effects with Qualitative Predictors

Now consider what happens when we add an interaction term between `temperature` and `holiday`.

```{r binary_int}
MLR_binary_interaction <- lm(rented_bike_count ~ temperature*holiday,
                      data = bike_share)
MLR_binary_interaction$coefficients
```

Again, the indicator variable created is called `holidayNo Holiday`. This variable takes on 1 when `holiday` takes on `No Holiday`. 

We can then see that our model has different intercepts and different slopes!

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

```{r echo=FALSE, fig.cap="Regression line for No Holiday (0) and Holiday (1) based on a linear model with main effects and an interaction effect for holiday and temperature.", fig.alt ='The image is a scatter plot graph displaying the relationship between rented bike count and temperature, with an overlay of holiday information. The x-axis represents temperature, ranging from -20 to 40 degrees, while the y-axis shows the rented bike count, ranging from 0 to over 3000. Data points are scattered across the plot, mostly concentrated between 0 and 40 degrees on the x-axis, and 0 to 2500 on the y-axis. Two trend lines are included: a red line indicating the trend for holidays and a blue line showing the trend for non-holidays. The lines are not parallel. The No holiday line has a larger intercept but a smaller slope. The line starts off above the holiday line, intersecting around a temperature of 30 degrees Celsius.'}
ggplot(bike_share, aes(temperature, rented_bike_count, color = holiday)) + 
  geom_point(alpha = 0.25, size = 1) + 
    geom_smooth(method = "lm",
              lwd = 2,
              se = FALSE) +
  ggtitle("rented_bike_count ~ temperature*holiday")
```

#### Qualitative Predictors with More Than Two Levels

The ideas presented above can be generalized to categorical variables with more that two levels. Suppose that $Z_i$ is a variables with three levels, "L1", "L2" and "L3". Then we need to create $3-1=2$ *dummy variables* (or *indicator variables*): 
$$
Z_{i1} = I(Z_i = L1)  \mbox{ and } Z_{i2} = I(Z_i = L2)
$$
We do not need a new dummy for level "L3" since 
$$Z_{i1} = Z_{i2} = 0$$
would encode "L3".

In `R`, the first level alphabetically becomes the 'reference' level.

```{r}
MLR_qualitative_interaction <- lm(rented_bike_count ~ temperature*seasons,
                      data = bike_share)
MLR_qualitative_interaction$coefficients
```

- Finally, we should be cautious when there are many categorical variables in the data. Since we need to expand each of them into multiple indicator variables, the number of predictors can increase quickly.
- We need to be careful if we intend use data splitting methods like CV as well. Some levels may not appear in a training set in cases of levels that occur infrequently.
- Consider for example the `ames_raw` housing data from the `AmesHousing` R package. We consider the
variable `SalePrice` as response, and the rest as covariates. 

```{r}
print(AmesHousing::ames_raw, n = 5)
```

- Out of about 80 covariates, about 40 are categorical. However, after expanding each categorical variable into dummy variables, we will in fact have about $300$ predictors!
- If the sample size were smaller, say $n=500$, standard techniques like 5-fold CV (training set size will be
$400$) or $70\% - 30\%$ split (training set size $350$) may produce unreliable results due to number of predictors being close to training set size. In general, we should always check the size of the model matrix before choosing a proper data splitting method.


\newpage


## Inference

The linear model has many great properties that allow us to make inference! Let's cover a few of this briefly.

### Standard errors

The estimated coefficients, and thus the fitted regression line are random quantities since they change from sample to sample. Thus we need
a way to quantify the variability associated with the estimates!

To this end, we require additional assumptions on the error terms $\epsilon_i$.

For simplicity, we will use the following set of assumptions:


&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


Note: There are ways to relax these assumptions. For example, if we have large sample size, we might relax the normality assumption under certain conditions on $\mathbf{X}$.

The assumptions above imply that

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

One way to quantify the variability associated with estimation of $\beta$'s is to compute the *standard error (SE)* of the estimates, defined as, 

$$
SE(\widehat\beta_j) = \sqrt{var(\widehat\beta_j)}.
$$

For a general multiple linear regression model with model matrix
$\mathbf{X}$, the standard error of $\beta_j$ can be computed as
$$
SE(\widehat\beta_j) = \sigma\sqrt{(j+1)\mbox{th diagonal element of} (\mathbf{X}^T\mathbf{X})^{-1}}, j = 0, 1, \ldots, p.
$$
where $j=0$ corresponds to the intercept $\beta_0$.

- A special case is the simple linear regression, where the standard errors, given fixed values of $x_1, \ldots, x_n$ are 
$$
SE(\widehat\beta_0) = \sqrt{\sigma^2\left[\frac{1}{n} + \frac{\bar x}{\sum_{i=1}^n (x_i - \bar x)^2}\right]}, \,\, SE(\widehat\beta_1) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}}.
$$ 

- Examining the expression of standard errors of $\widehat\beta_0$ and
$\widehat\beta_1$ shows that SE will be smallest if the denominator
$\sum_{i=1}^n (x_i - \bar x)^2$ is maximized. 
- Thus, $SE$ is smallest if
the predictor values, $X_i$'s, are more spread out from their center.

Note that the standard error expressions depend on the error variance $\sigma^2$, which is an unknown quantity. 

- We can estimate $\sigma^2$, and $\sigma$, using the residual sum of squares. 

    + **Residual standard error (RSE)** 

$$
\widehat\sigma^2 = \frac{RSS}{n-(p+1)}~~~~\widehat\sigma = \sqrt{\frac{RSS}{n-(p+1)}},
$$ 

where $p+1$ is the number of columns in the model matrix $\mathbf{X}$. The resulting estimator of $\sigma$ is known as **residual standard error (RSE)**

+ We then plug-in $\widehat\sigma^2$ in place of $\sigma^2$ in
the expressions of standard errors, to obtain *estimated standard
errors*, $\widehat{SE}(\widehat\beta_j)$. For simplicity of
presentation, we will still denote the estimated standard error by
$SE(\widehat\beta_j)$.

The denominator $n-(p+1)$ in the expression of $\widehat\sigma^2$ merits some discussion. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

We can see the standard errors in `R` using the `summary()` function.

```{r}
MLR_ols <- lm(rented_bike_count ~ temperature + wind_speed,
                 data = bike_share)
summary(MLR_ols)
```

The first column of the output above are the estimates that we
have discussed before. The second column gives the standard errors of the estimates.

Alternatively, we can directly use the following code:

- `vcov()` function produces the matrix $\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$
- `diag()` then extracts the diagonal elements of the matrix
- Taking the square root yields the SEs
```{r}
se <- sqrt(diag(vcov(MLR_ols)))
se
```

The last part of the summary output above shows
`Residual standard error: ` `r summary(MLR_ols)$sigma`. This is our $\widehat\sigma$. 

- We can use the `sigma()` function on the model fit to directly
obtain this value.

```{r}
sigma(MLR_ols)
```

The standard errors, along with normality assumption on the errors, further enable us to perform statistical inference on the coefficients:

-   Construct confidence intervals of $\widehat\beta_j$: a set/range of values which contain the "true" value of $\beta_j$ with high probability. This can be investigated by a $t$-statistic based confidence interval.

-   Perform hypothesis tests to determine whether the $j$-the predictor has any linear association with $Y$, $H_0:\beta_j = 0$ vs. $H_1:\beta_j \neq 0$. This can be investigated using a $t$-test.

-   Perform hypothesis tests to determine whether the *any* of the
    predictors has any linear association with $Y$,
    $H_0:\beta_1 = \ldots = \beta_p = 0$ vs. $H_1:$ at least on
    $\beta_j$ is non-zero. We can use $F$-test to answer this question.

We discuss each of the items below.

\newpage

### Confidence interval

Without going into mathematical details, we can obtain a confidence interval for $\beta_j$ using the standard errors.

**Confidence interval for $\beta_j$**

- A $100(1-\alpha)\%$ confidence interval for $\beta_j$ is

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- Again, the number $n-(p+1)$ is the same as we see in the
estimator $\widehat\sigma^2$. This number is called the *degrees of freedom* of the $t$-distribution used above.

```{r tcdf, echo=FALSE, fig.margin = T, fig.cap="t PDF and quantiles. The shaded region has area p, and the x-axis value corresponding to the solid vertical line represents the  (1-p)-quantile. In this example, we have p = 0.05, and the vertical line represents the upper 0.95-quantile.", fig.height=5, fig.width=6}
curve(dt(x, df  = 3), from = -5, to = 5, lwd=2, ylab = "t_3 PDF")
# shaded region
left <- qt(0.05, df = 3, lower.tail = FALSE)
right <- 5
 
x  <- c(left, seq(left, right, 0.01), right)
y  <- c(0, dt(seq(left, right, 0.01), df=3), 0)
polygon(x,y, col="steelblue")
```

In `R`, we can use the function `confint()` on our model fit to obtain *individual* confidence intervals for the regression coefficients.

```{r}
## 95\% confidence intervals
ci <- confint(MLR_ols, level = 0.95)
ci
```

- We can interpret the intervals for intercept by saying that when `temperature` and `wind_speed` are 0, on average `rented_bike_count` is estimated to be between `r round(ci[1,1], 2)` and `r round(ci[1,2], 2)` with $95\%$ confidence. 

- We can interpret the interval for the `temperature` slope as follows: we estimate, with $95\%$ confidence,
that `rented_bike_count`, on average, decreases between `r round(abs(ci[2,1]), 2)` and
`r round(abs(ci[2,2]), 2)` for 1 unit increase in `temperature` if we hold `wind_speed` constant.

\newpage

### $t$-test

We can also perform hypothesis tests on the regression coefficients. 

- If our main interest is in testing the association between $X_j$ and $Y$, we test for 
$$
H_0:\beta_j = 0 \mbox{ vs. } H_0:\beta_j \neq 0. 
$$ 

- Note that $\beta_j = 0$ implies that $X_j$ is not in the model, and thus not associated with $Y$. We can use the $t$-statistic to perform the test:

**$t$-test for** $H_0:\beta_j = 0 \mbox{ vs. } H_0:\beta_j \neq 0$

- The $t$ test statistic is

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- The test statistic measures how far away the estimated value of
$\beta_1$ is from zero compared to the variability of the estimate
measured by $SE(\widehat\beta_1)$. 
- We expect the statistic $t$ to have a
$t_{n-(p+1)}$ distribution *if* $H_0$ is true. 
- We reject $H_0$ if the observed value of $t$ is very large or very small compared to what we expect from a $t_{n-(p+1)}$ distribution. 

$$
\mbox{p-value} = P(t_{n-(p+1)} > |t|) = 2[1 - F(|t|)],
$$ 
where $F(|t|)$ is the CDF of the $t_{n-(p+1)}$ distribution evaluated
at $|t|$. 

- We reject $H_0$ if the p-value if smaller than $\alpha$, where
we set $\alpha$ to be a small value (usually set to $5\%$). 

- The quantity $\alpha$ is called the type I error of the test (probability of rejecting $H_0$ when it should not be rejected).

```{r tpv, echo=FALSE, fig.margin = T, fig.cap="Two-tailed p-value for a t-test."}

curve(dt(x, df  = 3), from = -5, to = 5, lwd=2, ylab = "t_{n-1} PDF")
# shaded region
left <- qt(0.05, df = 3, lower.tail = FALSE)
right <- 5
 
x  <- c(left, seq(left, right, 0.01), right)
y  <- c(0, dt(seq(left, right, 0.01), df=3), 0)
polygon(x,y, col="steelblue")
left <- -5
right <- qt(0.95, df = 3, lower.tail = FALSE)
 
x  <- c(left, seq(left, right, 0.01), right)
y  <- c(0, dt(seq(left, right, 0.01), df=3), 0)
polygon(x,y, col="steelblue")
```

In R, we can use the `summary()` function to obtain the test results.

```{r}
summary(MLR_ols)
```

We see that the p-value associated with $\beta_1$ (coefficient of
`temperature`) is very small, and thus reject $H_0$. We conclude that `temperature`
has a linear relationship with `rented_bike_count`, even in the presence of `wind_speed`.

- Another way to test $H_0:\beta_j = 0$ at $\alpha = 0.05$ is to check whether the value zero is in the $95\%$ confidence interval of $\beta_j$ or not.

```{r}
confint(MLR_ols)
```

**Remember**: Caution must be taken to interpret results from model with interaction terms. 

- Let us investigate the summary of a model fit on the `Boston` housing data we looked at previously. 
- Let's use main effects and an interaciton between `lstat` and `age`

```{r}
boston_interaction <- lm(medv ~ lstat*age, data = Boston)
summary(boston_interaction)
```


\newpage


### $F$-test

In the multiple linear regression with $p$ predictors, we investigate whether the linear model is at all useful by testing
$H_0:\beta_1 = \ldots = \beta_p = 0$ vs. $H_1:$ at least on $\beta_j$ is
non-zero. This is called the **global F-test**

- In general, we can test for *any subset* of the predictors using $F$-test, that is, $$
H_0:\beta_{p - q+1} = \ldots = \beta_p = 0,
$$ where we are testing the effects of the last $q$ predictors (last $q$
columns of $\mathbf{X}$).

**$F$-test for** $H_0:\beta_{p - q+1} = \ldots = \beta_p = 0$

- Let $RSS_0$ be the residual sum of squares of the model where we fit all the predictors **except the last $q$**. 
- Recall $RSS$ denotes the residual sum of squares for the full model. 
- The $F$-statistic is

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  


Let us revisit the interaction model for `rented_bike_count` with `temperature`, `holiday` and `temperature*holiday`
as predictors 

- Mathematically, we write the model as $$
Y_i = \beta_0 + X_{i1}\beta_1 + X_{i2}\beta_2 + X_{i1}X_{i2}\beta_3 + \epsilon,
$$ where $X_{i1}$ and $X_{i2}$ correspond to `temperature` and a `holiday` indicator variable, respectively. 
- Suppose we want to test the overall model utility, that is,
jointly test the effects of all the three terms,
$H_0:\beta_1 = \beta_2 = \beta_3 = 0$. The F-test results can be found at the bottom of the summary output for this model:

```{r}
summary(MLR_binary_interaction)
```

Note the line: "`F-statistic:  1206 on 3 and 8756 DF,  p-value: < 2.2e-16`". 

- The very small p-value indicates that we reject $H_0$, and the model is useful in predicting $Y$.

- Alternatively, we can fit two models: the full model (`MLR_binary_interaction`) and another model with only an intercept term. 

- Then we can conduct $F$-test ourselves using the `anova()` function in `R`.

```{r}
# Reduced model with only intercept
MLR_red <- lm(rented_bike_count ~ 1, data = bike_share)
anova(MLR_red, MLR_binary_interaction)
```

- Now suppose we want to test "whether *holiday* may have any association with response" or not. Since we have the interaction term, we have to test
for both main and interaction effects of `holiday`. 
- We can use the $F$-test method to test
$H_0:\beta_2 = \beta_3 = 0$

```{r}
# Reduced model with only intercept and temperature is in SLR_fit
anova(SLR_fit, MLR_binary_interaction)
```

Here we have observed $F$-statistic $14.536$ with a p-value of
$4.985e-07$. We reject $H_0$, and can
conclude that `holiday` does have association with the `rented_bike_count`, even in the prescense of `temperature`.

- Note: When $q=1$, that is, we are testing for one predictor only, the $t$-test and $F$-test are equivalent. 
- In fact, the square of the $t$-statistic will give the $F$-statistic!

\newpage

#### Why is the Global $F$-test Needed?

So why do we need the $F$-test when we can examine $t$-test results for each predictor in our model? You might think that if at least one $t$-test gives a significant result we know our model is useful.

- The main issue is the *type I error* or *level* of the tests. That is, the probability of rejecting a null hypothesis when the null hypothesis is true.
- Typically, we set $\alpha = 0.05$ as level of the tests. 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  
    
- Using the F-statistic to test the model utility first does not suffer from this problem because it adjusts for the number of predictors. 

    + If $H_0$ is true, there is only a $5\%$ chance that the F-statistic detect a false significance regardless of the number of predictors.

\newpage

### Model Diagnostics

To estimate standard errors, and to perform inference, we needed certain assumptions on the errors and the model as a whole:

-   The relationship between $Y$ and $X$'s is reasonably described by the linear regression model.

-   Constant error variance $\sigma^2$.

-   Errors are iid normally distributed.

Other practical issue include:

-   Multicollinearity among predictors

-   Presence of influential points

For our inference to be valid, we need to make sure the assumptions mentioned above are satisfied to some extent. We present some diagnostics methods to address each of the issues mentioned above.

#### Deviation from Linearity

To evaluate whether the relationship posited by the fitted regression model actually captures the true relationship, we can use *residual plots*. 

- For simple linear regression, we can plot the residuals vs. the predictor. 
- For multiple linear regression, it is easier to plot residual
vs. the predicted responses. 

If the model specification is adequate, there should be *no clear pattern* in the residual plot. 

In contrast, any pattern in the residual plot would indicate the model does not capture the relationship between $X$ and $Y$ well. 

- In this case transforming data (either $X$ or $Y$ or both) might prove useful.

- Alternatively, a non-linear/non-parametric regression might be considered as well.

- In an `R` session, you can use `plot()` on a fitted `lm` object to obtain some basic diagnostic plots. We'll discuss these plots for one of our models after discussing other diagnostics.

#### Non-constant Variance of Errors

The standard errors, confidence intervals, and hypothesis testing
procedures discussed so far depends on the assumption of constant
variance of the errors: 

&nbsp;  
&nbsp;  
&nbsp;  
&nbsp;  

- We call such errors *homoscedastic*. 
- If errors have different variance, such phenomenon is called *heteroscedasticity*.

Large deviations from homoscedasticity may require a transformation of our variable(s).


#### Normality of errors

Normality of errors are needed for development of confidence intervals and testing procedures discussed above. However, this assumption can be relaxed for large enough sample size. 

- Usually, visual displays such as a **normal Q-Q plot** of the residuals is used to check normality assumption.

- If the points align with the diagonal line well enough, we can conclude
that the normality assumption is satisfactory. However, keep in mind
that Q-Q plot is merely a visual tool, and often samples from non-normal
distributions can produce normal like Q-Q plot (and vice-versa).


#### Influential points

Outliers and high leverage points can be detected using residual plots
with *studentized residuals* and *leverage statistics*.

Recall our linear model is
$\mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}.$ We estimate
the regression parameter as
$\widehat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}.$
Thus we we can predict the entire response vector $\mathbf{Y}$ by
plugging-in $\widehat{\mathbf{\beta}}$ as $$
\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\mathbf{\beta}} = [\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]\mathbf{Y} = \mathbf{H}\mathbf{Y},
$$ where
$\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$. The
matrix $\mathbf{H}$ is called the *hat matrix*. Detection of influential
points depend on the following two results:

- It can be shown that variance of the $i$-th residual, $var(Y_i - \widehat Y_i)$ is equal to $\sigma^2 (1 - {\mathbf{H}}_{ii})$, where ${\mathbf{H}}_{ii}$ is the $i$th diagonal entry of $\mathbf{H}$.

- The $i$-th diagonal entry of $\mathbf{H}$, ${\mathbf{H}}_{ii}$, is called the *leverage* of the $i$-th observation.

We define *studentized residuals* as residuals divided by their standard
deviations. 

- We can plot the studentized residuals against fitted values
to detect outliers. 
- Observations whose studentized residuals are quite far away from the rest (say more than three in absolute value) are possible outliers. 

#### Investigating Assumptions

Let's consider our MLR model for `rented_bike_count` that included `temperature`, `holiday`, and their interaction as predictors.

Running `plot()` on the fitted object yields the following four plots:
```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_res_fitted.png")
```

- This plot shows a reasonably distinct pattern in the residuals. There are a few more 'large' positive residuals as we have higher fitted values. This indicates that our model may not be doing as well as it could be at modeling the relationships in the data.
- We also see a definitive 'trumpet' shape. This indicates a non-constant variance.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_qq.png")
```

- This qq-plot shows that the middle of the data roughly falls along the diagonal but there is a clear deviation for the lower and upper values. With a large sample size, this isn't too concerning though.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_std_res_fitted.png")
```

- This plot is similar to the first but a bit more sophisticated as it uses the square root of the standardized residuals. 
- These standardized residuals are given by taking the residual and dividing by the standard deviation times the square root of one minus the leverage: $R_i/(s\sqrt{1-h_{ii}})$
- We again see a bit of a pattern through the residuals with smaller residuals for lower fitted values and larger for high fitted values.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_std_res_lev_log.png")
```

- Nothing too striking appears here overall. 

Overall, adding more predictors into the model would likely be quite useful here!

As mentioned earlier, we can use transformations to help make these assumptions more reasonable. Let's do a **log** transform of our response (natural log).

```{r}
bike_share <- 
  bike_share |>
  mutate(log_rented_bike_count = log(rented_bike_count+1)) #add an offset for log(0) issues
MLR_log_int <- lm(log_rented_bike_count ~ temperature*holiday, data = bike_share)
```

First, let's look at the summary of the model fit.
```{r}
summary(MLR_log_int)
```

Great, now let's look at the diagnostic plots to see if any issues were fixed.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_res_fitted_log.png")
```

-Generally, our residuals seem to have far less of a trend. 
- However, there are some conserning observations that have very large residuals. These should be investigated further

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_qq_log.png")
```

- This got much worse but a little better at the same time. Again, the large sample size indicates we don't have to be as concerned with the normality assumption.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_std_res_fitted_log.png")
```

- Here we again see vast improvement for the most part. 
- The chunk of odd values at the top warrant further investigation.

```{r, echo = FALSE, fig.width = 4}
knitr::include_graphics("img/interaction_std_res_lev_log.png")
```

- This looks a good bit worse due to the large negative observations. However, that may mostly be due to the group of weird observations mentioned earlier. 

#### Collinearity

Lastly, we should discuss **collinearity**. This refers to high correlation between two or more predictors.

- Presence of such high correlation may lead to numerical instability of
linear model fitting, reducing accuracy of estimation of regression
coefficients, and reducing power of hypothesis tests.

Consider the two linear model fits for `Boston` data: 

- Model A: `tax` and `rad` on `medv`
- Model B: `lstat` and `tax` on `medv` 

The results are shown below:

```{r, echo=FALSE}
# Model A: medv on everything
mod1 <- lm(medv ~ tax + rad, data = Boston)
# Model B: medv ~ everything except tax
mod2 <- lm(medv ~ lstat + rad, data = Boston)
a <- broom::tidy(mod1)
b <- broom::tidy(mod2)
#cc <- full_join(a, b, "term")
cc <- rbind(a,b)
cc[,-1] <- round(cc[,-1], 4)
#cc <- column_to_rownames(cc, "term")
cc %>% kbl(booktabs = TRUE) %>%
#  kbl(booktabs = TRUE, 
#      format = "latex",
#      table.envir='table*') %>%
#  kable_styling(latex_options = c("hold_position"),
#                full_width=F) %>%
  pack_rows("Model A", 1, 3) %>%
  pack_rows("Model B", 4, 6)

cat("\n\n")
```

Notice that in presence of `tax` the estimate and standard errors of
`rad` changes drastically. This is because, `tax` and `rad` are highly
correlated 

- The figure below shows the correlation plot of `Boston` data, where we see that indeed `tax` and `rad` have high correlation.

```{r cr, echo=FALSE, fig.cap="Correlation plot of Boston data.", fig.width=7, fig.height=8}
ggcorrplot::ggcorrplot(cor(Boston), 
                       hc.order = TRUE, lab = TRUE,
                       ggtheme = theme_bw(base_size = 18)) + 
#   theme_bw(base_size = 18) + 
  theme(legend.position = "top", legend.key.width = unit(1, 'cm'))
```

If more that two predictors are closely related, we call the situation
*multicollinearity*. Such situations can not be detected by simply
inspecting the correlation plot. Instead, we may look at the *variance
inflation factor (VIF)*.

**Variance inflation factor (VIF)**

- The variance inflation factor is the ratio of the variance of $\widehat\beta_j$ when fitting the full model to the variance if fit on its own. 
- The VIF can be computed as follows:
$$
VIF = (1 - R^2_j)^{-1},
$$
where $R^2_j$ is the $R^2$ value from a regression of $X_j$ onto the remaining predictors.


The minimum value of VIF is $1$. As a rule of thumb, a VIF
value larger than 5 or 10 indicates a problematic amount of
multicollinearity. 

- We can use `car::vif()` to calculate VIFs. In our
example above, the VIF for each model are shown below.

```{r, echo=FALSE}
a <- car::vif(mod1)
b <- car::vif(mod2)
cat("Model A:\n")
a
cat("Model B:\n")
b
```

In presence of multicollinearity, we can exclude the problematic
predictors. Alternatively, we can combine the collinear predictors.,
e.g., taking an average.

We'll investigate many different methods for building models that can account for this issue!


# Recap

We have now fully described our MLR model. 

- We often fit the model using OLS or (a form of) maximum likelihood under the normal errors assumption
- We can conduct inference reasonably easily using this model. Diagnostic plots can help us feel more confident in the inferences we make
- We can include interaction terms, polynomial terms, and qualitative predictors through the use of indicator variables

The more predictors we include, the more flexible our model. But how flexible is too flexible? When do we have enough? That is what we take up next!